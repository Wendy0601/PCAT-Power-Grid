{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import numpy as np\n",
    "from scipy import *\n",
    "from numpy import dot, multiply, diag, power, pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv,norm\n",
    "from scipy.linalg import svd, svdvals \n",
    "import scipy.io as sio  \n",
    "import re  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD   \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from utility import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "savename = 'FGSM_regu_test'  \n",
    "n_class =87\n",
    "dim_input = 1\n",
    "# parameters for CNN \n",
    "patience = 30  \n",
    "gamma = 0.1\n",
    "batch_size =70\n",
    "display_step = 100  \n",
    "num_bus =68 \n",
    "learning_rate = 0.01               \n",
    "rootPath =  './data'\n",
    "trainName = 'train_data.mat'  \n",
    "testName = 'testing_sigPQ_perturb_1' \n",
    "scenario = 1 # choose 1 or 2 denoting the two kinds of testing data in the corresponding scenario \n",
    "model_dir  = './saved_model' \n",
    "epsilon = 0.005\n",
    "k = 7\n",
    "alpha = 0.01 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "weight_decay = 5e-5\n",
    "epochs =800  \n",
    "dim_input = 1 \n",
    "dim_hidden = [4,8,8,8]\n",
    "nclass = 87\n",
    "seed = 1 \n",
    "early_stop = False\n",
    "lam  = 0.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = sio.loadmat(os.path.join(rootPath, trainName))\n",
    "linedata, Y,  line_neib = loadline(rootPath  ) \n",
    "Y_ri = np.r_[np.c_[Y.real, -Y.imag], np.c_[ Y.imag, Y.real ]].T \n",
    "w = choose_w(linedata,2)  \n",
    "train_x,    train_labels, train_num  = load_all_data_VI(w,rootPath, trainName ) \n",
    "cur_up_limit, cur_down_limit = current_dist(rootPath) \n",
    "vol_up_limit, vol_down_limit = vol_dist(rootPath) \n",
    "up_limit = epsilon * np.r_[vol_up_limit, cur_up_limit]\n",
    "down_limit = epsilon *np.r_[vol_down_limit, cur_down_limit]\n",
    "up_limit = convert_shape(up_limit, batch_size)\n",
    "down_limit = convert_shape(down_limit, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seed, dim_input, dim_hidden, up_limit, down_limit, batch_size, step_size, k):\n",
    "    np.random.seed( seed)\n",
    "    torch.manual_seed( seed) \n",
    "    \n",
    "    model = Net(dim_input, dim_hidden, nclass) \n",
    "    model.apply(weights_init) \n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "    criterion = CrossEntropyLoss()   \n",
    "    pre_robust_acc = 0. \n",
    "    x_train, y_train = Variable(train_x)  , Variable(train_labels) \n",
    "    train_best = float('Inf') \n",
    "    train_loss_list= [] \n",
    "    for epoch in range( epochs):  \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_n = 0  \n",
    "        for i in range(int(train_x.shape[0] / batch_size)):\n",
    "            id_train = np.random.choice(train_x.shape[0], batch_size, replace= False) \n",
    "            model.eval()\n",
    "            batch_size, _, size_U, _ = x_train[id_train].shape \n",
    "            delta = torch.randn((batch_size, 1, 2*size_U, 1)).detach()\n",
    "            delta = torch.min(torch.max(delta,   down_limit),   up_limit) \n",
    "            delta.requires_grad_() \n",
    "            delta_U = delta[:, 0, :size_U, 0]\n",
    "            delta_I = delta[:, 0, size_U:,0]  \n",
    "            output = model(x_train[id_train] +    delta[:, :, :size_U, :] )  \n",
    "            loss = F.cross_entropy(output, y_train[id_train], size_average = False )\\\n",
    "            - gamma*torch.nn.functional.l1_loss( (torch.matmul( delta_U , torch.FloatTensor(Y_ri))   ), (delta_I), size_average = False)\n",
    "            loss.backward()\n",
    "            grad = delta.grad.detach()\n",
    "            delta = delta.detach() + step_size * torch.sign(grad.detach()) \n",
    "            delta = torch.min(torch.max(delta,   down_limit),   up_limit)  \n",
    "            model.train() \n",
    "            x_adv = x_train[id_train].detach()  + delta[:, :, :size_U, :].detach()\n",
    "            output = model(x_adv)\n",
    "            loss = criterion(output, y_train[id_train])\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()  #* y_train[id_train].size(0)\n",
    "            train_acc  = (output.max(1)[1] == y_train[id_train]).sum().item() *100/np.shape(id_train)[0]  \n",
    "            train_n += y_train[id_train].size(0)\n",
    "            if epoch%5 == 0: \n",
    "                print('Training Epoch: {}, [{}/{}, {:.0f}%], loss is {:.6f}'\\\n",
    "                      .format(epoch  , i * batch_size, train_x.shape[0],  train_acc  , train_loss  )) \n",
    "                if train_loss < train_best:\n",
    "                    print('Best Epoch is', epoch)\n",
    "                    train_best = train_loss \n",
    "                    torch.save(model.state_dict(), os.path.join(model_dir, savename+'.pt'))\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(model_dir, savename+'.tar')) \n",
    "            train_loss_list.append(loss.item())  \n",
    "        if not early_stop:\n",
    "            best_state_dict = model.state_dict()    \n",
    "    plt.plot(train_loss_list)\n",
    "    return train_loss_list , train_best \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 0, [0/560, 0%], loss is 4.599880\n",
      "Best Epoch is 0\n",
      "Training Epoch: 0, [70/560, 3%], loss is 9.090025\n",
      "Training Epoch: 0, [140/560, 4%], loss is 13.526078\n",
      "Training Epoch: 0, [210/560, 3%], loss is 17.816147\n",
      "Training Epoch: 0, [280/560, 7%], loss is 22.081490\n",
      "Training Epoch: 0, [350/560, 3%], loss is 26.319813\n",
      "Training Epoch: 0, [420/560, 3%], loss is 30.383198\n",
      "Training Epoch: 0, [490/560, 10%], loss is 34.372400\n",
      "Training Epoch: 5, [0/560, 34%], loss is 2.447065\n",
      "Best Epoch is 5\n",
      "Training Epoch: 5, [70/560, 41%], loss is 4.734198\n",
      "Training Epoch: 5, [140/560, 37%], loss is 6.964426\n",
      "Training Epoch: 5, [210/560, 47%], loss is 9.141148\n",
      "Training Epoch: 5, [280/560, 37%], loss is 11.393148\n",
      "Training Epoch: 5, [350/560, 57%], loss is 13.267356\n",
      "Training Epoch: 5, [420/560, 47%], loss is 15.313982\n",
      "Training Epoch: 5, [490/560, 41%], loss is 17.432594\n",
      "Training Epoch: 10, [0/560, 50%], loss is 1.489978\n",
      "Best Epoch is 10\n",
      "Training Epoch: 10, [70/560, 56%], loss is 2.823025\n",
      "Training Epoch: 10, [140/560, 74%], loss is 3.918056\n",
      "Training Epoch: 10, [210/560, 64%], loss is 5.131253\n",
      "Training Epoch: 10, [280/560, 70%], loss is 6.236982\n",
      "Training Epoch: 10, [350/560, 61%], loss is 7.488305\n",
      "Training Epoch: 10, [420/560, 70%], loss is 8.580531\n",
      "Training Epoch: 10, [490/560, 71%], loss is 9.611821\n",
      "Training Epoch: 15, [0/560, 71%], loss is 0.931728\n",
      "Best Epoch is 15\n",
      "Training Epoch: 15, [70/560, 77%], loss is 1.784963\n",
      "Training Epoch: 15, [140/560, 67%], loss is 2.766504\n",
      "Training Epoch: 15, [210/560, 67%], loss is 3.749707\n",
      "Training Epoch: 15, [280/560, 67%], loss is 4.747742\n",
      "Training Epoch: 15, [350/560, 67%], loss is 5.764866\n",
      "Training Epoch: 15, [420/560, 76%], loss is 6.670498\n",
      "Training Epoch: 15, [490/560, 70%], loss is 7.569681\n",
      "Training Epoch: 20, [0/560, 80%], loss is 0.643653\n",
      "Best Epoch is 20\n",
      "Training Epoch: 20, [70/560, 73%], loss is 1.418496\n",
      "Training Epoch: 20, [140/560, 83%], loss is 2.077883\n",
      "Training Epoch: 20, [210/560, 80%], loss is 2.646245\n",
      "Training Epoch: 20, [280/560, 80%], loss is 3.278878\n",
      "Training Epoch: 20, [350/560, 77%], loss is 3.905611\n",
      "Training Epoch: 20, [420/560, 79%], loss is 4.504069\n",
      "Training Epoch: 20, [490/560, 86%], loss is 5.069234\n",
      "Training Epoch: 25, [0/560, 79%], loss is 0.557469\n",
      "Best Epoch is 25\n",
      "Training Epoch: 25, [70/560, 67%], loss is 1.324589\n",
      "Training Epoch: 25, [140/560, 70%], loss is 2.150647\n",
      "Training Epoch: 25, [210/560, 77%], loss is 2.802932\n",
      "Training Epoch: 25, [280/560, 71%], loss is 3.554287\n",
      "Training Epoch: 25, [350/560, 66%], loss is 4.476383\n",
      "Training Epoch: 25, [420/560, 69%], loss is 5.277715\n",
      "Training Epoch: 25, [490/560, 74%], loss is 5.925335\n",
      "Training Epoch: 30, [0/560, 79%], loss is 0.604821\n",
      "Training Epoch: 30, [70/560, 80%], loss is 1.149708\n",
      "Training Epoch: 30, [140/560, 89%], loss is 1.550784\n",
      "Training Epoch: 30, [210/560, 76%], loss is 2.109003\n",
      "Training Epoch: 30, [280/560, 81%], loss is 2.784361\n",
      "Training Epoch: 30, [350/560, 71%], loss is 3.661233\n",
      "Training Epoch: 30, [420/560, 80%], loss is 4.299527\n",
      "Training Epoch: 30, [490/560, 86%], loss is 4.819864\n",
      "Training Epoch: 35, [0/560, 84%], loss is 0.411138\n",
      "Best Epoch is 35\n",
      "Training Epoch: 35, [70/560, 80%], loss is 1.079420\n",
      "Training Epoch: 35, [140/560, 83%], loss is 1.596299\n",
      "Training Epoch: 35, [210/560, 89%], loss is 2.080008\n",
      "Training Epoch: 35, [280/560, 83%], loss is 2.520317\n",
      "Training Epoch: 35, [350/560, 80%], loss is 3.079842\n",
      "Training Epoch: 35, [420/560, 76%], loss is 3.712428\n",
      "Training Epoch: 35, [490/560, 84%], loss is 4.181517\n",
      "Training Epoch: 40, [0/560, 86%], loss is 0.392250\n",
      "Best Epoch is 40\n",
      "Training Epoch: 40, [70/560, 87%], loss is 0.801888\n",
      "Training Epoch: 40, [140/560, 86%], loss is 1.296557\n",
      "Training Epoch: 40, [210/560, 83%], loss is 1.812611\n",
      "Training Epoch: 40, [280/560, 89%], loss is 2.199843\n",
      "Training Epoch: 40, [350/560, 93%], loss is 2.488074\n",
      "Training Epoch: 40, [420/560, 89%], loss is 2.805908\n",
      "Training Epoch: 40, [490/560, 83%], loss is 3.166138\n",
      "Training Epoch: 45, [0/560, 84%], loss is 0.410968\n",
      "Training Epoch: 45, [70/560, 86%], loss is 0.867018\n",
      "Training Epoch: 45, [140/560, 84%], loss is 1.354446\n",
      "Training Epoch: 45, [210/560, 90%], loss is 1.638649\n",
      "Training Epoch: 45, [280/560, 94%], loss is 1.913384\n",
      "Training Epoch: 45, [350/560, 84%], loss is 2.367616\n",
      "Training Epoch: 45, [420/560, 91%], loss is 2.716970\n",
      "Training Epoch: 45, [490/560, 84%], loss is 3.129863\n",
      "Training Epoch: 50, [0/560, 89%], loss is 0.454752\n",
      "Training Epoch: 50, [70/560, 87%], loss is 0.897039\n",
      "Training Epoch: 50, [140/560, 80%], loss is 1.458986\n",
      "Training Epoch: 50, [210/560, 86%], loss is 1.952271\n",
      "Training Epoch: 50, [280/560, 86%], loss is 2.370750\n",
      "Training Epoch: 50, [350/560, 76%], loss is 2.899819\n",
      "Training Epoch: 50, [420/560, 81%], loss is 3.343699\n",
      "Training Epoch: 50, [490/560, 86%], loss is 3.763497\n",
      "Training Epoch: 55, [0/560, 80%], loss is 0.500220\n",
      "Training Epoch: 55, [70/560, 87%], loss is 0.856174\n",
      "Training Epoch: 55, [140/560, 79%], loss is 1.378351\n",
      "Training Epoch: 55, [210/560, 87%], loss is 1.843570\n",
      "Training Epoch: 55, [280/560, 83%], loss is 2.276939\n",
      "Training Epoch: 55, [350/560, 77%], loss is 2.930013\n",
      "Training Epoch: 55, [420/560, 83%], loss is 3.420528\n",
      "Training Epoch: 55, [490/560, 83%], loss is 3.819834\n",
      "Training Epoch: 60, [0/560, 83%], loss is 0.443192\n",
      "Training Epoch: 60, [70/560, 93%], loss is 0.712085\n",
      "Training Epoch: 60, [140/560, 91%], loss is 1.003959\n",
      "Training Epoch: 60, [210/560, 89%], loss is 1.420071\n",
      "Training Epoch: 60, [280/560, 84%], loss is 1.814034\n",
      "Training Epoch: 60, [350/560, 89%], loss is 2.204938\n",
      "Training Epoch: 60, [420/560, 91%], loss is 2.456906\n",
      "Training Epoch: 60, [490/560, 83%], loss is 2.906946\n",
      "Training Epoch: 65, [0/560, 89%], loss is 0.364761\n",
      "Best Epoch is 65\n",
      "Training Epoch: 65, [70/560, 89%], loss is 0.691613\n",
      "Training Epoch: 65, [140/560, 83%], loss is 1.142323\n",
      "Training Epoch: 65, [210/560, 89%], loss is 1.446286\n",
      "Training Epoch: 65, [280/560, 80%], loss is 1.858837\n",
      "Training Epoch: 65, [350/560, 86%], loss is 2.151285\n",
      "Training Epoch: 65, [420/560, 79%], loss is 2.747627\n",
      "Training Epoch: 65, [490/560, 84%], loss is 3.207561\n",
      "Training Epoch: 70, [0/560, 74%], loss is 0.695761\n",
      "Training Epoch: 70, [70/560, 97%], loss is 0.873204\n",
      "Training Epoch: 70, [140/560, 84%], loss is 1.236399\n",
      "Training Epoch: 70, [210/560, 84%], loss is 1.659962\n",
      "Training Epoch: 70, [280/560, 73%], loss is 2.356437\n",
      "Training Epoch: 70, [350/560, 83%], loss is 2.971340\n",
      "Training Epoch: 70, [420/560, 86%], loss is 3.387575\n",
      "Training Epoch: 70, [490/560, 79%], loss is 3.946183\n",
      "Training Epoch: 75, [0/560, 93%], loss is 0.276216\n",
      "Best Epoch is 75\n",
      "Training Epoch: 75, [70/560, 97%], loss is 0.500854\n",
      "Training Epoch: 75, [140/560, 86%], loss is 0.869420\n",
      "Training Epoch: 75, [210/560, 87%], loss is 1.221275\n",
      "Training Epoch: 75, [280/560, 87%], loss is 1.484772\n",
      "Training Epoch: 75, [350/560, 90%], loss is 1.739620\n",
      "Training Epoch: 75, [420/560, 90%], loss is 2.008718\n",
      "Training Epoch: 75, [490/560, 87%], loss is 2.378128\n",
      "Training Epoch: 80, [0/560, 79%], loss is 0.481092\n",
      "Training Epoch: 80, [70/560, 90%], loss is 0.747044\n",
      "Training Epoch: 80, [140/560, 77%], loss is 1.509836\n",
      "Training Epoch: 80, [210/560, 86%], loss is 1.902294\n",
      "Training Epoch: 80, [280/560, 89%], loss is 2.224877\n",
      "Training Epoch: 80, [350/560, 93%], loss is 2.520514\n",
      "Training Epoch: 80, [420/560, 86%], loss is 2.857736\n",
      "Training Epoch: 80, [490/560, 90%], loss is 3.203754\n",
      "Training Epoch: 85, [0/560, 97%], loss is 0.161614\n",
      "Best Epoch is 85\n",
      "Training Epoch: 85, [70/560, 91%], loss is 0.551609\n",
      "Training Epoch: 85, [140/560, 83%], loss is 0.977762\n",
      "Training Epoch: 85, [210/560, 87%], loss is 1.274189\n",
      "Training Epoch: 85, [280/560, 87%], loss is 1.583684\n",
      "Training Epoch: 85, [350/560, 91%], loss is 1.901497\n",
      "Training Epoch: 85, [420/560, 89%], loss is 2.205117\n",
      "Training Epoch: 85, [490/560, 91%], loss is 2.466068\n",
      "Training Epoch: 90, [0/560, 90%], loss is 0.261929\n",
      "Training Epoch: 90, [70/560, 87%], loss is 0.602520\n",
      "Training Epoch: 90, [140/560, 90%], loss is 0.858529\n",
      "Training Epoch: 90, [210/560, 90%], loss is 1.113512\n",
      "Training Epoch: 90, [280/560, 86%], loss is 1.387275\n",
      "Training Epoch: 90, [350/560, 90%], loss is 1.635975\n",
      "Training Epoch: 90, [420/560, 89%], loss is 1.924596\n",
      "Training Epoch: 90, [490/560, 87%], loss is 2.271959\n",
      "Training Epoch: 95, [0/560, 86%], loss is 0.306256\n",
      "Training Epoch: 95, [70/560, 93%], loss is 0.647119\n",
      "Training Epoch: 95, [140/560, 94%], loss is 0.815683\n",
      "Training Epoch: 95, [210/560, 91%], loss is 1.076821\n",
      "Training Epoch: 95, [280/560, 93%], loss is 1.337956\n",
      "Training Epoch: 95, [350/560, 91%], loss is 1.575285\n",
      "Training Epoch: 95, [420/560, 91%], loss is 1.772811\n",
      "Training Epoch: 95, [490/560, 86%], loss is 2.268219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100, [0/560, 91%], loss is 0.247265\n",
      "Training Epoch: 100, [70/560, 89%], loss is 0.564861\n",
      "Training Epoch: 100, [140/560, 89%], loss is 0.835068\n",
      "Training Epoch: 100, [210/560, 87%], loss is 1.209579\n",
      "Training Epoch: 100, [280/560, 80%], loss is 1.624558\n",
      "Training Epoch: 100, [350/560, 93%], loss is 1.881071\n",
      "Training Epoch: 100, [420/560, 86%], loss is 2.211329\n",
      "Training Epoch: 100, [490/560, 91%], loss is 2.478014\n",
      "Training Epoch: 105, [0/560, 86%], loss is 0.713690\n",
      "Training Epoch: 105, [70/560, 84%], loss is 1.084600\n",
      "Training Epoch: 105, [140/560, 87%], loss is 1.463745\n",
      "Training Epoch: 105, [210/560, 80%], loss is 2.087129\n",
      "Training Epoch: 105, [280/560, 89%], loss is 2.435305\n",
      "Training Epoch: 105, [350/560, 89%], loss is 2.793610\n",
      "Training Epoch: 105, [420/560, 80%], loss is 3.228585\n",
      "Training Epoch: 105, [490/560, 89%], loss is 3.600450\n",
      "Training Epoch: 110, [0/560, 86%], loss is 0.388014\n",
      "Training Epoch: 110, [70/560, 89%], loss is 0.648177\n",
      "Training Epoch: 110, [140/560, 90%], loss is 0.904064\n",
      "Training Epoch: 110, [210/560, 86%], loss is 1.274226\n",
      "Training Epoch: 110, [280/560, 91%], loss is 1.623840\n",
      "Training Epoch: 110, [350/560, 97%], loss is 1.782451\n",
      "Training Epoch: 110, [420/560, 86%], loss is 2.195203\n",
      "Training Epoch: 110, [490/560, 84%], loss is 2.587030\n",
      "Training Epoch: 115, [0/560, 83%], loss is 0.271851\n",
      "Training Epoch: 115, [70/560, 87%], loss is 0.555995\n",
      "Training Epoch: 115, [140/560, 87%], loss is 0.869005\n",
      "Training Epoch: 115, [210/560, 97%], loss is 1.070927\n",
      "Training Epoch: 115, [280/560, 93%], loss is 1.321055\n",
      "Training Epoch: 115, [350/560, 90%], loss is 1.568048\n",
      "Training Epoch: 115, [420/560, 84%], loss is 1.887164\n",
      "Training Epoch: 115, [490/560, 86%], loss is 2.227781\n",
      "Training Epoch: 120, [0/560, 89%], loss is 0.366961\n",
      "Training Epoch: 120, [70/560, 93%], loss is 0.588208\n",
      "Training Epoch: 120, [140/560, 87%], loss is 0.846377\n",
      "Training Epoch: 120, [210/560, 86%], loss is 1.160667\n",
      "Training Epoch: 120, [280/560, 81%], loss is 1.617625\n",
      "Training Epoch: 120, [350/560, 91%], loss is 1.861417\n",
      "Training Epoch: 120, [420/560, 89%], loss is 2.231012\n",
      "Training Epoch: 120, [490/560, 84%], loss is 2.568170\n",
      "Training Epoch: 125, [0/560, 91%], loss is 0.300915\n",
      "Training Epoch: 125, [70/560, 83%], loss is 0.699972\n",
      "Training Epoch: 125, [140/560, 84%], loss is 1.081821\n",
      "Training Epoch: 125, [210/560, 87%], loss is 1.410673\n",
      "Training Epoch: 125, [280/560, 93%], loss is 1.656847\n",
      "Training Epoch: 125, [350/560, 86%], loss is 1.984509\n",
      "Training Epoch: 125, [420/560, 91%], loss is 2.181353\n",
      "Training Epoch: 125, [490/560, 90%], loss is 2.475561\n",
      "Training Epoch: 130, [0/560, 93%], loss is 0.181562\n",
      "Training Epoch: 130, [70/560, 89%], loss is 0.483645\n",
      "Training Epoch: 130, [140/560, 86%], loss is 0.821103\n",
      "Training Epoch: 130, [210/560, 86%], loss is 1.168595\n",
      "Training Epoch: 130, [280/560, 90%], loss is 1.449339\n",
      "Training Epoch: 130, [350/560, 87%], loss is 1.721073\n",
      "Training Epoch: 130, [420/560, 94%], loss is 1.931994\n",
      "Training Epoch: 130, [490/560, 86%], loss is 2.308210\n",
      "Training Epoch: 135, [0/560, 93%], loss is 0.162422\n",
      "Training Epoch: 135, [70/560, 89%], loss is 0.496669\n",
      "Training Epoch: 135, [140/560, 93%], loss is 0.656909\n",
      "Training Epoch: 135, [210/560, 91%], loss is 0.836253\n",
      "Training Epoch: 135, [280/560, 86%], loss is 1.160150\n",
      "Training Epoch: 135, [350/560, 90%], loss is 1.543806\n",
      "Training Epoch: 135, [420/560, 86%], loss is 2.024091\n",
      "Training Epoch: 135, [490/560, 89%], loss is 2.373109\n",
      "Training Epoch: 140, [0/560, 86%], loss is 0.292769\n",
      "Training Epoch: 140, [70/560, 87%], loss is 0.558936\n",
      "Training Epoch: 140, [140/560, 84%], loss is 0.902808\n",
      "Training Epoch: 140, [210/560, 94%], loss is 1.086125\n",
      "Training Epoch: 140, [280/560, 90%], loss is 1.386329\n",
      "Training Epoch: 140, [350/560, 94%], loss is 1.596669\n",
      "Training Epoch: 140, [420/560, 89%], loss is 1.882681\n",
      "Training Epoch: 140, [490/560, 87%], loss is 2.225864\n",
      "Training Epoch: 145, [0/560, 89%], loss is 0.257922\n",
      "Training Epoch: 145, [70/560, 91%], loss is 0.496795\n",
      "Training Epoch: 145, [140/560, 84%], loss is 0.850136\n",
      "Training Epoch: 145, [210/560, 93%], loss is 1.037446\n",
      "Training Epoch: 145, [280/560, 96%], loss is 1.246835\n",
      "Training Epoch: 145, [350/560, 89%], loss is 1.570473\n",
      "Training Epoch: 145, [420/560, 89%], loss is 1.872096\n",
      "Training Epoch: 145, [490/560, 93%], loss is 2.081286\n",
      "Training Epoch: 150, [0/560, 97%], loss is 0.145619\n",
      "Best Epoch is 150\n",
      "Training Epoch: 150, [70/560, 89%], loss is 0.477033\n",
      "Training Epoch: 150, [140/560, 89%], loss is 0.761047\n",
      "Training Epoch: 150, [210/560, 91%], loss is 1.028608\n",
      "Training Epoch: 150, [280/560, 84%], loss is 1.368784\n",
      "Training Epoch: 150, [350/560, 91%], loss is 1.601451\n",
      "Training Epoch: 150, [420/560, 90%], loss is 1.810713\n",
      "Training Epoch: 150, [490/560, 89%], loss is 2.136580\n",
      "Training Epoch: 155, [0/560, 96%], loss is 0.121799\n",
      "Best Epoch is 155\n",
      "Training Epoch: 155, [70/560, 91%], loss is 0.325502\n",
      "Training Epoch: 155, [140/560, 96%], loss is 0.514552\n",
      "Training Epoch: 155, [210/560, 99%], loss is 0.646336\n",
      "Training Epoch: 155, [280/560, 91%], loss is 0.901171\n",
      "Training Epoch: 155, [350/560, 90%], loss is 1.124007\n",
      "Training Epoch: 155, [420/560, 84%], loss is 1.458765\n",
      "Training Epoch: 155, [490/560, 96%], loss is 1.602664\n",
      "Training Epoch: 160, [0/560, 90%], loss is 0.305964\n",
      "Training Epoch: 160, [70/560, 83%], loss is 0.731803\n",
      "Training Epoch: 160, [140/560, 84%], loss is 1.283505\n",
      "Training Epoch: 160, [210/560, 80%], loss is 1.734926\n",
      "Training Epoch: 160, [280/560, 80%], loss is 2.231285\n",
      "Training Epoch: 160, [350/560, 91%], loss is 2.488840\n",
      "Training Epoch: 160, [420/560, 87%], loss is 2.891413\n",
      "Training Epoch: 160, [490/560, 90%], loss is 3.198542\n",
      "Training Epoch: 165, [0/560, 87%], loss is 0.290406\n",
      "Training Epoch: 165, [70/560, 91%], loss is 0.519427\n",
      "Training Epoch: 165, [140/560, 94%], loss is 0.766684\n",
      "Training Epoch: 165, [210/560, 94%], loss is 0.955179\n",
      "Training Epoch: 165, [280/560, 99%], loss is 1.089092\n",
      "Training Epoch: 165, [350/560, 93%], loss is 1.328722\n",
      "Training Epoch: 165, [420/560, 89%], loss is 1.644723\n",
      "Training Epoch: 165, [490/560, 97%], loss is 1.763331\n",
      "Training Epoch: 170, [0/560, 94%], loss is 0.242558\n",
      "Training Epoch: 170, [70/560, 93%], loss is 0.524170\n",
      "Training Epoch: 170, [140/560, 87%], loss is 0.792162\n",
      "Training Epoch: 170, [210/560, 91%], loss is 0.963590\n",
      "Training Epoch: 170, [280/560, 91%], loss is 1.195658\n",
      "Training Epoch: 170, [350/560, 83%], loss is 1.689155\n",
      "Training Epoch: 170, [420/560, 93%], loss is 1.982022\n",
      "Training Epoch: 170, [490/560, 90%], loss is 2.218411\n",
      "Training Epoch: 175, [0/560, 90%], loss is 0.188315\n",
      "Training Epoch: 175, [70/560, 89%], loss is 0.484145\n",
      "Training Epoch: 175, [140/560, 91%], loss is 0.669703\n",
      "Training Epoch: 175, [210/560, 94%], loss is 0.843849\n",
      "Training Epoch: 175, [280/560, 90%], loss is 1.102096\n",
      "Training Epoch: 175, [350/560, 93%], loss is 1.307838\n",
      "Training Epoch: 175, [420/560, 91%], loss is 1.650107\n",
      "Training Epoch: 175, [490/560, 87%], loss is 1.963618\n",
      "Training Epoch: 180, [0/560, 96%], loss is 0.213769\n",
      "Training Epoch: 180, [70/560, 86%], loss is 0.506038\n",
      "Training Epoch: 180, [140/560, 89%], loss is 0.767785\n",
      "Training Epoch: 180, [210/560, 84%], loss is 1.181126\n",
      "Training Epoch: 180, [280/560, 86%], loss is 1.558914\n",
      "Training Epoch: 180, [350/560, 89%], loss is 1.802809\n",
      "Training Epoch: 180, [420/560, 94%], loss is 2.056378\n",
      "Training Epoch: 180, [490/560, 86%], loss is 2.430742\n",
      "Training Epoch: 185, [0/560, 90%], loss is 0.340638\n",
      "Training Epoch: 185, [70/560, 86%], loss is 0.650685\n",
      "Training Epoch: 185, [140/560, 89%], loss is 0.933232\n",
      "Training Epoch: 185, [210/560, 83%], loss is 1.292207\n",
      "Training Epoch: 185, [280/560, 93%], loss is 1.556282\n",
      "Training Epoch: 185, [350/560, 89%], loss is 1.878702\n",
      "Training Epoch: 185, [420/560, 91%], loss is 2.139971\n",
      "Training Epoch: 185, [490/560, 96%], loss is 2.284175\n",
      "Training Epoch: 190, [0/560, 91%], loss is 0.228657\n",
      "Training Epoch: 190, [70/560, 90%], loss is 0.500741\n",
      "Training Epoch: 190, [140/560, 87%], loss is 0.808020\n",
      "Training Epoch: 190, [210/560, 91%], loss is 1.047723\n",
      "Training Epoch: 190, [280/560, 94%], loss is 1.225549\n",
      "Training Epoch: 190, [350/560, 96%], loss is 1.367582\n",
      "Training Epoch: 190, [420/560, 97%], loss is 1.483900\n",
      "Training Epoch: 190, [490/560, 93%], loss is 1.650277\n",
      "Training Epoch: 195, [0/560, 94%], loss is 0.151821\n",
      "Training Epoch: 195, [70/560, 94%], loss is 0.342737\n",
      "Training Epoch: 195, [140/560, 94%], loss is 0.511289\n",
      "Training Epoch: 195, [210/560, 94%], loss is 0.652160\n",
      "Training Epoch: 195, [280/560, 93%], loss is 0.803405\n",
      "Training Epoch: 195, [350/560, 94%], loss is 0.980188\n",
      "Training Epoch: 195, [420/560, 99%], loss is 1.121620\n",
      "Training Epoch: 195, [490/560, 90%], loss is 1.358194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 200, [0/560, 93%], loss is 0.123837\n",
      "Training Epoch: 200, [70/560, 90%], loss is 0.289700\n",
      "Training Epoch: 200, [140/560, 93%], loss is 0.481654\n",
      "Training Epoch: 200, [210/560, 97%], loss is 0.621277\n",
      "Training Epoch: 200, [280/560, 93%], loss is 0.845089\n",
      "Training Epoch: 200, [350/560, 91%], loss is 1.036086\n",
      "Training Epoch: 200, [420/560, 97%], loss is 1.182209\n",
      "Training Epoch: 200, [490/560, 94%], loss is 1.353374\n",
      "Training Epoch: 205, [0/560, 84%], loss is 0.328061\n",
      "Training Epoch: 205, [70/560, 87%], loss is 0.639518\n",
      "Training Epoch: 205, [140/560, 93%], loss is 0.849810\n",
      "Training Epoch: 205, [210/560, 91%], loss is 1.092057\n",
      "Training Epoch: 205, [280/560, 94%], loss is 1.294851\n",
      "Training Epoch: 205, [350/560, 91%], loss is 1.612081\n",
      "Training Epoch: 205, [420/560, 90%], loss is 1.997121\n",
      "Training Epoch: 205, [490/560, 86%], loss is 2.399600\n",
      "Training Epoch: 210, [0/560, 94%], loss is 0.221358\n",
      "Training Epoch: 210, [70/560, 91%], loss is 0.516945\n",
      "Training Epoch: 210, [140/560, 90%], loss is 0.915063\n",
      "Training Epoch: 210, [210/560, 87%], loss is 1.209789\n",
      "Training Epoch: 210, [280/560, 90%], loss is 1.519348\n",
      "Training Epoch: 210, [350/560, 91%], loss is 1.745257\n",
      "Training Epoch: 210, [420/560, 87%], loss is 2.119166\n",
      "Training Epoch: 210, [490/560, 94%], loss is 2.329288\n",
      "Training Epoch: 215, [0/560, 83%], loss is 0.369483\n",
      "Training Epoch: 215, [70/560, 83%], loss is 0.913182\n",
      "Training Epoch: 215, [140/560, 89%], loss is 1.282331\n",
      "Training Epoch: 215, [210/560, 87%], loss is 1.651453\n",
      "Training Epoch: 215, [280/560, 87%], loss is 1.939589\n",
      "Training Epoch: 215, [350/560, 96%], loss is 2.104773\n",
      "Training Epoch: 215, [420/560, 81%], loss is 2.453443\n",
      "Training Epoch: 215, [490/560, 84%], loss is 2.822799\n",
      "Training Epoch: 220, [0/560, 90%], loss is 0.259147\n",
      "Training Epoch: 220, [70/560, 91%], loss is 0.434069\n",
      "Training Epoch: 220, [140/560, 91%], loss is 0.630269\n",
      "Training Epoch: 220, [210/560, 84%], loss is 0.956149\n",
      "Training Epoch: 220, [280/560, 89%], loss is 1.313117\n",
      "Training Epoch: 220, [350/560, 89%], loss is 1.585602\n",
      "Training Epoch: 220, [420/560, 89%], loss is 1.863675\n",
      "Training Epoch: 220, [490/560, 100%], loss is 1.968317\n",
      "Training Epoch: 225, [0/560, 91%], loss is 0.203735\n",
      "Training Epoch: 225, [70/560, 89%], loss is 0.435518\n",
      "Training Epoch: 225, [140/560, 87%], loss is 0.915915\n",
      "Training Epoch: 225, [210/560, 90%], loss is 1.145752\n",
      "Training Epoch: 225, [280/560, 93%], loss is 1.329366\n",
      "Training Epoch: 225, [350/560, 96%], loss is 1.501819\n",
      "Training Epoch: 225, [420/560, 94%], loss is 1.640082\n",
      "Training Epoch: 225, [490/560, 91%], loss is 2.033620\n",
      "Training Epoch: 230, [0/560, 90%], loss is 0.293404\n",
      "Training Epoch: 230, [70/560, 93%], loss is 0.454762\n",
      "Training Epoch: 230, [140/560, 91%], loss is 0.645080\n",
      "Training Epoch: 230, [210/560, 91%], loss is 0.907902\n",
      "Training Epoch: 230, [280/560, 93%], loss is 1.088779\n",
      "Training Epoch: 230, [350/560, 83%], loss is 1.422627\n",
      "Training Epoch: 230, [420/560, 97%], loss is 1.580278\n",
      "Training Epoch: 230, [490/560, 93%], loss is 1.793695\n",
      "Training Epoch: 235, [0/560, 90%], loss is 0.234239\n",
      "Training Epoch: 235, [70/560, 91%], loss is 0.525321\n",
      "Training Epoch: 235, [140/560, 89%], loss is 0.804605\n",
      "Training Epoch: 235, [210/560, 96%], loss is 0.918771\n",
      "Training Epoch: 235, [280/560, 93%], loss is 1.108145\n",
      "Training Epoch: 235, [350/560, 90%], loss is 1.388993\n",
      "Training Epoch: 235, [420/560, 93%], loss is 1.544739\n",
      "Training Epoch: 235, [490/560, 89%], loss is 1.756731\n",
      "Training Epoch: 240, [0/560, 93%], loss is 0.217183\n",
      "Training Epoch: 240, [70/560, 87%], loss is 0.420235\n",
      "Training Epoch: 240, [140/560, 89%], loss is 0.617390\n",
      "Training Epoch: 240, [210/560, 91%], loss is 0.811433\n",
      "Training Epoch: 240, [280/560, 94%], loss is 1.057295\n",
      "Training Epoch: 240, [350/560, 94%], loss is 1.246103\n",
      "Training Epoch: 240, [420/560, 93%], loss is 1.404605\n",
      "Training Epoch: 240, [490/560, 96%], loss is 1.557240\n",
      "Training Epoch: 245, [0/560, 91%], loss is 0.218438\n",
      "Training Epoch: 245, [70/560, 90%], loss is 0.474572\n",
      "Training Epoch: 245, [140/560, 94%], loss is 0.654900\n",
      "Training Epoch: 245, [210/560, 94%], loss is 0.831504\n",
      "Training Epoch: 245, [280/560, 94%], loss is 0.996984\n",
      "Training Epoch: 245, [350/560, 93%], loss is 1.199010\n",
      "Training Epoch: 245, [420/560, 93%], loss is 1.367869\n",
      "Training Epoch: 245, [490/560, 93%], loss is 1.560480\n",
      "Training Epoch: 250, [0/560, 96%], loss is 0.183686\n",
      "Training Epoch: 250, [70/560, 89%], loss is 0.486448\n",
      "Training Epoch: 250, [140/560, 90%], loss is 0.752517\n",
      "Training Epoch: 250, [210/560, 89%], loss is 1.054460\n",
      "Training Epoch: 250, [280/560, 94%], loss is 1.233878\n",
      "Training Epoch: 250, [350/560, 91%], loss is 1.429346\n",
      "Training Epoch: 250, [420/560, 89%], loss is 1.740267\n",
      "Training Epoch: 250, [490/560, 93%], loss is 1.952899\n",
      "Training Epoch: 255, [0/560, 97%], loss is 0.092236\n",
      "Best Epoch is 255\n",
      "Training Epoch: 255, [70/560, 94%], loss is 0.210520\n",
      "Training Epoch: 255, [140/560, 87%], loss is 0.455237\n",
      "Training Epoch: 255, [210/560, 96%], loss is 0.623594\n",
      "Training Epoch: 255, [280/560, 90%], loss is 0.860651\n",
      "Training Epoch: 255, [350/560, 83%], loss is 1.212337\n",
      "Training Epoch: 255, [420/560, 94%], loss is 1.372516\n",
      "Training Epoch: 255, [490/560, 91%], loss is 1.542970\n",
      "Training Epoch: 260, [0/560, 91%], loss is 0.195669\n",
      "Training Epoch: 260, [70/560, 87%], loss is 0.535141\n",
      "Training Epoch: 260, [140/560, 91%], loss is 0.750265\n",
      "Training Epoch: 260, [210/560, 96%], loss is 0.899424\n",
      "Training Epoch: 260, [280/560, 89%], loss is 1.193877\n",
      "Training Epoch: 260, [350/560, 93%], loss is 1.357583\n",
      "Training Epoch: 260, [420/560, 86%], loss is 1.610338\n",
      "Training Epoch: 260, [490/560, 93%], loss is 1.790852\n",
      "Training Epoch: 265, [0/560, 91%], loss is 0.198189\n",
      "Training Epoch: 265, [70/560, 93%], loss is 0.418204\n",
      "Training Epoch: 265, [140/560, 93%], loss is 0.607263\n",
      "Training Epoch: 265, [210/560, 99%], loss is 0.699368\n",
      "Training Epoch: 265, [280/560, 94%], loss is 0.810528\n",
      "Training Epoch: 265, [350/560, 93%], loss is 0.926080\n",
      "Training Epoch: 265, [420/560, 94%], loss is 1.093126\n",
      "Training Epoch: 265, [490/560, 97%], loss is 1.198875\n",
      "Training Epoch: 270, [0/560, 94%], loss is 0.120628\n",
      "Training Epoch: 270, [70/560, 97%], loss is 0.212237\n",
      "Training Epoch: 270, [140/560, 94%], loss is 0.358154\n",
      "Training Epoch: 270, [210/560, 99%], loss is 0.455509\n",
      "Training Epoch: 270, [280/560, 94%], loss is 0.606377\n",
      "Training Epoch: 270, [350/560, 90%], loss is 0.838234\n",
      "Training Epoch: 270, [420/560, 94%], loss is 1.009540\n",
      "Training Epoch: 270, [490/560, 97%], loss is 1.108855\n",
      "Training Epoch: 275, [0/560, 90%], loss is 0.252049\n",
      "Training Epoch: 275, [70/560, 93%], loss is 0.423328\n",
      "Training Epoch: 275, [140/560, 99%], loss is 0.504110\n",
      "Training Epoch: 275, [210/560, 96%], loss is 0.647449\n",
      "Training Epoch: 275, [280/560, 91%], loss is 0.902845\n",
      "Training Epoch: 275, [350/560, 86%], loss is 1.191896\n",
      "Training Epoch: 275, [420/560, 91%], loss is 1.455043\n",
      "Training Epoch: 275, [490/560, 91%], loss is 1.706472\n",
      "Training Epoch: 280, [0/560, 91%], loss is 0.290101\n",
      "Training Epoch: 280, [70/560, 89%], loss is 0.627975\n",
      "Training Epoch: 280, [140/560, 89%], loss is 0.970883\n",
      "Training Epoch: 280, [210/560, 93%], loss is 1.214007\n",
      "Training Epoch: 280, [280/560, 90%], loss is 1.446302\n",
      "Training Epoch: 280, [350/560, 89%], loss is 1.749260\n",
      "Training Epoch: 280, [420/560, 87%], loss is 1.995098\n",
      "Training Epoch: 280, [490/560, 91%], loss is 2.172373\n",
      "Training Epoch: 285, [0/560, 91%], loss is 0.235127\n",
      "Training Epoch: 285, [70/560, 89%], loss is 0.660925\n",
      "Training Epoch: 285, [140/560, 84%], loss is 1.046882\n",
      "Training Epoch: 285, [210/560, 93%], loss is 1.211329\n",
      "Training Epoch: 285, [280/560, 97%], loss is 1.384279\n",
      "Training Epoch: 285, [350/560, 91%], loss is 1.603496\n",
      "Training Epoch: 285, [420/560, 90%], loss is 1.875919\n",
      "Training Epoch: 285, [490/560, 89%], loss is 2.077180\n",
      "Training Epoch: 290, [0/560, 91%], loss is 0.206543\n",
      "Training Epoch: 290, [70/560, 94%], loss is 0.416702\n",
      "Training Epoch: 290, [140/560, 86%], loss is 0.772080\n",
      "Training Epoch: 290, [210/560, 91%], loss is 1.143863\n",
      "Training Epoch: 290, [280/560, 93%], loss is 1.376396\n",
      "Training Epoch: 290, [350/560, 97%], loss is 1.564085\n",
      "Training Epoch: 290, [420/560, 94%], loss is 1.831431\n",
      "Training Epoch: 290, [490/560, 94%], loss is 1.986790\n",
      "Training Epoch: 295, [0/560, 91%], loss is 0.242416\n",
      "Training Epoch: 295, [70/560, 97%], loss is 0.388807\n",
      "Training Epoch: 295, [140/560, 93%], loss is 0.603524\n",
      "Training Epoch: 295, [210/560, 97%], loss is 0.755278\n",
      "Training Epoch: 295, [280/560, 97%], loss is 0.851269\n",
      "Training Epoch: 295, [350/560, 93%], loss is 1.036580\n",
      "Training Epoch: 295, [420/560, 93%], loss is 1.208029\n",
      "Training Epoch: 295, [490/560, 91%], loss is 1.409438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 300, [0/560, 90%], loss is 0.246543\n",
      "Training Epoch: 300, [70/560, 86%], loss is 0.594448\n",
      "Training Epoch: 300, [140/560, 87%], loss is 0.880366\n",
      "Training Epoch: 300, [210/560, 90%], loss is 1.099181\n",
      "Training Epoch: 300, [280/560, 99%], loss is 1.191623\n",
      "Training Epoch: 300, [350/560, 94%], loss is 1.357487\n",
      "Training Epoch: 300, [420/560, 93%], loss is 1.568408\n",
      "Training Epoch: 300, [490/560, 99%], loss is 1.691181\n",
      "Training Epoch: 305, [0/560, 87%], loss is 0.229496\n",
      "Training Epoch: 305, [70/560, 93%], loss is 0.427428\n",
      "Training Epoch: 305, [140/560, 89%], loss is 0.680836\n",
      "Training Epoch: 305, [210/560, 90%], loss is 0.917776\n",
      "Training Epoch: 305, [280/560, 97%], loss is 1.029827\n",
      "Training Epoch: 305, [350/560, 93%], loss is 1.210706\n",
      "Training Epoch: 305, [420/560, 91%], loss is 1.447410\n",
      "Training Epoch: 305, [490/560, 89%], loss is 1.639226\n",
      "Training Epoch: 310, [0/560, 91%], loss is 0.246420\n",
      "Training Epoch: 310, [70/560, 97%], loss is 0.372177\n",
      "Training Epoch: 310, [140/560, 96%], loss is 0.471895\n",
      "Training Epoch: 310, [210/560, 90%], loss is 0.666454\n",
      "Training Epoch: 310, [280/560, 91%], loss is 0.922708\n",
      "Training Epoch: 310, [350/560, 97%], loss is 1.037443\n",
      "Training Epoch: 310, [420/560, 91%], loss is 1.283265\n",
      "Training Epoch: 310, [490/560, 89%], loss is 1.530077\n",
      "Training Epoch: 315, [0/560, 91%], loss is 0.171671\n",
      "Training Epoch: 315, [70/560, 97%], loss is 0.293620\n",
      "Training Epoch: 315, [140/560, 96%], loss is 0.428714\n",
      "Training Epoch: 315, [210/560, 91%], loss is 0.616055\n",
      "Training Epoch: 315, [280/560, 87%], loss is 0.856521\n",
      "Training Epoch: 315, [350/560, 91%], loss is 1.039899\n",
      "Training Epoch: 315, [420/560, 93%], loss is 1.188138\n",
      "Training Epoch: 315, [490/560, 96%], loss is 1.315950\n",
      "Training Epoch: 320, [0/560, 90%], loss is 0.175737\n",
      "Training Epoch: 320, [70/560, 97%], loss is 0.267797\n",
      "Training Epoch: 320, [140/560, 96%], loss is 0.386203\n",
      "Training Epoch: 320, [210/560, 93%], loss is 0.540288\n",
      "Training Epoch: 320, [280/560, 93%], loss is 0.700960\n",
      "Training Epoch: 320, [350/560, 93%], loss is 0.801875\n",
      "Training Epoch: 320, [420/560, 94%], loss is 0.961378\n",
      "Training Epoch: 320, [490/560, 93%], loss is 1.144593\n",
      "Training Epoch: 325, [0/560, 93%], loss is 0.157066\n",
      "Training Epoch: 325, [70/560, 93%], loss is 0.290143\n",
      "Training Epoch: 325, [140/560, 97%], loss is 0.382470\n",
      "Training Epoch: 325, [210/560, 90%], loss is 0.649290\n",
      "Training Epoch: 325, [280/560, 93%], loss is 0.784907\n",
      "Training Epoch: 325, [350/560, 97%], loss is 0.909746\n",
      "Training Epoch: 325, [420/560, 93%], loss is 1.121294\n",
      "Training Epoch: 325, [490/560, 93%], loss is 1.342660\n",
      "Training Epoch: 330, [0/560, 93%], loss is 0.148968\n",
      "Training Epoch: 330, [70/560, 99%], loss is 0.236253\n",
      "Training Epoch: 330, [140/560, 93%], loss is 0.406723\n",
      "Training Epoch: 330, [210/560, 91%], loss is 0.596316\n",
      "Training Epoch: 330, [280/560, 86%], loss is 0.914615\n",
      "Training Epoch: 330, [350/560, 94%], loss is 1.044779\n",
      "Training Epoch: 330, [420/560, 99%], loss is 1.164196\n",
      "Training Epoch: 330, [490/560, 86%], loss is 1.472296\n",
      "Training Epoch: 335, [0/560, 93%], loss is 0.129413\n",
      "Training Epoch: 335, [70/560, 90%], loss is 0.387494\n",
      "Training Epoch: 335, [140/560, 94%], loss is 0.538320\n",
      "Training Epoch: 335, [210/560, 94%], loss is 0.760167\n",
      "Training Epoch: 335, [280/560, 94%], loss is 0.901530\n",
      "Training Epoch: 335, [350/560, 89%], loss is 1.389256\n",
      "Training Epoch: 335, [420/560, 89%], loss is 1.649640\n",
      "Training Epoch: 335, [490/560, 93%], loss is 1.801910\n",
      "Training Epoch: 340, [0/560, 99%], loss is 0.099183\n",
      "Training Epoch: 340, [70/560, 91%], loss is 0.266227\n",
      "Training Epoch: 340, [140/560, 89%], loss is 0.523844\n",
      "Training Epoch: 340, [210/560, 94%], loss is 0.656845\n",
      "Training Epoch: 340, [280/560, 96%], loss is 0.767740\n",
      "Training Epoch: 340, [350/560, 93%], loss is 0.923015\n",
      "Training Epoch: 340, [420/560, 91%], loss is 1.062411\n",
      "Training Epoch: 340, [490/560, 90%], loss is 1.336770\n",
      "Training Epoch: 345, [0/560, 94%], loss is 0.193826\n",
      "Training Epoch: 345, [70/560, 83%], loss is 0.733433\n",
      "Training Epoch: 345, [140/560, 91%], loss is 0.923829\n",
      "Training Epoch: 345, [210/560, 91%], loss is 1.149765\n",
      "Training Epoch: 345, [280/560, 87%], loss is 1.509936\n",
      "Training Epoch: 345, [350/560, 84%], loss is 1.816441\n",
      "Training Epoch: 345, [420/560, 87%], loss is 2.500961\n",
      "Training Epoch: 345, [490/560, 90%], loss is 2.827942\n",
      "Training Epoch: 350, [0/560, 96%], loss is 0.172152\n",
      "Training Epoch: 350, [70/560, 94%], loss is 0.338704\n",
      "Training Epoch: 350, [140/560, 89%], loss is 0.730979\n",
      "Training Epoch: 350, [210/560, 93%], loss is 0.925056\n",
      "Training Epoch: 350, [280/560, 91%], loss is 1.105815\n",
      "Training Epoch: 350, [350/560, 90%], loss is 1.313576\n",
      "Training Epoch: 350, [420/560, 89%], loss is 1.657082\n",
      "Training Epoch: 350, [490/560, 90%], loss is 1.943285\n",
      "Training Epoch: 355, [0/560, 99%], loss is 0.080131\n",
      "Best Epoch is 355\n",
      "Training Epoch: 355, [70/560, 89%], loss is 0.269893\n",
      "Training Epoch: 355, [140/560, 83%], loss is 0.549634\n",
      "Training Epoch: 355, [210/560, 96%], loss is 0.699810\n",
      "Training Epoch: 355, [280/560, 91%], loss is 0.850154\n",
      "Training Epoch: 355, [350/560, 91%], loss is 1.064369\n",
      "Training Epoch: 355, [420/560, 90%], loss is 1.242351\n",
      "Training Epoch: 355, [490/560, 91%], loss is 1.442529\n",
      "Training Epoch: 360, [0/560, 94%], loss is 0.145051\n",
      "Training Epoch: 360, [70/560, 94%], loss is 0.263732\n",
      "Training Epoch: 360, [140/560, 87%], loss is 0.606887\n",
      "Training Epoch: 360, [210/560, 96%], loss is 0.773539\n",
      "Training Epoch: 360, [280/560, 97%], loss is 0.934997\n",
      "Training Epoch: 360, [350/560, 93%], loss is 1.080486\n",
      "Training Epoch: 360, [420/560, 94%], loss is 1.225490\n",
      "Training Epoch: 360, [490/560, 96%], loss is 1.468698\n",
      "Training Epoch: 365, [0/560, 94%], loss is 0.125270\n",
      "Training Epoch: 365, [70/560, 96%], loss is 0.218652\n",
      "Training Epoch: 365, [140/560, 90%], loss is 0.476126\n",
      "Training Epoch: 365, [210/560, 91%], loss is 0.657059\n",
      "Training Epoch: 365, [280/560, 93%], loss is 0.821706\n",
      "Training Epoch: 365, [350/560, 96%], loss is 0.934828\n",
      "Training Epoch: 365, [420/560, 93%], loss is 1.124495\n",
      "Training Epoch: 365, [490/560, 94%], loss is 1.265514\n",
      "Training Epoch: 370, [0/560, 97%], loss is 0.096085\n",
      "Training Epoch: 370, [70/560, 96%], loss is 0.250705\n",
      "Training Epoch: 370, [140/560, 97%], loss is 0.361678\n",
      "Training Epoch: 370, [210/560, 91%], loss is 0.620591\n",
      "Training Epoch: 370, [280/560, 93%], loss is 0.783569\n",
      "Training Epoch: 370, [350/560, 97%], loss is 0.866092\n",
      "Training Epoch: 370, [420/560, 89%], loss is 1.136709\n",
      "Training Epoch: 370, [490/560, 94%], loss is 1.289463\n",
      "Training Epoch: 375, [0/560, 91%], loss is 0.180617\n",
      "Training Epoch: 375, [70/560, 94%], loss is 0.318263\n",
      "Training Epoch: 375, [140/560, 89%], loss is 0.563032\n",
      "Training Epoch: 375, [210/560, 99%], loss is 0.656361\n",
      "Training Epoch: 375, [280/560, 93%], loss is 0.837586\n",
      "Training Epoch: 375, [350/560, 94%], loss is 0.981752\n",
      "Training Epoch: 375, [420/560, 93%], loss is 1.172991\n",
      "Training Epoch: 375, [490/560, 93%], loss is 1.362972\n",
      "Training Epoch: 380, [0/560, 91%], loss is 0.194952\n",
      "Training Epoch: 380, [70/560, 96%], loss is 0.338978\n",
      "Training Epoch: 380, [140/560, 94%], loss is 0.515810\n",
      "Training Epoch: 380, [210/560, 97%], loss is 0.623566\n",
      "Training Epoch: 380, [280/560, 99%], loss is 0.735870\n",
      "Training Epoch: 380, [350/560, 97%], loss is 0.886197\n",
      "Training Epoch: 380, [420/560, 94%], loss is 1.046802\n",
      "Training Epoch: 380, [490/560, 93%], loss is 1.237909\n",
      "Training Epoch: 385, [0/560, 96%], loss is 0.090946\n",
      "Training Epoch: 385, [70/560, 97%], loss is 0.194688\n",
      "Training Epoch: 385, [140/560, 97%], loss is 0.279624\n",
      "Training Epoch: 385, [210/560, 91%], loss is 0.445713\n",
      "Training Epoch: 385, [280/560, 97%], loss is 0.561149\n",
      "Training Epoch: 385, [350/560, 94%], loss is 0.712471\n",
      "Training Epoch: 385, [420/560, 99%], loss is 0.803437\n",
      "Training Epoch: 385, [490/560, 94%], loss is 0.973331\n",
      "Training Epoch: 390, [0/560, 96%], loss is 0.157069\n",
      "Training Epoch: 390, [70/560, 91%], loss is 0.330614\n",
      "Training Epoch: 390, [140/560, 94%], loss is 0.455052\n",
      "Training Epoch: 390, [210/560, 91%], loss is 0.623937\n",
      "Training Epoch: 390, [280/560, 97%], loss is 0.759582\n",
      "Training Epoch: 390, [350/560, 91%], loss is 0.928721\n",
      "Training Epoch: 390, [420/560, 96%], loss is 1.072711\n",
      "Training Epoch: 390, [490/560, 94%], loss is 1.221692\n",
      "Training Epoch: 395, [0/560, 94%], loss is 0.124172\n",
      "Training Epoch: 395, [70/560, 90%], loss is 0.304427\n",
      "Training Epoch: 395, [140/560, 93%], loss is 0.470098\n",
      "Training Epoch: 395, [210/560, 94%], loss is 0.658428\n",
      "Training Epoch: 395, [280/560, 87%], loss is 0.915353\n",
      "Training Epoch: 395, [350/560, 97%], loss is 1.024933\n",
      "Training Epoch: 395, [420/560, 99%], loss is 1.118808\n",
      "Training Epoch: 395, [490/560, 96%], loss is 1.213032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 400, [0/560, 94%], loss is 0.140933\n",
      "Training Epoch: 400, [70/560, 93%], loss is 0.262699\n",
      "Training Epoch: 400, [140/560, 90%], loss is 0.474874\n",
      "Training Epoch: 400, [210/560, 94%], loss is 0.643344\n",
      "Training Epoch: 400, [280/560, 97%], loss is 0.761692\n",
      "Training Epoch: 400, [350/560, 97%], loss is 0.970055\n",
      "Training Epoch: 400, [420/560, 93%], loss is 1.144382\n",
      "Training Epoch: 400, [490/560, 93%], loss is 1.329769\n",
      "Training Epoch: 405, [0/560, 93%], loss is 0.135032\n",
      "Training Epoch: 405, [70/560, 91%], loss is 0.392291\n",
      "Training Epoch: 405, [140/560, 90%], loss is 0.765467\n",
      "Training Epoch: 405, [210/560, 94%], loss is 0.882062\n",
      "Training Epoch: 405, [280/560, 99%], loss is 1.010079\n",
      "Training Epoch: 405, [350/560, 81%], loss is 1.622313\n",
      "Training Epoch: 405, [420/560, 93%], loss is 1.794975\n",
      "Training Epoch: 405, [490/560, 93%], loss is 1.957394\n",
      "Training Epoch: 410, [0/560, 86%], loss is 0.377354\n",
      "Training Epoch: 410, [70/560, 91%], loss is 0.601555\n",
      "Training Epoch: 410, [140/560, 97%], loss is 0.751395\n",
      "Training Epoch: 410, [210/560, 93%], loss is 1.001678\n",
      "Training Epoch: 410, [280/560, 90%], loss is 1.395387\n",
      "Training Epoch: 410, [350/560, 90%], loss is 1.740318\n",
      "Training Epoch: 410, [420/560, 91%], loss is 1.965167\n",
      "Training Epoch: 410, [490/560, 91%], loss is 2.200773\n",
      "Training Epoch: 415, [0/560, 90%], loss is 0.423962\n",
      "Training Epoch: 415, [70/560, 87%], loss is 0.701402\n",
      "Training Epoch: 415, [140/560, 94%], loss is 0.856339\n",
      "Training Epoch: 415, [210/560, 96%], loss is 1.013126\n",
      "Training Epoch: 415, [280/560, 91%], loss is 1.237107\n",
      "Training Epoch: 415, [350/560, 93%], loss is 1.411234\n",
      "Training Epoch: 415, [420/560, 94%], loss is 1.602321\n",
      "Training Epoch: 415, [490/560, 96%], loss is 1.710364\n",
      "Training Epoch: 420, [0/560, 84%], loss is 0.280066\n",
      "Training Epoch: 420, [70/560, 93%], loss is 0.449281\n",
      "Training Epoch: 420, [140/560, 93%], loss is 0.599993\n",
      "Training Epoch: 420, [210/560, 89%], loss is 0.892515\n",
      "Training Epoch: 420, [280/560, 93%], loss is 1.077319\n",
      "Training Epoch: 420, [350/560, 91%], loss is 1.315141\n",
      "Training Epoch: 420, [420/560, 90%], loss is 1.573916\n",
      "Training Epoch: 420, [490/560, 83%], loss is 1.915896\n",
      "Training Epoch: 425, [0/560, 93%], loss is 0.163558\n",
      "Training Epoch: 425, [70/560, 91%], loss is 0.430916\n",
      "Training Epoch: 425, [140/560, 87%], loss is 0.806003\n",
      "Training Epoch: 425, [210/560, 94%], loss is 0.976114\n",
      "Training Epoch: 425, [280/560, 89%], loss is 1.165728\n",
      "Training Epoch: 425, [350/560, 94%], loss is 1.315614\n",
      "Training Epoch: 425, [420/560, 96%], loss is 1.488839\n",
      "Training Epoch: 425, [490/560, 94%], loss is 1.710535\n",
      "Training Epoch: 430, [0/560, 94%], loss is 0.132248\n",
      "Training Epoch: 430, [70/560, 94%], loss is 0.284353\n",
      "Training Epoch: 430, [140/560, 96%], loss is 0.457157\n",
      "Training Epoch: 430, [210/560, 94%], loss is 0.575911\n",
      "Training Epoch: 430, [280/560, 94%], loss is 0.747496\n",
      "Training Epoch: 430, [350/560, 94%], loss is 0.863687\n",
      "Training Epoch: 430, [420/560, 96%], loss is 1.021836\n",
      "Training Epoch: 430, [490/560, 94%], loss is 1.181046\n",
      "Training Epoch: 435, [0/560, 96%], loss is 0.158459\n",
      "Training Epoch: 435, [70/560, 94%], loss is 0.354842\n",
      "Training Epoch: 435, [140/560, 94%], loss is 0.517055\n",
      "Training Epoch: 435, [210/560, 94%], loss is 0.668080\n",
      "Training Epoch: 435, [280/560, 91%], loss is 0.910521\n",
      "Training Epoch: 435, [350/560, 93%], loss is 1.029087\n",
      "Training Epoch: 435, [420/560, 89%], loss is 1.279972\n",
      "Training Epoch: 435, [490/560, 87%], loss is 1.622772\n",
      "Training Epoch: 440, [0/560, 80%], loss is 0.506734\n",
      "Training Epoch: 440, [70/560, 90%], loss is 1.121578\n",
      "Training Epoch: 440, [140/560, 84%], loss is 1.842826\n",
      "Training Epoch: 440, [210/560, 96%], loss is 2.002976\n",
      "Training Epoch: 440, [280/560, 89%], loss is 2.205875\n",
      "Training Epoch: 440, [350/560, 91%], loss is 2.391175\n",
      "Training Epoch: 440, [420/560, 89%], loss is 2.843099\n",
      "Training Epoch: 440, [490/560, 89%], loss is 3.119609\n",
      "Training Epoch: 445, [0/560, 91%], loss is 0.194976\n",
      "Training Epoch: 445, [70/560, 90%], loss is 0.383242\n",
      "Training Epoch: 445, [140/560, 91%], loss is 0.601173\n",
      "Training Epoch: 445, [210/560, 91%], loss is 0.819876\n",
      "Training Epoch: 445, [280/560, 94%], loss is 0.986766\n",
      "Training Epoch: 445, [350/560, 90%], loss is 1.290415\n",
      "Training Epoch: 445, [420/560, 89%], loss is 1.807136\n",
      "Training Epoch: 445, [490/560, 94%], loss is 2.067811\n",
      "Training Epoch: 450, [0/560, 97%], loss is 0.099509\n",
      "Training Epoch: 450, [70/560, 94%], loss is 0.252687\n",
      "Training Epoch: 450, [140/560, 91%], loss is 0.545253\n",
      "Training Epoch: 450, [210/560, 94%], loss is 0.754113\n",
      "Training Epoch: 450, [280/560, 94%], loss is 0.857371\n",
      "Training Epoch: 450, [350/560, 96%], loss is 0.972689\n",
      "Training Epoch: 450, [420/560, 96%], loss is 1.070378\n",
      "Training Epoch: 450, [490/560, 90%], loss is 1.268350\n",
      "Training Epoch: 455, [0/560, 91%], loss is 0.191495\n",
      "Training Epoch: 455, [70/560, 97%], loss is 0.291361\n",
      "Training Epoch: 455, [140/560, 91%], loss is 0.461581\n",
      "Training Epoch: 455, [210/560, 94%], loss is 0.590444\n",
      "Training Epoch: 455, [280/560, 99%], loss is 0.656233\n",
      "Training Epoch: 455, [350/560, 90%], loss is 0.884496\n",
      "Training Epoch: 455, [420/560, 97%], loss is 0.996980\n",
      "Training Epoch: 455, [490/560, 99%], loss is 1.093231\n",
      "Training Epoch: 460, [0/560, 97%], loss is 0.088797\n",
      "Training Epoch: 460, [70/560, 93%], loss is 0.196982\n",
      "Training Epoch: 460, [140/560, 93%], loss is 0.372426\n",
      "Training Epoch: 460, [210/560, 93%], loss is 0.508711\n",
      "Training Epoch: 460, [280/560, 93%], loss is 0.704812\n",
      "Training Epoch: 460, [350/560, 96%], loss is 0.920373\n",
      "Training Epoch: 460, [420/560, 97%], loss is 1.028887\n",
      "Training Epoch: 460, [490/560, 96%], loss is 1.174082\n",
      "Training Epoch: 465, [0/560, 90%], loss is 0.175302\n",
      "Training Epoch: 465, [70/560, 89%], loss is 0.472117\n",
      "Training Epoch: 465, [140/560, 99%], loss is 0.538875\n",
      "Training Epoch: 465, [210/560, 97%], loss is 0.650095\n",
      "Training Epoch: 465, [280/560, 91%], loss is 0.769434\n",
      "Training Epoch: 465, [350/560, 90%], loss is 1.341162\n",
      "Training Epoch: 465, [420/560, 96%], loss is 1.443794\n",
      "Training Epoch: 465, [490/560, 93%], loss is 1.803760\n",
      "Training Epoch: 470, [0/560, 93%], loss is 0.123636\n",
      "Training Epoch: 470, [70/560, 86%], loss is 0.391574\n",
      "Training Epoch: 470, [140/560, 97%], loss is 0.514246\n",
      "Training Epoch: 470, [210/560, 91%], loss is 0.765548\n",
      "Training Epoch: 470, [280/560, 96%], loss is 0.850476\n",
      "Training Epoch: 470, [350/560, 91%], loss is 1.015211\n",
      "Training Epoch: 470, [420/560, 89%], loss is 1.368154\n",
      "Training Epoch: 470, [490/560, 90%], loss is 1.612129\n",
      "Training Epoch: 475, [0/560, 96%], loss is 0.113216\n",
      "Training Epoch: 475, [70/560, 91%], loss is 0.368621\n",
      "Training Epoch: 475, [140/560, 96%], loss is 0.482659\n",
      "Training Epoch: 475, [210/560, 91%], loss is 0.735569\n",
      "Training Epoch: 475, [280/560, 94%], loss is 0.936033\n",
      "Training Epoch: 475, [350/560, 97%], loss is 1.018356\n",
      "Training Epoch: 475, [420/560, 91%], loss is 1.186228\n",
      "Training Epoch: 475, [490/560, 93%], loss is 1.433248\n",
      "Training Epoch: 480, [0/560, 96%], loss is 0.114298\n",
      "Training Epoch: 480, [70/560, 99%], loss is 0.180120\n",
      "Training Epoch: 480, [140/560, 97%], loss is 0.258671\n",
      "Training Epoch: 480, [210/560, 96%], loss is 0.404327\n",
      "Training Epoch: 480, [280/560, 96%], loss is 0.518823\n",
      "Training Epoch: 480, [350/560, 91%], loss is 0.650631\n",
      "Training Epoch: 480, [420/560, 96%], loss is 0.708573\n",
      "Training Epoch: 480, [490/560, 96%], loss is 0.843096\n",
      "Training Epoch: 485, [0/560, 94%], loss is 0.164692\n",
      "Training Epoch: 485, [70/560, 89%], loss is 0.404066\n",
      "Training Epoch: 485, [140/560, 94%], loss is 0.498692\n",
      "Training Epoch: 485, [210/560, 97%], loss is 0.550162\n",
      "Training Epoch: 485, [280/560, 99%], loss is 0.653959\n",
      "Training Epoch: 485, [350/560, 87%], loss is 0.987496\n",
      "Training Epoch: 485, [420/560, 93%], loss is 1.179110\n",
      "Training Epoch: 485, [490/560, 89%], loss is 1.624686\n",
      "Training Epoch: 490, [0/560, 91%], loss is 0.191023\n",
      "Training Epoch: 490, [70/560, 97%], loss is 0.277365\n",
      "Training Epoch: 490, [140/560, 93%], loss is 0.412278\n",
      "Training Epoch: 490, [210/560, 93%], loss is 0.589763\n",
      "Training Epoch: 490, [280/560, 94%], loss is 0.723759\n",
      "Training Epoch: 490, [350/560, 97%], loss is 0.812674\n",
      "Training Epoch: 490, [420/560, 94%], loss is 1.005054\n",
      "Training Epoch: 490, [490/560, 96%], loss is 1.143667\n",
      "Training Epoch: 495, [0/560, 93%], loss is 0.139261\n",
      "Training Epoch: 495, [70/560, 94%], loss is 0.267660\n",
      "Training Epoch: 495, [140/560, 91%], loss is 0.457564\n",
      "Training Epoch: 495, [210/560, 96%], loss is 0.575789\n",
      "Training Epoch: 495, [280/560, 96%], loss is 0.738542\n",
      "Training Epoch: 495, [350/560, 91%], loss is 0.999979\n",
      "Training Epoch: 495, [420/560, 87%], loss is 1.348231\n",
      "Training Epoch: 495, [490/560, 96%], loss is 1.475726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 500, [0/560, 84%], loss is 0.399232\n",
      "Training Epoch: 500, [70/560, 94%], loss is 0.530797\n",
      "Training Epoch: 500, [140/560, 91%], loss is 0.887626\n",
      "Training Epoch: 500, [210/560, 90%], loss is 1.182354\n",
      "Training Epoch: 500, [280/560, 93%], loss is 1.399531\n",
      "Training Epoch: 500, [350/560, 94%], loss is 1.565223\n",
      "Training Epoch: 500, [420/560, 91%], loss is 1.780616\n",
      "Training Epoch: 500, [490/560, 90%], loss is 2.178031\n",
      "Training Epoch: 505, [0/560, 90%], loss is 0.247169\n",
      "Training Epoch: 505, [70/560, 91%], loss is 0.556859\n",
      "Training Epoch: 505, [140/560, 94%], loss is 0.683827\n",
      "Training Epoch: 505, [210/560, 97%], loss is 0.810587\n",
      "Training Epoch: 505, [280/560, 94%], loss is 0.957129\n",
      "Training Epoch: 505, [350/560, 97%], loss is 1.102115\n",
      "Training Epoch: 505, [420/560, 89%], loss is 1.362432\n",
      "Training Epoch: 505, [490/560, 96%], loss is 1.456164\n",
      "Training Epoch: 510, [0/560, 96%], loss is 0.109780\n",
      "Training Epoch: 510, [70/560, 94%], loss is 0.278044\n",
      "Training Epoch: 510, [140/560, 93%], loss is 0.458383\n",
      "Training Epoch: 510, [210/560, 94%], loss is 0.547488\n",
      "Training Epoch: 510, [280/560, 97%], loss is 0.663370\n",
      "Training Epoch: 510, [350/560, 96%], loss is 0.836250\n",
      "Training Epoch: 510, [420/560, 93%], loss is 0.973939\n",
      "Training Epoch: 510, [490/560, 96%], loss is 1.127351\n",
      "Training Epoch: 515, [0/560, 91%], loss is 0.266023\n",
      "Training Epoch: 515, [70/560, 96%], loss is 0.376532\n",
      "Training Epoch: 515, [140/560, 93%], loss is 0.590777\n",
      "Training Epoch: 515, [210/560, 91%], loss is 0.711962\n",
      "Training Epoch: 515, [280/560, 100%], loss is 0.758144\n",
      "Training Epoch: 515, [350/560, 91%], loss is 0.942264\n",
      "Training Epoch: 515, [420/560, 94%], loss is 1.098625\n",
      "Training Epoch: 515, [490/560, 91%], loss is 1.285999\n",
      "Training Epoch: 520, [0/560, 94%], loss is 0.122197\n",
      "Training Epoch: 520, [70/560, 93%], loss is 0.307262\n",
      "Training Epoch: 520, [140/560, 94%], loss is 0.442215\n",
      "Training Epoch: 520, [210/560, 99%], loss is 0.515180\n",
      "Training Epoch: 520, [280/560, 96%], loss is 0.595387\n",
      "Training Epoch: 520, [350/560, 93%], loss is 0.837127\n",
      "Training Epoch: 520, [420/560, 96%], loss is 0.960831\n",
      "Training Epoch: 520, [490/560, 97%], loss is 1.042313\n",
      "Training Epoch: 525, [0/560, 91%], loss is 0.173093\n",
      "Training Epoch: 525, [70/560, 93%], loss is 0.387274\n",
      "Training Epoch: 525, [140/560, 91%], loss is 0.585166\n",
      "Training Epoch: 525, [210/560, 96%], loss is 0.792280\n",
      "Training Epoch: 525, [280/560, 96%], loss is 0.923660\n",
      "Training Epoch: 525, [350/560, 99%], loss is 0.995526\n",
      "Training Epoch: 525, [420/560, 91%], loss is 1.177349\n",
      "Training Epoch: 525, [490/560, 93%], loss is 1.302593\n",
      "Training Epoch: 530, [0/560, 93%], loss is 0.141118\n",
      "Training Epoch: 530, [70/560, 97%], loss is 0.260882\n",
      "Training Epoch: 530, [140/560, 96%], loss is 0.358241\n",
      "Training Epoch: 530, [210/560, 96%], loss is 0.446100\n",
      "Training Epoch: 530, [280/560, 96%], loss is 0.536621\n",
      "Training Epoch: 530, [350/560, 90%], loss is 0.877201\n",
      "Training Epoch: 530, [420/560, 96%], loss is 0.971916\n",
      "Training Epoch: 530, [490/560, 93%], loss is 1.129935\n",
      "Training Epoch: 535, [0/560, 89%], loss is 0.360599\n",
      "Training Epoch: 535, [70/560, 94%], loss is 0.494972\n",
      "Training Epoch: 535, [140/560, 90%], loss is 0.758455\n",
      "Training Epoch: 535, [210/560, 89%], loss is 1.035750\n",
      "Training Epoch: 535, [280/560, 93%], loss is 1.273620\n",
      "Training Epoch: 535, [350/560, 89%], loss is 1.486390\n",
      "Training Epoch: 535, [420/560, 99%], loss is 1.575181\n",
      "Training Epoch: 535, [490/560, 87%], loss is 1.832848\n",
      "Training Epoch: 540, [0/560, 97%], loss is 0.159706\n",
      "Training Epoch: 540, [70/560, 94%], loss is 0.291286\n",
      "Training Epoch: 540, [140/560, 94%], loss is 0.482084\n",
      "Training Epoch: 540, [210/560, 91%], loss is 0.682588\n",
      "Training Epoch: 540, [280/560, 90%], loss is 0.953890\n",
      "Training Epoch: 540, [350/560, 97%], loss is 1.051931\n",
      "Training Epoch: 540, [420/560, 93%], loss is 1.205200\n",
      "Training Epoch: 540, [490/560, 94%], loss is 1.346637\n",
      "Training Epoch: 545, [0/560, 97%], loss is 0.078942\n",
      "Best Epoch is 545\n",
      "Training Epoch: 545, [70/560, 90%], loss is 0.311867\n",
      "Training Epoch: 545, [140/560, 97%], loss is 0.407367\n",
      "Training Epoch: 545, [210/560, 96%], loss is 0.529863\n",
      "Training Epoch: 545, [280/560, 94%], loss is 0.688851\n",
      "Training Epoch: 545, [350/560, 97%], loss is 0.779889\n",
      "Training Epoch: 545, [420/560, 96%], loss is 0.957496\n",
      "Training Epoch: 545, [490/560, 86%], loss is 1.362331\n",
      "Training Epoch: 550, [0/560, 93%], loss is 0.173846\n",
      "Training Epoch: 550, [70/560, 94%], loss is 0.323184\n",
      "Training Epoch: 550, [140/560, 94%], loss is 0.470030\n",
      "Training Epoch: 550, [210/560, 100%], loss is 0.508860\n",
      "Training Epoch: 550, [280/560, 90%], loss is 0.715283\n",
      "Training Epoch: 550, [350/560, 80%], loss is 1.233707\n",
      "Training Epoch: 550, [420/560, 87%], loss is 1.627202\n",
      "Training Epoch: 550, [490/560, 91%], loss is 1.881657\n",
      "Training Epoch: 555, [0/560, 96%], loss is 0.153679\n",
      "Training Epoch: 555, [70/560, 96%], loss is 0.277616\n",
      "Training Epoch: 555, [140/560, 97%], loss is 0.355522\n",
      "Training Epoch: 555, [210/560, 89%], loss is 0.575951\n",
      "Training Epoch: 555, [280/560, 93%], loss is 0.794254\n",
      "Training Epoch: 555, [350/560, 96%], loss is 0.951606\n",
      "Training Epoch: 555, [420/560, 93%], loss is 1.131442\n",
      "Training Epoch: 555, [490/560, 97%], loss is 1.189932\n",
      "Training Epoch: 560, [0/560, 97%], loss is 0.108739\n",
      "Training Epoch: 560, [70/560, 97%], loss is 0.196833\n",
      "Training Epoch: 560, [140/560, 93%], loss is 0.335051\n",
      "Training Epoch: 560, [210/560, 94%], loss is 0.514826\n",
      "Training Epoch: 560, [280/560, 99%], loss is 0.581580\n",
      "Training Epoch: 560, [350/560, 99%], loss is 0.639840\n",
      "Training Epoch: 560, [420/560, 93%], loss is 0.807369\n",
      "Training Epoch: 560, [490/560, 96%], loss is 0.887141\n",
      "Training Epoch: 565, [0/560, 96%], loss is 0.097980\n",
      "Training Epoch: 565, [70/560, 97%], loss is 0.205422\n",
      "Training Epoch: 565, [140/560, 99%], loss is 0.265532\n",
      "Training Epoch: 565, [210/560, 99%], loss is 0.358395\n",
      "Training Epoch: 565, [280/560, 96%], loss is 0.453851\n",
      "Training Epoch: 565, [350/560, 97%], loss is 0.602600\n",
      "Training Epoch: 565, [420/560, 99%], loss is 0.716953\n",
      "Training Epoch: 565, [490/560, 93%], loss is 0.887671\n",
      "Training Epoch: 570, [0/560, 97%], loss is 0.097497\n",
      "Training Epoch: 570, [70/560, 91%], loss is 0.240008\n",
      "Training Epoch: 570, [140/560, 97%], loss is 0.352296\n",
      "Training Epoch: 570, [210/560, 91%], loss is 0.580748\n",
      "Training Epoch: 570, [280/560, 91%], loss is 0.774155\n",
      "Training Epoch: 570, [350/560, 93%], loss is 0.949224\n",
      "Training Epoch: 570, [420/560, 97%], loss is 1.034336\n",
      "Training Epoch: 570, [490/560, 93%], loss is 1.162080\n",
      "Training Epoch: 575, [0/560, 94%], loss is 0.121781\n",
      "Training Epoch: 575, [70/560, 96%], loss is 0.220136\n",
      "Training Epoch: 575, [140/560, 94%], loss is 0.342082\n",
      "Training Epoch: 575, [210/560, 94%], loss is 0.470734\n",
      "Training Epoch: 575, [280/560, 97%], loss is 0.579436\n",
      "Training Epoch: 575, [350/560, 94%], loss is 0.759738\n",
      "Training Epoch: 575, [420/560, 97%], loss is 0.864913\n",
      "Training Epoch: 575, [490/560, 91%], loss is 1.044334\n",
      "Training Epoch: 580, [0/560, 96%], loss is 0.166969\n",
      "Training Epoch: 580, [70/560, 97%], loss is 0.249821\n",
      "Training Epoch: 580, [140/560, 96%], loss is 0.350456\n",
      "Training Epoch: 580, [210/560, 93%], loss is 0.465731\n",
      "Training Epoch: 580, [280/560, 97%], loss is 0.557818\n",
      "Training Epoch: 580, [350/560, 96%], loss is 0.668883\n",
      "Training Epoch: 580, [420/560, 94%], loss is 0.793102\n",
      "Training Epoch: 580, [490/560, 90%], loss is 1.007594\n",
      "Training Epoch: 585, [0/560, 90%], loss is 0.225233\n",
      "Training Epoch: 585, [70/560, 90%], loss is 0.641431\n",
      "Training Epoch: 585, [140/560, 94%], loss is 0.826444\n",
      "Training Epoch: 585, [210/560, 90%], loss is 1.270654\n",
      "Training Epoch: 585, [280/560, 87%], loss is 1.671326\n",
      "Training Epoch: 585, [350/560, 91%], loss is 1.983804\n",
      "Training Epoch: 585, [420/560, 86%], loss is 2.314884\n",
      "Training Epoch: 585, [490/560, 93%], loss is 2.456014\n",
      "Training Epoch: 590, [0/560, 97%], loss is 0.083752\n",
      "Training Epoch: 590, [70/560, 93%], loss is 0.242996\n",
      "Training Epoch: 590, [140/560, 97%], loss is 0.342156\n",
      "Training Epoch: 590, [210/560, 96%], loss is 0.441243\n",
      "Training Epoch: 590, [280/560, 94%], loss is 0.617485\n",
      "Training Epoch: 590, [350/560, 89%], loss is 0.830748\n",
      "Training Epoch: 590, [420/560, 90%], loss is 1.068897\n",
      "Training Epoch: 590, [490/560, 96%], loss is 1.201972\n",
      "Training Epoch: 595, [0/560, 93%], loss is 0.179694\n",
      "Training Epoch: 595, [70/560, 93%], loss is 0.342609\n",
      "Training Epoch: 595, [140/560, 94%], loss is 0.585282\n",
      "Training Epoch: 595, [210/560, 99%], loss is 0.672309\n",
      "Training Epoch: 595, [280/560, 96%], loss is 0.814441\n",
      "Training Epoch: 595, [350/560, 96%], loss is 1.147115\n",
      "Training Epoch: 595, [420/560, 91%], loss is 1.449591\n",
      "Training Epoch: 595, [490/560, 94%], loss is 1.569749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 600, [0/560, 96%], loss is 0.121078\n",
      "Training Epoch: 600, [70/560, 94%], loss is 0.326495\n",
      "Training Epoch: 600, [140/560, 93%], loss is 0.453316\n",
      "Training Epoch: 600, [210/560, 90%], loss is 0.707260\n",
      "Training Epoch: 600, [280/560, 93%], loss is 0.878310\n",
      "Training Epoch: 600, [350/560, 87%], loss is 1.113825\n",
      "Training Epoch: 600, [420/560, 96%], loss is 1.304042\n",
      "Training Epoch: 600, [490/560, 93%], loss is 1.559476\n",
      "Training Epoch: 605, [0/560, 91%], loss is 0.207173\n",
      "Training Epoch: 605, [70/560, 90%], loss is 0.423522\n",
      "Training Epoch: 605, [140/560, 90%], loss is 0.660945\n",
      "Training Epoch: 605, [210/560, 90%], loss is 0.861916\n",
      "Training Epoch: 605, [280/560, 91%], loss is 1.035380\n",
      "Training Epoch: 605, [350/560, 93%], loss is 1.205006\n",
      "Training Epoch: 605, [420/560, 96%], loss is 1.304453\n",
      "Training Epoch: 605, [490/560, 93%], loss is 1.517797\n",
      "Training Epoch: 610, [0/560, 99%], loss is 0.048408\n",
      "Best Epoch is 610\n",
      "Training Epoch: 610, [70/560, 91%], loss is 0.227070\n",
      "Training Epoch: 610, [140/560, 94%], loss is 0.373763\n",
      "Training Epoch: 610, [210/560, 93%], loss is 0.529697\n",
      "Training Epoch: 610, [280/560, 91%], loss is 0.693806\n",
      "Training Epoch: 610, [350/560, 93%], loss is 0.870091\n",
      "Training Epoch: 610, [420/560, 96%], loss is 0.996021\n",
      "Training Epoch: 610, [490/560, 96%], loss is 1.081743\n",
      "Training Epoch: 615, [0/560, 93%], loss is 0.161695\n",
      "Training Epoch: 615, [70/560, 93%], loss is 0.310381\n",
      "Training Epoch: 615, [140/560, 96%], loss is 0.399988\n",
      "Training Epoch: 615, [210/560, 94%], loss is 0.537466\n",
      "Training Epoch: 615, [280/560, 94%], loss is 0.694460\n",
      "Training Epoch: 615, [350/560, 93%], loss is 0.833816\n",
      "Training Epoch: 615, [420/560, 93%], loss is 1.021845\n",
      "Training Epoch: 615, [490/560, 94%], loss is 1.111277\n",
      "Training Epoch: 620, [0/560, 90%], loss is 0.312953\n",
      "Training Epoch: 620, [70/560, 93%], loss is 0.521650\n",
      "Training Epoch: 620, [140/560, 96%], loss is 0.709704\n",
      "Training Epoch: 620, [210/560, 94%], loss is 0.837677\n",
      "Training Epoch: 620, [280/560, 99%], loss is 0.908629\n",
      "Training Epoch: 620, [350/560, 96%], loss is 1.016072\n",
      "Training Epoch: 620, [420/560, 93%], loss is 1.156394\n",
      "Training Epoch: 620, [490/560, 90%], loss is 1.383996\n",
      "Training Epoch: 625, [0/560, 86%], loss is 0.268384\n",
      "Training Epoch: 625, [70/560, 96%], loss is 0.417901\n",
      "Training Epoch: 625, [140/560, 94%], loss is 0.611980\n",
      "Training Epoch: 625, [210/560, 91%], loss is 0.848363\n",
      "Training Epoch: 625, [280/560, 90%], loss is 1.047715\n",
      "Training Epoch: 625, [350/560, 94%], loss is 1.194882\n",
      "Training Epoch: 625, [420/560, 93%], loss is 1.404468\n",
      "Training Epoch: 625, [490/560, 90%], loss is 1.594945\n",
      "Training Epoch: 630, [0/560, 99%], loss is 0.192200\n",
      "Training Epoch: 630, [70/560, 89%], loss is 0.559087\n",
      "Training Epoch: 630, [140/560, 90%], loss is 0.874025\n",
      "Training Epoch: 630, [210/560, 93%], loss is 1.040347\n",
      "Training Epoch: 630, [280/560, 93%], loss is 1.238313\n",
      "Training Epoch: 630, [350/560, 76%], loss is 1.722651\n",
      "Training Epoch: 630, [420/560, 93%], loss is 1.947704\n",
      "Training Epoch: 630, [490/560, 90%], loss is 2.267228\n",
      "Training Epoch: 635, [0/560, 91%], loss is 0.148365\n",
      "Training Epoch: 635, [70/560, 87%], loss is 0.393762\n",
      "Training Epoch: 635, [140/560, 93%], loss is 0.600808\n",
      "Training Epoch: 635, [210/560, 96%], loss is 0.714131\n",
      "Training Epoch: 635, [280/560, 89%], loss is 0.911618\n",
      "Training Epoch: 635, [350/560, 96%], loss is 1.050609\n",
      "Training Epoch: 635, [420/560, 94%], loss is 1.237079\n",
      "Training Epoch: 635, [490/560, 89%], loss is 1.451221\n",
      "Training Epoch: 640, [0/560, 90%], loss is 0.194521\n",
      "Training Epoch: 640, [70/560, 89%], loss is 0.436682\n",
      "Training Epoch: 640, [140/560, 96%], loss is 0.568374\n",
      "Training Epoch: 640, [210/560, 97%], loss is 0.697198\n",
      "Training Epoch: 640, [280/560, 86%], loss is 0.977952\n",
      "Training Epoch: 640, [350/560, 96%], loss is 1.067029\n",
      "Training Epoch: 640, [420/560, 93%], loss is 1.244119\n",
      "Training Epoch: 640, [490/560, 91%], loss is 1.475942\n",
      "Training Epoch: 645, [0/560, 93%], loss is 0.172749\n",
      "Training Epoch: 645, [70/560, 94%], loss is 0.336371\n",
      "Training Epoch: 645, [140/560, 96%], loss is 0.448119\n",
      "Training Epoch: 645, [210/560, 94%], loss is 0.582491\n",
      "Training Epoch: 645, [280/560, 94%], loss is 0.705710\n",
      "Training Epoch: 645, [350/560, 96%], loss is 0.774657\n",
      "Training Epoch: 645, [420/560, 94%], loss is 0.893204\n",
      "Training Epoch: 645, [490/560, 99%], loss is 0.948250\n",
      "Training Epoch: 650, [0/560, 100%], loss is 0.042383\n",
      "Best Epoch is 650\n",
      "Training Epoch: 650, [70/560, 94%], loss is 0.182629\n",
      "Training Epoch: 650, [140/560, 96%], loss is 0.272542\n",
      "Training Epoch: 650, [210/560, 93%], loss is 0.405188\n",
      "Training Epoch: 650, [280/560, 91%], loss is 0.577462\n",
      "Training Epoch: 650, [350/560, 96%], loss is 0.670412\n",
      "Training Epoch: 650, [420/560, 97%], loss is 0.746272\n",
      "Training Epoch: 650, [490/560, 96%], loss is 0.870502\n",
      "Training Epoch: 655, [0/560, 97%], loss is 0.092577\n",
      "Training Epoch: 655, [70/560, 94%], loss is 0.210658\n",
      "Training Epoch: 655, [140/560, 93%], loss is 0.436767\n",
      "Training Epoch: 655, [210/560, 91%], loss is 0.569720\n",
      "Training Epoch: 655, [280/560, 91%], loss is 0.756840\n",
      "Training Epoch: 655, [350/560, 97%], loss is 0.839275\n",
      "Training Epoch: 655, [420/560, 97%], loss is 0.911108\n",
      "Training Epoch: 655, [490/560, 94%], loss is 1.045437\n",
      "Training Epoch: 660, [0/560, 94%], loss is 0.121876\n",
      "Training Epoch: 660, [70/560, 94%], loss is 0.260920\n",
      "Training Epoch: 660, [140/560, 96%], loss is 0.341365\n",
      "Training Epoch: 660, [210/560, 100%], loss is 0.367272\n",
      "Training Epoch: 660, [280/560, 96%], loss is 0.481404\n",
      "Training Epoch: 660, [350/560, 94%], loss is 0.615418\n",
      "Training Epoch: 660, [420/560, 93%], loss is 0.736433\n",
      "Training Epoch: 660, [490/560, 97%], loss is 0.817632\n",
      "Training Epoch: 665, [0/560, 90%], loss is 0.228390\n",
      "Training Epoch: 665, [70/560, 96%], loss is 0.304002\n",
      "Training Epoch: 665, [140/560, 91%], loss is 0.517599\n",
      "Training Epoch: 665, [210/560, 93%], loss is 0.655615\n",
      "Training Epoch: 665, [280/560, 96%], loss is 0.768228\n",
      "Training Epoch: 665, [350/560, 97%], loss is 0.845233\n",
      "Training Epoch: 665, [420/560, 94%], loss is 1.005468\n",
      "Training Epoch: 665, [490/560, 91%], loss is 1.176110\n",
      "Training Epoch: 670, [0/560, 97%], loss is 0.173956\n",
      "Training Epoch: 670, [70/560, 91%], loss is 0.323032\n",
      "Training Epoch: 670, [140/560, 91%], loss is 0.615861\n",
      "Training Epoch: 670, [210/560, 96%], loss is 0.729798\n",
      "Training Epoch: 670, [280/560, 96%], loss is 0.856531\n",
      "Training Epoch: 670, [350/560, 93%], loss is 1.007489\n",
      "Training Epoch: 670, [420/560, 96%], loss is 1.128611\n",
      "Training Epoch: 670, [490/560, 94%], loss is 1.309705\n",
      "Training Epoch: 675, [0/560, 91%], loss is 0.292072\n",
      "Training Epoch: 675, [70/560, 94%], loss is 0.806038\n",
      "Training Epoch: 675, [140/560, 93%], loss is 0.970294\n",
      "Training Epoch: 675, [210/560, 96%], loss is 1.080764\n",
      "Training Epoch: 675, [280/560, 97%], loss is 1.201731\n",
      "Training Epoch: 675, [350/560, 97%], loss is 1.317909\n",
      "Training Epoch: 675, [420/560, 96%], loss is 1.461083\n",
      "Training Epoch: 675, [490/560, 93%], loss is 1.672606\n",
      "Training Epoch: 680, [0/560, 90%], loss is 0.470139\n",
      "Training Epoch: 680, [70/560, 91%], loss is 0.850477\n",
      "Training Epoch: 680, [140/560, 91%], loss is 1.063682\n",
      "Training Epoch: 680, [210/560, 97%], loss is 1.232157\n",
      "Training Epoch: 680, [280/560, 94%], loss is 1.419278\n",
      "Training Epoch: 680, [350/560, 91%], loss is 1.582062\n",
      "Training Epoch: 680, [420/560, 91%], loss is 1.840699\n",
      "Training Epoch: 680, [490/560, 90%], loss is 2.282474\n",
      "Training Epoch: 685, [0/560, 91%], loss is 0.178137\n",
      "Training Epoch: 685, [70/560, 93%], loss is 0.309093\n",
      "Training Epoch: 685, [140/560, 91%], loss is 0.507261\n",
      "Training Epoch: 685, [210/560, 99%], loss is 0.611992\n",
      "Training Epoch: 685, [280/560, 97%], loss is 0.739088\n",
      "Training Epoch: 685, [350/560, 94%], loss is 0.892680\n",
      "Training Epoch: 685, [420/560, 94%], loss is 1.293509\n",
      "Training Epoch: 685, [490/560, 93%], loss is 1.484049\n",
      "Training Epoch: 690, [0/560, 97%], loss is 0.137912\n",
      "Training Epoch: 690, [70/560, 97%], loss is 0.190155\n",
      "Training Epoch: 690, [140/560, 91%], loss is 0.345285\n",
      "Training Epoch: 690, [210/560, 94%], loss is 0.474548\n",
      "Training Epoch: 690, [280/560, 96%], loss is 0.614521\n",
      "Training Epoch: 690, [350/560, 91%], loss is 0.853546\n",
      "Training Epoch: 690, [420/560, 94%], loss is 1.042483\n",
      "Training Epoch: 690, [490/560, 91%], loss is 1.205728\n",
      "Training Epoch: 695, [0/560, 93%], loss is 0.222078\n",
      "Training Epoch: 695, [70/560, 93%], loss is 0.380904\n",
      "Training Epoch: 695, [140/560, 97%], loss is 0.481606\n",
      "Training Epoch: 695, [210/560, 91%], loss is 0.655598\n",
      "Training Epoch: 695, [280/560, 89%], loss is 0.977135\n",
      "Training Epoch: 695, [350/560, 97%], loss is 1.111451\n",
      "Training Epoch: 695, [420/560, 89%], loss is 1.495149\n",
      "Training Epoch: 695, [490/560, 89%], loss is 1.738882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 700, [0/560, 93%], loss is 0.153761\n",
      "Training Epoch: 700, [70/560, 99%], loss is 0.241701\n",
      "Training Epoch: 700, [140/560, 94%], loss is 0.372997\n",
      "Training Epoch: 700, [210/560, 89%], loss is 0.600308\n",
      "Training Epoch: 700, [280/560, 97%], loss is 0.696394\n",
      "Training Epoch: 700, [350/560, 99%], loss is 0.805255\n",
      "Training Epoch: 700, [420/560, 91%], loss is 0.956070\n",
      "Training Epoch: 700, [490/560, 97%], loss is 1.045164\n",
      "Training Epoch: 705, [0/560, 91%], loss is 0.197620\n",
      "Training Epoch: 705, [70/560, 96%], loss is 0.319314\n",
      "Training Epoch: 705, [140/560, 96%], loss is 0.470642\n",
      "Training Epoch: 705, [210/560, 93%], loss is 0.693721\n",
      "Training Epoch: 705, [280/560, 97%], loss is 0.816819\n",
      "Training Epoch: 705, [350/560, 93%], loss is 1.026928\n",
      "Training Epoch: 705, [420/560, 97%], loss is 1.120232\n",
      "Training Epoch: 705, [490/560, 96%], loss is 1.250354\n",
      "Training Epoch: 710, [0/560, 97%], loss is 0.112728\n",
      "Training Epoch: 710, [70/560, 97%], loss is 0.188814\n",
      "Training Epoch: 710, [140/560, 91%], loss is 0.371842\n",
      "Training Epoch: 710, [210/560, 87%], loss is 0.654818\n",
      "Training Epoch: 710, [280/560, 96%], loss is 0.751362\n",
      "Training Epoch: 710, [350/560, 97%], loss is 0.899417\n",
      "Training Epoch: 710, [420/560, 97%], loss is 1.043252\n",
      "Training Epoch: 710, [490/560, 94%], loss is 1.170058\n",
      "Training Epoch: 715, [0/560, 93%], loss is 0.147911\n",
      "Training Epoch: 715, [70/560, 99%], loss is 0.208531\n",
      "Training Epoch: 715, [140/560, 97%], loss is 0.282697\n",
      "Training Epoch: 715, [210/560, 97%], loss is 0.382732\n",
      "Training Epoch: 715, [280/560, 96%], loss is 0.496911\n",
      "Training Epoch: 715, [350/560, 94%], loss is 0.614463\n",
      "Training Epoch: 715, [420/560, 97%], loss is 0.760118\n",
      "Training Epoch: 715, [490/560, 97%], loss is 0.869627\n",
      "Training Epoch: 720, [0/560, 100%], loss is 0.074277\n",
      "Training Epoch: 720, [70/560, 96%], loss is 0.184283\n",
      "Training Epoch: 720, [140/560, 96%], loss is 0.281623\n",
      "Training Epoch: 720, [210/560, 93%], loss is 0.466700\n",
      "Training Epoch: 720, [280/560, 97%], loss is 0.576341\n",
      "Training Epoch: 720, [350/560, 96%], loss is 0.699882\n",
      "Training Epoch: 720, [420/560, 91%], loss is 0.899444\n",
      "Training Epoch: 720, [490/560, 94%], loss is 1.027163\n",
      "Training Epoch: 725, [0/560, 96%], loss is 0.104762\n",
      "Training Epoch: 725, [70/560, 97%], loss is 0.174614\n",
      "Training Epoch: 725, [140/560, 97%], loss is 0.269954\n",
      "Training Epoch: 725, [210/560, 87%], loss is 0.521468\n",
      "Training Epoch: 725, [280/560, 90%], loss is 0.830648\n",
      "Training Epoch: 725, [350/560, 87%], loss is 1.056758\n",
      "Training Epoch: 725, [420/560, 97%], loss is 1.158694\n",
      "Training Epoch: 725, [490/560, 94%], loss is 1.358792\n",
      "Training Epoch: 730, [0/560, 97%], loss is 0.136025\n",
      "Training Epoch: 730, [70/560, 97%], loss is 0.222083\n",
      "Training Epoch: 730, [140/560, 94%], loss is 0.342606\n",
      "Training Epoch: 730, [210/560, 97%], loss is 0.416426\n",
      "Training Epoch: 730, [280/560, 94%], loss is 0.529859\n",
      "Training Epoch: 730, [350/560, 90%], loss is 0.827713\n",
      "Training Epoch: 730, [420/560, 99%], loss is 0.875726\n",
      "Training Epoch: 730, [490/560, 91%], loss is 1.076856\n",
      "Training Epoch: 735, [0/560, 94%], loss is 0.148560\n",
      "Training Epoch: 735, [70/560, 96%], loss is 0.228914\n",
      "Training Epoch: 735, [140/560, 97%], loss is 0.302057\n",
      "Training Epoch: 735, [210/560, 96%], loss is 0.438964\n",
      "Training Epoch: 735, [280/560, 91%], loss is 0.619365\n",
      "Training Epoch: 735, [350/560, 93%], loss is 0.826006\n",
      "Training Epoch: 735, [420/560, 93%], loss is 1.065233\n",
      "Training Epoch: 735, [490/560, 93%], loss is 1.237747\n",
      "Training Epoch: 740, [0/560, 99%], loss is 0.076321\n",
      "Training Epoch: 740, [70/560, 97%], loss is 0.149487\n",
      "Training Epoch: 740, [140/560, 91%], loss is 0.339312\n",
      "Training Epoch: 740, [210/560, 91%], loss is 0.499421\n",
      "Training Epoch: 740, [280/560, 97%], loss is 0.557178\n",
      "Training Epoch: 740, [350/560, 97%], loss is 0.625717\n",
      "Training Epoch: 740, [420/560, 94%], loss is 0.747598\n",
      "Training Epoch: 740, [490/560, 90%], loss is 0.932636\n",
      "Training Epoch: 745, [0/560, 99%], loss is 0.071591\n",
      "Training Epoch: 745, [70/560, 97%], loss is 0.168366\n",
      "Training Epoch: 745, [140/560, 97%], loss is 0.256224\n",
      "Training Epoch: 745, [210/560, 94%], loss is 0.364126\n",
      "Training Epoch: 745, [280/560, 96%], loss is 0.439978\n",
      "Training Epoch: 745, [350/560, 94%], loss is 0.553846\n",
      "Training Epoch: 745, [420/560, 96%], loss is 0.681316\n",
      "Training Epoch: 745, [490/560, 99%], loss is 0.758374\n",
      "Training Epoch: 750, [0/560, 93%], loss is 0.207226\n",
      "Training Epoch: 750, [70/560, 89%], loss is 0.522250\n",
      "Training Epoch: 750, [140/560, 91%], loss is 0.762677\n",
      "Training Epoch: 750, [210/560, 91%], loss is 0.910737\n",
      "Training Epoch: 750, [280/560, 94%], loss is 1.058274\n",
      "Training Epoch: 750, [350/560, 100%], loss is 1.109335\n",
      "Training Epoch: 750, [420/560, 91%], loss is 1.331224\n",
      "Training Epoch: 750, [490/560, 99%], loss is 1.391035\n",
      "Training Epoch: 755, [0/560, 93%], loss is 0.255660\n",
      "Training Epoch: 755, [70/560, 97%], loss is 0.427835\n",
      "Training Epoch: 755, [140/560, 89%], loss is 0.733374\n",
      "Training Epoch: 755, [210/560, 96%], loss is 0.840352\n",
      "Training Epoch: 755, [280/560, 94%], loss is 0.982411\n",
      "Training Epoch: 755, [350/560, 96%], loss is 1.083732\n",
      "Training Epoch: 755, [420/560, 91%], loss is 1.292872\n",
      "Training Epoch: 755, [490/560, 91%], loss is 1.501870\n",
      "Training Epoch: 760, [0/560, 91%], loss is 0.192292\n",
      "Training Epoch: 760, [70/560, 94%], loss is 0.310156\n",
      "Training Epoch: 760, [140/560, 96%], loss is 0.406868\n",
      "Training Epoch: 760, [210/560, 94%], loss is 0.531221\n",
      "Training Epoch: 760, [280/560, 96%], loss is 0.668626\n",
      "Training Epoch: 760, [350/560, 96%], loss is 0.843926\n",
      "Training Epoch: 760, [420/560, 90%], loss is 1.060544\n",
      "Training Epoch: 760, [490/560, 93%], loss is 1.209113\n",
      "Training Epoch: 765, [0/560, 99%], loss is 0.083526\n",
      "Training Epoch: 765, [70/560, 94%], loss is 0.213120\n",
      "Training Epoch: 765, [140/560, 91%], loss is 0.374411\n",
      "Training Epoch: 765, [210/560, 94%], loss is 0.518595\n",
      "Training Epoch: 765, [280/560, 96%], loss is 0.608634\n",
      "Training Epoch: 765, [350/560, 96%], loss is 0.701250\n",
      "Training Epoch: 765, [420/560, 91%], loss is 0.848148\n",
      "Training Epoch: 765, [490/560, 94%], loss is 0.971465\n",
      "Training Epoch: 770, [0/560, 91%], loss is 0.206996\n",
      "Training Epoch: 770, [70/560, 89%], loss is 0.427168\n",
      "Training Epoch: 770, [140/560, 97%], loss is 0.532215\n",
      "Training Epoch: 770, [210/560, 94%], loss is 0.727905\n",
      "Training Epoch: 770, [280/560, 93%], loss is 0.910251\n",
      "Training Epoch: 770, [350/560, 91%], loss is 1.054746\n",
      "Training Epoch: 770, [420/560, 89%], loss is 1.277980\n",
      "Training Epoch: 770, [490/560, 94%], loss is 1.407949\n",
      "Training Epoch: 775, [0/560, 93%], loss is 0.312677\n",
      "Training Epoch: 775, [70/560, 87%], loss is 0.721283\n",
      "Training Epoch: 775, [140/560, 93%], loss is 1.005902\n",
      "Training Epoch: 775, [210/560, 89%], loss is 1.305130\n",
      "Training Epoch: 775, [280/560, 97%], loss is 1.422702\n",
      "Training Epoch: 775, [350/560, 90%], loss is 1.610919\n",
      "Training Epoch: 775, [420/560, 93%], loss is 1.789192\n",
      "Training Epoch: 775, [490/560, 90%], loss is 2.185806\n",
      "Training Epoch: 780, [0/560, 90%], loss is 0.197017\n",
      "Training Epoch: 780, [70/560, 96%], loss is 0.299233\n",
      "Training Epoch: 780, [140/560, 96%], loss is 0.425774\n",
      "Training Epoch: 780, [210/560, 99%], loss is 0.515332\n",
      "Training Epoch: 780, [280/560, 87%], loss is 0.849319\n",
      "Training Epoch: 780, [350/560, 96%], loss is 0.979943\n",
      "Training Epoch: 780, [420/560, 91%], loss is 1.177023\n",
      "Training Epoch: 780, [490/560, 90%], loss is 1.365140\n",
      "Training Epoch: 785, [0/560, 99%], loss is 0.102878\n",
      "Training Epoch: 785, [70/560, 94%], loss is 0.290323\n",
      "Training Epoch: 785, [140/560, 87%], loss is 0.638516\n",
      "Training Epoch: 785, [210/560, 97%], loss is 0.767590\n",
      "Training Epoch: 785, [280/560, 90%], loss is 1.030195\n",
      "Training Epoch: 785, [350/560, 84%], loss is 1.541908\n",
      "Training Epoch: 785, [420/560, 94%], loss is 1.770096\n",
      "Training Epoch: 785, [490/560, 94%], loss is 1.884946\n",
      "Training Epoch: 790, [0/560, 94%], loss is 0.114476\n",
      "Training Epoch: 790, [70/560, 94%], loss is 0.234263\n",
      "Training Epoch: 790, [140/560, 96%], loss is 0.354235\n",
      "Training Epoch: 790, [210/560, 96%], loss is 0.489377\n",
      "Training Epoch: 790, [280/560, 91%], loss is 0.827570\n",
      "Training Epoch: 790, [350/560, 89%], loss is 1.097472\n",
      "Training Epoch: 790, [420/560, 93%], loss is 1.303330\n",
      "Training Epoch: 790, [490/560, 94%], loss is 1.493624\n",
      "Training Epoch: 795, [0/560, 97%], loss is 0.091187\n",
      "Training Epoch: 795, [70/560, 94%], loss is 0.225809\n",
      "Training Epoch: 795, [140/560, 94%], loss is 0.381918\n",
      "Training Epoch: 795, [210/560, 96%], loss is 0.471088\n",
      "Training Epoch: 795, [280/560, 93%], loss is 0.604399\n",
      "Training Epoch: 795, [350/560, 97%], loss is 0.696341\n",
      "Training Epoch: 795, [420/560, 93%], loss is 1.051445\n",
      "Training Epoch: 795, [490/560, 91%], loss is 1.220559\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_loss, train_best  = main(seed, dim_input, dim_hidden, up_limit, down_limit, batch_size, alpha, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_sigPQ_perturb_1\n",
      "Test set results: loss= 0.0893 accuracy= 96.9643 1-hop accuracy = 0.9982\n",
      "testing_sigPQ_perturb_1.5\n",
      "Test set results: loss= 0.3887 accuracy= 88.5714 1-hop accuracy = 0.9643\n",
      "testing_sigPQ_perturb_2\n",
      "Test set results: loss= 1.0563 accuracy= 78.7500 1-hop accuracy = 0.9196\n",
      "testing_sigPQ_perturb_3\n",
      "Test set results: loss= 2.8310 accuracy= 64.6429 1-hop accuracy = 0.8375\n",
      "[[96.96 88.57 78.75 64.64]]\n"
     ]
    }
   ],
   "source": [
    "# Test the performance\n",
    "model_test = Net(dim_input, dim_hidden, nclass)\n",
    "model_test.load_state_dict(torch.load( os.path.join(model_dir,  savename + '.pt')))\n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "acc_list = scenario_test(scenario, w, rootPath, model_test)\n",
    "print(acc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
