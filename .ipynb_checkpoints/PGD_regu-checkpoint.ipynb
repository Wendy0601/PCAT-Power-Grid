{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.005 (vol + cur limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "#import copy\n",
    "from numpy.lib.function_base import copy\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from apex import amp  \n",
    "import numpy as np\n",
    "from scipy import *\n",
    "from numpy import dot, multiply, diag, power, pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv,norm\n",
    "from scipy.linalg import svd, svdvals \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io as sio  \n",
    "import re \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD   \n",
    "import torch.optim as optim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "savename = 'PGD_regu_new'\n",
    "fault_type = 2; # 0--Tp; 1--DLG; 2--LG; 3--LL\n",
    "impe_type = 1; # 1~2 fault impedance increases when the fault_type is from 1-3  \n",
    "n_class =87\n",
    "dim_input = 1\n",
    "# parameters for CNN \n",
    "patience = 30  \n",
    "lambda_loss_amount = 0.1\n",
    "batch_size =70\n",
    "display_step = 100  \n",
    "num_bus =68 \n",
    "learning_rate = 0.01               \n",
    "rootPath =  '/Users/wenting/Documents/04_research/08_Explainable_DNN/02_simu/01_data'\n",
    "trainName = 'train_original_sigPQ.mat'#'train_PQ_pertub_02pu.mat'   \n",
    "testName = 'testing_sigPQ_perturb_0.5_percentage'#  'Test_perturb_0.5pu_type_' + str(fault_type) + '_impedance_' + str(impe_type) + '_sigNew'#'Test_input_perturb_05pu_type_' + str(fault_type) + '_impedance_' + str(impe_type) + '_new'\n",
    "model_dir  = '/Users/wenting/Documents/04_research/08_Explainable_DNN/02_simu/03_saved' \n",
    "epsilon = 0.005 \n",
    "k = 7\n",
    "alpha = 0.01#step size for sigma\n",
    "low_limit =  0.0\n",
    "up_limit =  2.0\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr_max = 0.1 * learning_rate\n",
    "lr_min = 1.2 * learning_rate\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-5\n",
    "epochs =800\n",
    "lr_schedule = 'cyclic'\n",
    "dim_input = 1 \n",
    "dim_hidden = [4,8,8,8]\n",
    "nclass = 87\n",
    "seed = 1\n",
    "pgd_alpha =  alpha\n",
    "early_stop = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadline():\n",
    "    #linePath = '/Users/wenting/Documents/04_research/08_Explainable_DNN/02_simu/01_data' \n",
    "    data = sio.loadmat(os.path.join(rootPath, 'train_original_sig.mat'))\n",
    "    line = data['line']\n",
    "    Y = data['Y']\n",
    "    #v = data['dV_feature']\n",
    "    #psi= imag(Y @ v)\n",
    "    labels = data['Labels'][0]\n",
    "    Yabs = abs(Y)\n",
    "    Yabs1= Yabs.todense()# - Yabs.diagonal() # not include itself\n",
    "    line_neib = {}\n",
    "    for r in range(line.shape[0]):\n",
    "        busl , busr = line[r, :2]\n",
    "        nl = np.r_[np.where(line[:, 0] ==  busl )[0], np.where(line[:, 1] ==  busl )[0] ]\n",
    "        nr = np.r_[np.where(line[:, 0] ==  busr )[0], np.where(line[:,1] ==  busr )[0] ] \n",
    "        #print(np.r_[nl, nr])\n",
    "        line_neib[r + 1] = np.unique(np.r_[nl+1, nr+1])\n",
    "    return line, Y.todense(),    line_neib \n",
    "\n",
    "def normalize(x):\n",
    "    N, col = x.shape\n",
    "    x = (x - np.mean(x, 0))/np.std(x, 0)\n",
    "    return x\n",
    "  \n",
    "def load_data_VI(w,path,name, norm = 0): \n",
    "    base = 100.0\n",
    "    # 'I0_all', 'Itheta0_all','I1_all', 'Itheta1_all', 'V0_all', 'theta0_all','V1_all', 'theta1_all', 'Labels','I1_all', 'I0_all' , 'Y', 'line', 'bus' \n",
    "    PathName = os.path.join(path, name)\n",
    "    data=sio.loadmat(PathName);   \n",
    "    V0= data['V0_all'] \n",
    "    theta0= data['theta0_all'] \n",
    "    V1= data['V1_all'] \n",
    "    theta1= data['theta1_all'] \n",
    "    I0= data['I0_all'] \n",
    "    Itheta0= data['Itheta0_all'] \n",
    "    I1= data['I1_all'] \n",
    "    Itheta1= data['Itheta1_all'] \n",
    "    Y_ad= data['Y'] \n",
    "    num_bus, num_sample = np.shape(V0)\n",
    "    dreal = np.zeros((num_bus, num_sample))\n",
    "    dimg = np.zeros((num_bus, num_sample)) \n",
    "    dmag  = V1  - V0 \n",
    "    dtheta  = theta1  - theta0 \n",
    "    temp_real = dmag * np.cos(dtheta)\n",
    "    #print('before',temp_real[:, 0])\n",
    "    dreal[w,:] =temp_real[w,:]\n",
    "    #print('after',dreal[:, 0])\n",
    "    temp_imag = (dmag * np.sin(dtheta))\n",
    "    dimg[w,:] = temp_imag[w,:] \n",
    "    dIreal = np.zeros((num_bus, num_sample))\n",
    "    dIimg = np.zeros((num_bus, num_sample))\n",
    "    dImag = (I1  - I0  ) \n",
    "    dItheta  = Itheta1 - Itheta0 \n",
    "    tempI_r = (dImag * np.cos(dItheta)) \n",
    "    dIreal[w,:] = tempI_r[w,:] \n",
    "    tempI_i =  (dImag * np.sin(dItheta))\n",
    "    dIimg[w,:] =tempI_i[w,:]\n",
    "    #Labels = data['Labels'][0]\n",
    "    #psi= Y_ad[:,w] @ dV[w,:] \n",
    "    train_x = np.float64(np.r_[dreal, dimg].T)#, dIreal, dIimg \n",
    "    train_i = np.float64(np.r_[  dIreal, dIimg ].T)\n",
    "    #print(train_x[0,:])\n",
    "    if norm:\n",
    "        train_x = normalize(train_x)\n",
    "    #train_psi = np.float64((psi).imag.T )  \n",
    "    train_labels = data['Labels'][0]# sio.loadmat(os.path.join(path, 'train_PQ_pertub_1pu.mat'))['Labels'][0]#data['y_num'] \n",
    "    col, buses = np.shape(train_x) \n",
    "    #train_psi =   torch.FloatTensor(train_psi)\n",
    "    train_x = torch.FloatTensor(train_x)\n",
    "    train_x = torch.unsqueeze(train_x, 1)\n",
    "    train_x = torch.unsqueeze(train_x, 3)\n",
    "    train_i = torch.FloatTensor(train_i)\n",
    "    train_i = torch.unsqueeze(train_i, 1)\n",
    "    train_i = torch.unsqueeze(train_i, 3)\n",
    "    #train_psi = torch.unsqueeze(train_psi, 1)\n",
    "    #train_psi = torch.unsqueeze(train_psi, 3)\n",
    "    #train_x = np.float64(np.reshape(train_data, (int(col ), buses,times)))  \n",
    "    train_y = np.reshape(train_labels, [col,]) #np.zeros((int(col/times), n_classes))\n",
    "    #for i in range(int(col/times)):\n",
    "        #train_y[i,train_labels[0][i] ] = 1;\n",
    "    return train_x, train_i,  torch.LongTensor(train_y) ,col \n",
    "    \n",
    "def load_data_VI_new(w,path,name,mag = False,  norm = 0): \n",
    "    base = 100.0\n",
    "    # 'I0_all', 'Itheta0_all','I1_all', 'Itheta1_all', 'V0_all', 'theta0_all','V1_all', 'theta1_all', 'Labels','I1_all', 'I0_all' , 'Y', 'line', 'bus' \n",
    "    PathName = os.path.join(path, name)\n",
    "    data=sio.loadmat(PathName);   \n",
    "    mag0= data['V0_all']  \n",
    "    theta0= data['theta0_all'] \n",
    "    mag1= data['V1_all'] \n",
    "    theta1= data['theta1_all'] \n",
    "    Imag0= data['I0_all'] \n",
    "    Itheta0= data['Itheta0_all'] \n",
    "    Imag1= data['I1_all'] \n",
    "    Itheta1= data['Itheta1_all'] \n",
    "    Y_ad= data['Y'] \n",
    "    V0 = mag0 * np.cos(theta0  ) + 1j * mag0 * np.sin(theta0  )\n",
    "    V1 = mag1 * np.cos(theta1 ) + 1j * mag1 * np.sin(theta1  )\n",
    "    num_bus, num_sample = np.shape(V0)\n",
    "    dV = V1 - V0 \n",
    "    if not mag:  \n",
    "        dreal = np.zeros((num_bus, num_sample))\n",
    "        dimg = np.zeros((num_bus, num_sample))  \n",
    "        all_real = np.real(dV)\n",
    "        all_imag = np.imag(dV) \n",
    "        dreal[w,:] =all_real[w,:]  \n",
    "        dimg[w,:] = all_imag[w,:]  \n",
    "        train_x = np.float64(np.r_[dreal, dimg].T) \n",
    "    else:\n",
    "        dmag = np.zeros((num_bus, num_sample))\n",
    "        dtheta = np.zeros((num_bus, num_sample))  \n",
    "        all_mag = np.abs(dV)\n",
    "        all_theta = np.angle(dV) \n",
    "        dmag[w,:] =all_mag[w,:]  \n",
    "        dtheta[w,:] = all_theta[w,:]  \n",
    "        train_x = np.float64(np.r_[dmag, dtheta].T)  \n",
    "    if norm:\n",
    "        train_x = normalize(train_x) \n",
    "    train_labels = data['Labels'][0]# sio.loadmat(os.path.join(path, 'train_PQ_pertub_1pu.mat'))['Labels'][0]#data['y_num'] \n",
    "    col, buses = np.shape(train_x) \n",
    "    #train_psi =   torch.FloatTensor(train_psi)\n",
    "    train_x = torch.FloatTensor(train_x)\n",
    "    train_x = torch.unsqueeze(train_x, 1)\n",
    "    train_x = torch.unsqueeze(train_x, 3) \n",
    "    train_y = np.reshape(train_labels, [col,])  \n",
    "    return train_x,   torch.LongTensor(train_y) ,col  \n",
    " \n",
    "\n",
    "def choose_w(line, thres, num_bus = 68): # choose the measured buses by the threshold of degrees\n",
    "    all_freq =np.zeros((1,num_bus))\n",
    "    for i in range( num_bus):\n",
    "        ifreq = np.shape(np.where(line[:,0] == i+1 ))[1] + np.shape(np.where(line[:,1] == i+1))[1]\n",
    "        all_freq[0][i] = ifreq \n",
    "    w = [i for i in range(num_bus) if all_freq[0][i] >thres]  \n",
    "    return w\n",
    "\n",
    "# load data\n",
    "def load_all_data_VI(w):\n",
    "    global train_x,    train_labels, train_num, test_x,test_psi, test_labels, test_num,  samples,buses, times \n",
    "    train_x,   train_labels, train_num = load_data_VI_new(w,rootPath, trainName)  \n",
    "    test_x,    test_labels,test_num= load_data_VI_new(w,rootPath, testName)  \n",
    "    samples,buses,_, _  = np.shape(train_x)  \n",
    "\n",
    "def one_hot_neib(test_labels, line_neib):\n",
    "    num_class = torch.max(test_labels)  + 1\n",
    "    num_sample = test_labels.shape[0]\n",
    "    y_neib = torch.zeros((num_sample, num_class)) \n",
    "    for i in range(num_sample): \n",
    "        ind = (test_labels[i].cpu() ).numpy()   \n",
    "        y_neib[i,  line_neib[int(ind)  ]  ] = 1  \n",
    "        #assert line_neib[test_labels[i]-1].shape[0] == line_neib[int(ind)-1].shape[0]\n",
    "    return y_neib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self, dim_input = 1, dim_hidden = [4,8,8,8], batch_size = 50 ):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d( dim_input , dim_hidden[0] ,  kernel_size=(5,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[0]), # normalize along the 2-dim if input has four dims \n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(dim_hidden[0], dim_hidden[1], kernel_size=(5,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[1]),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(dim_hidden[1], dim_hidden[2], kernel_size= (3,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[2]),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(dim_hidden[2], dim_hidden[3], kernel_size= (3,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[3]),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(6 * dim_hidden[3] , nclass)\n",
    "        )\n",
    "    \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x) \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.linear_layers(x)\n",
    "        return x \n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        #torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.xavier_uniform_(m.weight) \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias) \n",
    "        \n",
    "def evaluate_pgd(test_loader, model, attack_iters, restarts):\n",
    "    epsilon = (8 / 255.) / std\n",
    "    alpha = (2 / 255.) / std\n",
    "    pgd_loss = 0\n",
    "    pgd_acc = 0\n",
    "    n = 0\n",
    "    model.eval()\n",
    "    for i, (X, y) in enumerate(test_loader):\n",
    "        #X, y = X.cuda(), y.cuda()\n",
    "        pgd_delta = attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts)\n",
    "        with torch.no_grad():\n",
    "            output = model(X + pgd_delta)\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            pgd_loss += loss.item() * y.size(0)\n",
    "            pgd_acc += (output.max(1)[1] == y).sum().item()\n",
    "            n += y.size(0)\n",
    "    return pgd_loss/n, pgd_acc/n\n",
    "\n",
    "\n",
    "def evaluate_standard(test_loader, model):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    n = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "            #X, y = X.cuda(), y.cuda()\n",
    "            output = model(X)\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            test_loss += loss.item() * y.size(0)\n",
    "            test_acc += (output.max(1)[1] == y).sum().item()\n",
    "            n += y.size(0)\n",
    "    return test_loss/n, test_acc/n\n",
    "\n",
    "def clamp(X, lower_limit, upper_limit): \n",
    "    return torch.max(torch.min(X, upper_limit ), lower_limit)   \n",
    "\n",
    " \n",
    "\n",
    "def attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts, opt=None):\n",
    "    # Restart PGD to find the adversarial examples that have larger loss than the max_loss\n",
    "    #After restarts serveral times, many data samples have the worst-case perturbations generated with different init delta\n",
    "    max_loss = torch.zeros(y.shape[0]) \n",
    "    max_delta = torch.zeros_like(X) \n",
    "    for zz in range(restarts): \n",
    "        delta = torch.FloatTensor(X).uniform_(-epsilon, epsilon)\n",
    "        #delta.data = clamp(delta, lower_limit - X, upper_limit - X)\n",
    "        delta.requires_grad = True\n",
    "        for _ in range(attack_iters):\n",
    "            output = model(X + delta)\n",
    "            index = torch.where(output.max(1)[1] == y)# only use the correct examples\n",
    "            if len(index[0]) == 0:\n",
    "                break\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            loss.backward()\n",
    "            grad = delta.grad.detach()\n",
    "            d = delta[index[0], :, :, :]\n",
    "            g = grad[index[0], :, :, :]\n",
    "            limit = epsilon* torch.ones_like( delta[index[0], :, :, :])\n",
    "            d = clamp(d + alpha * torch.sign(g), -limit, limit)\n",
    "            #d = clamp(d, lower_limit - X[index[0], :, :, :], upper_limit - X[index[0], :, :, :]) \n",
    "            delta.data[index[0], :, :, :] = d\n",
    "            delta.grad.zero_()\n",
    "        all_loss = F.cross_entropy(model(X+delta), y, reduction='none').detach()\n",
    "        max_delta[all_loss >= max_loss] = delta.detach()[all_loss >= max_loss]\n",
    "        max_loss = torch.max(max_loss, all_loss)\n",
    "    return max_delta\n",
    "\n",
    "\n",
    "model = Net(dim_input, dim_hidden)  \n",
    "model.apply(weights_init) \n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "#optimizer = SGD(model.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)   \n",
    "criterion = CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_dist():\n",
    "    data = sio.loadmat(os.path.join(rootPath, 'distri_cur_nofault.mat'))\n",
    "    up_real = data['up_real']\n",
    "    up_imag = data['up_imag']\n",
    "    down_real = data['down_real']\n",
    "    down_imag = data['down_imag'] \n",
    "    up_limit = np.r_[up_real, up_imag]\n",
    "    down_limit = np.r_[down_real, down_imag]\n",
    "    return  up_limit, down_limit\n",
    "\n",
    "def vol_dist():\n",
    "    data = sio.loadmat(os.path.join(rootPath, 'distri_vol_nofault.mat'))\n",
    "    up_real = data['up_real']\n",
    "    up_imag = data['up_imag']\n",
    "    down_real = data['down_real']\n",
    "    down_imag = data['down_imag'] \n",
    "    up_limit = np.r_[up_real, up_imag]\n",
    "    down_limit = np.r_[down_real, down_imag]\n",
    "    return  up_limit, down_limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def sample_I(Imean_list, Istd_list, choice, num):\n",
    "    rol, col = Imean_list.shape \n",
    "    assert choice < col \n",
    "    result = np.zeros((num, rol))\n",
    "    for i in range(rol):\n",
    "        if (Imean_list[ i, choice] - Istd_list[ i, choice]) < (Imean_list[ i, choice] + Istd_list[ i, choice]):\n",
    "            result[:, i] = np.random.uniform( (Imean_list[ i, choice] - Istd_list[ i, choice]),(Imean_list[ i, choice] + Istd_list[ i, choice]),num)\n",
    "    return result \n",
    "\n",
    "def range_I(Imean_list, Istd_list ):\n",
    "    rol, col = Imean_list.shape  \n",
    "    rangel = np.zeros((rol, 1))\n",
    "    ranger = np.zeros((rol, 1))\n",
    "    for i in range(rol):\n",
    "        rangel[ i ] = max(Imean_list[ i, :]  ) \n",
    "        ranger[ i] = min(Imean_list[ i, :] - Istd_list[ i, :] ) \n",
    "    return rangel, ranger\n",
    "\n",
    "def convert_shape(up_limit, batch_size):\n",
    "    up_limit = torch.FloatTensor(up_limit)\n",
    "    up_limit = torch.unsqueeze(up_limit, 0)\n",
    "    up_limit = torch.unsqueeze(up_limit, 0)\n",
    "    up_limit  = up_limit.repeat(batch_size,1,1,1)\n",
    "    return up_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 136)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa3b82609d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = sio.loadmat(os.path.join(rootPath, trainName))\n",
    "linedata, Y,  line_neib = loadline() \n",
    "Y_ri = np.r_[np.c_[Y.real, -Y.imag], np.c_[ Y.imag, Y.real ]].T\n",
    "print(Y_ri.shape)\n",
    "w = choose_w(linedata,2)  \n",
    "load_all_data_VI(w ) \n",
    "#Imean_list, Istd_list = current_dist()\n",
    "#rangel, ranger = range_I(Imean_list, Istd_list) \n",
    "#up_limit = np.r_[np.ones((num_bus*2, 1))*epsilon, ranger]\n",
    "#down_limit = np.r_[-np.ones((num_bus*2, 1))*epsilon, rangel]\n",
    "    \n",
    "cur_up_limit, cur_down_limit = current_dist() \n",
    "vol_up_limit, vol_down_limit = vol_dist() \n",
    "up_limit = epsilon * np.r_[vol_up_limit, cur_up_limit]\n",
    "down_limit = epsilon *np.r_[vol_down_limit, cur_down_limit]\n",
    "\n",
    "plt.plot( cur_up_limit[:52,:])\n",
    "plt.plot(cur_down_limit[:52,:])\n",
    "plt.show()\n",
    "plt.plot( vol_up_limit )\n",
    "plt.plot(vol_down_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval_train(model, device, train_loader):\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            train_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('Training: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "    training_accuracy = correct / len(train_loader.dataset)\n",
    "    return train_loss, training_accuracy\n",
    "\n",
    "\n",
    "def eval_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = args.lr\n",
    "    if epoch >= 75:\n",
    "        lr = args.lr * 0.1\n",
    "    if epoch >= 90:\n",
    "        lr = args.lr * 0.01\n",
    "    if epoch >= 100:\n",
    "        lr = args.lr * 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(test_x, test_labels , model, line_neib ):\n",
    "    #test_x = torch.unsqueeze(test_x, 3) \n",
    "    test_x, test_labels = test_x , test_labels \n",
    "    num_test = test_labels.shape[0]\n",
    "    model.eval()\n",
    "    output = model(test_x  )  \n",
    "    loss_test = criterion(output , test_labels )\n",
    "    #loss_test = F.nll_loss(output , test_labels )\n",
    "    logit = torch.softmax(output, 1)\n",
    "    pred = output.max(1)[1] \n",
    "    acc_test = torch.eq(pred,test_labels).sum().item()*100/num_test\n",
    "    multi_labels =  one_hot_neib(test_labels, line_neib) \n",
    "    match = 0\n",
    "    #pred = output.max(1)[1] \n",
    "    for i in range(num_test):\n",
    "        match += multi_labels[i, pred[i]  ]  \n",
    "    acc_hop = torch.true_divide(match,num_test)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test ),\n",
    "         \"1-hop accuracy = {:.4f}\".format(acc_hop))\n",
    "    return acc_test, acc_hop\n",
    "\n",
    "def main(seed, dim_input, dim_hidden, up_limit, down_limit, batch_size, step_size, k):\n",
    "    np.random.seed( seed)\n",
    "    torch.manual_seed( seed)\n",
    "    torch.cuda.manual_seed(  seed)\n",
    "    \n",
    "    model = Net(dim_input, dim_hidden) \n",
    "    model.apply(weights_init) \n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "    criterion = CrossEntropyLoss()   \n",
    "     \n",
    "    pre_robust_acc = 0.\n",
    "    start = time.time() \n",
    "    num_sample, _, l , _= train_x.shape\n",
    "    x_train, y_train = Variable(train_x)  , Variable(train_labels)  \n",
    "    train_best = float('Inf')\n",
    "    train_loss_list = []\n",
    "    up_limit = convert_shape(up_limit, batch_size)\n",
    "    down_limit = convert_shape(down_limit, batch_size)\n",
    "    for epoch in range( epochs): \n",
    "        start_epoch_time = time.time()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_n = 0 \n",
    "        #sample_deltaI = sample_I(Imean_list, Istd_list, epoch%4, batch_size)\n",
    "        #delta_I =   torch.FloatTensor( sample_deltaI) \n",
    "        for i in range(int(train_x.shape[0] / batch_size)):\n",
    "            id_train = np.random.choice(train_x.shape[0], batch_size, replace= False) \n",
    "            model.eval()\n",
    "            batch_size, _, size_U, _ = x_train[id_train].shape \n",
    "            delta = torch.randn((batch_size, 1, 2*size_U, 1)).detach()\n",
    "            delta = torch.min(torch.max(delta,   down_limit),   up_limit) \n",
    "            for _ in range(k): \n",
    "                delta.requires_grad_() \n",
    "                delta_U = delta[:, 0, :size_U, 0]\n",
    "                delta_I = delta[:, 0, size_U:,0]  \n",
    "                output = model(x_train[id_train] +    delta[:, :, :size_U, :] )  \n",
    "                loss = F.cross_entropy(output, y_train[id_train], size_average = False )\\\n",
    "                - lambda_loss_amount*torch.nn.functional.l1_loss\\\n",
    "                ( (torch.matmul( delta_U , torch.FloatTensor(Y_ri))   ), (delta_I), size_average = False)\n",
    "                loss.backward()\n",
    "                grad = delta.grad.detach()\n",
    "                delta = delta.detach() + step_size * torch.sign(grad.detach()) \n",
    "                delta = torch.min(torch.max(delta,   down_limit),   up_limit)  \n",
    "            model.train() \n",
    "            x_adv = x_train[id_train].detach()  + delta[:, :, :size_U, :].detach()\n",
    "            output = model(x_adv)\n",
    "            loss = criterion(output, y_train[id_train])\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()  #* y_train[id_train].size(0)\n",
    "            train_acc  = (output.max(1)[1] == y_train[id_train]).sum().item() *100/np.shape(id_train)[0]  \n",
    "            train_n += y_train[id_train].size(0)\n",
    "            if epoch%5 == 0: \n",
    "                print('Training Epoch: {}, [{}/{}, {:.0f}%], loss is {:.6f}'\\\n",
    "                      .format(epoch  , i * batch_size, train_x.shape[0],  train_acc  , train_loss  )) \n",
    "                if train_loss < train_best:\n",
    "                    print('Best Epoch is', epoch)\n",
    "                    train_best = train_loss \n",
    "                    torch.save(model.state_dict(), os.path.join(model_dir, savename+'.pt'))\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(model_dir, savename+'.tar')) \n",
    "            train_loss_list.append(loss.item())\n",
    "            #scheduler.step()\n",
    "        if  early_stop:\n",
    "            # Check current PGD robustness of model using random minibatch\n",
    "            id_train = np.random.choice(train_x.shape[0], batch_size, replace= False) \n",
    "            pgd_delta = attack_pgd(model, x_train[id_train], y_train[id_train], epsilon, pgd_alpha, 5, 1, optimizer)\n",
    "            with torch.no_grad():\n",
    "                #output = model(clamp(x_train[id_train] + pgd_delta , low_limit* torch.ones_like(x_train[id_train]), up_limit* torch.ones_like(x_train[id_train])))\n",
    "                output = model(x_train[id_train] + pgd_delta)\n",
    "            robust_acc = (output.max(1)[1] == y_train[id_train]).sum().item() / y_train[id_train].size(0)\n",
    "            if robust_acc - pre_robust_acc < -0.2:\n",
    "                break\n",
    "            pre_robust_acc = robust_acc\n",
    "            best_state_dict = model.state_dict()#copy.deepcopy(model.state_dict())\n",
    "        epoch_time = time.time()\n",
    "        #lr = scheduler.get_lr()[0] \n",
    "        train_time = time.time()\n",
    "        #print('Epoch time is', epoch_time - start_epoch_time)\n",
    "        if not early_stop:\n",
    "            best_state_dict = model.state_dict() \n",
    "         \n",
    "        # Evaluation\n",
    "    model_test = Net(dim_input, dim_hidden)\n",
    "    model_test.load_state_dict(torch.load( os.path.join(model_dir, savename+'.pt')))\n",
    "    model_test.float()\n",
    "    model_test.eval()\n",
    "    test_x, test_i,  test_labels,test_num= load_data_VI(w,rootPath, testName)  \n",
    "    acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "    \n",
    "    return train_loss, train_best, model_test\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenting/opt/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 0, [0/560, 0%], loss is 4.601176\n",
      "Best Epoch is 0\n",
      "Training Epoch: 0, [70/560, 3%], loss is 9.094516\n",
      "Training Epoch: 0, [140/560, 4%], loss is 13.532919\n",
      "Training Epoch: 0, [210/560, 3%], loss is 17.826720\n",
      "Training Epoch: 0, [280/560, 4%], loss is 22.095142\n",
      "Training Epoch: 0, [350/560, 3%], loss is 26.341271\n",
      "Training Epoch: 0, [420/560, 3%], loss is 30.399306\n",
      "Training Epoch: 0, [490/560, 9%], loss is 34.384307\n",
      "Training Epoch: 5, [0/560, 33%], loss is 2.479852\n",
      "Best Epoch is 5\n",
      "Training Epoch: 5, [70/560, 39%], loss is 4.773050\n",
      "Training Epoch: 5, [140/560, 43%], loss is 6.994965\n",
      "Training Epoch: 5, [210/560, 44%], loss is 9.161483\n",
      "Training Epoch: 5, [280/560, 40%], loss is 11.447414\n",
      "Training Epoch: 5, [350/560, 53%], loss is 13.373450\n",
      "Training Epoch: 5, [420/560, 40%], loss is 15.423497\n",
      "Training Epoch: 5, [490/560, 41%], loss is 17.539521\n",
      "Training Epoch: 10, [0/560, 57%], loss is 1.362743\n",
      "Best Epoch is 10\n",
      "Training Epoch: 10, [70/560, 61%], loss is 2.598891\n",
      "Training Epoch: 10, [140/560, 67%], loss is 3.675929\n",
      "Training Epoch: 10, [210/560, 60%], loss is 4.900359\n",
      "Training Epoch: 10, [280/560, 74%], loss is 5.995838\n",
      "Training Epoch: 10, [350/560, 61%], loss is 7.158916\n",
      "Training Epoch: 10, [420/560, 70%], loss is 8.198520\n",
      "Training Epoch: 10, [490/560, 71%], loss is 9.207685\n",
      "Training Epoch: 15, [0/560, 76%], loss is 0.868113\n",
      "Best Epoch is 15\n",
      "Training Epoch: 15, [70/560, 74%], loss is 1.688546\n",
      "Training Epoch: 15, [140/560, 69%], loss is 2.721297\n",
      "Training Epoch: 15, [210/560, 66%], loss is 3.653694\n",
      "Training Epoch: 15, [280/560, 71%], loss is 4.544824\n",
      "Training Epoch: 15, [350/560, 69%], loss is 5.533330\n",
      "Training Epoch: 15, [420/560, 76%], loss is 6.422247\n",
      "Training Epoch: 15, [490/560, 80%], loss is 7.201017\n",
      "Training Epoch: 20, [0/560, 81%], loss is 0.623766\n",
      "Best Epoch is 20\n",
      "Training Epoch: 20, [70/560, 77%], loss is 1.361310\n",
      "Training Epoch: 20, [140/560, 80%], loss is 2.095424\n",
      "Training Epoch: 20, [210/560, 83%], loss is 2.681594\n",
      "Training Epoch: 20, [280/560, 83%], loss is 3.327060\n",
      "Training Epoch: 20, [350/560, 86%], loss is 3.920448\n",
      "Training Epoch: 20, [420/560, 81%], loss is 4.593249\n",
      "Training Epoch: 20, [490/560, 83%], loss is 5.212857\n",
      "Training Epoch: 25, [0/560, 91%], loss is 0.446511\n",
      "Best Epoch is 25\n",
      "Training Epoch: 25, [70/560, 70%], loss is 1.198698\n",
      "Training Epoch: 25, [140/560, 77%], loss is 1.896067\n",
      "Training Epoch: 25, [210/560, 79%], loss is 2.532608\n",
      "Training Epoch: 25, [280/560, 76%], loss is 3.202279\n",
      "Training Epoch: 25, [350/560, 80%], loss is 3.955794\n",
      "Training Epoch: 25, [420/560, 74%], loss is 4.606596\n",
      "Training Epoch: 25, [490/560, 77%], loss is 5.206616\n",
      "Training Epoch: 30, [0/560, 84%], loss is 0.592068\n",
      "Training Epoch: 30, [70/560, 77%], loss is 1.167159\n",
      "Training Epoch: 30, [140/560, 79%], loss is 1.687902\n",
      "Training Epoch: 30, [210/560, 79%], loss is 2.228291\n",
      "Training Epoch: 30, [280/560, 84%], loss is 2.746777\n",
      "Training Epoch: 30, [350/560, 76%], loss is 3.384414\n",
      "Training Epoch: 30, [420/560, 70%], loss is 4.167511\n",
      "Training Epoch: 30, [490/560, 77%], loss is 4.729922\n",
      "Training Epoch: 35, [0/560, 86%], loss is 0.414164\n",
      "Best Epoch is 35\n",
      "Training Epoch: 35, [70/560, 77%], loss is 1.069606\n",
      "Training Epoch: 35, [140/560, 84%], loss is 1.604249\n",
      "Training Epoch: 35, [210/560, 81%], loss is 2.103106\n",
      "Training Epoch: 35, [280/560, 76%], loss is 2.607554\n",
      "Training Epoch: 35, [350/560, 80%], loss is 3.269744\n",
      "Training Epoch: 35, [420/560, 76%], loss is 3.873768\n",
      "Training Epoch: 35, [490/560, 80%], loss is 4.385812\n",
      "Training Epoch: 40, [0/560, 91%], loss is 0.314912\n",
      "Best Epoch is 40\n",
      "Training Epoch: 40, [70/560, 84%], loss is 0.744420\n",
      "Training Epoch: 40, [140/560, 86%], loss is 1.194821\n",
      "Training Epoch: 40, [210/560, 80%], loss is 1.684776\n",
      "Training Epoch: 40, [280/560, 87%], loss is 2.075150\n",
      "Training Epoch: 40, [350/560, 93%], loss is 2.392245\n",
      "Training Epoch: 40, [420/560, 90%], loss is 2.696585\n",
      "Training Epoch: 40, [490/560, 86%], loss is 3.034915\n",
      "Training Epoch: 45, [0/560, 81%], loss is 0.519180\n",
      "Training Epoch: 45, [70/560, 80%], loss is 1.003275\n",
      "Training Epoch: 45, [140/560, 83%], loss is 1.636495\n",
      "Training Epoch: 45, [210/560, 84%], loss is 2.059413\n",
      "Training Epoch: 45, [280/560, 91%], loss is 2.311604\n",
      "Training Epoch: 45, [350/560, 81%], loss is 2.785118\n",
      "Training Epoch: 45, [420/560, 87%], loss is 3.262085\n",
      "Training Epoch: 45, [490/560, 80%], loss is 3.782941\n",
      "Training Epoch: 50, [0/560, 79%], loss is 0.560245\n",
      "Training Epoch: 50, [70/560, 81%], loss is 1.073457\n",
      "Training Epoch: 50, [140/560, 86%], loss is 1.514420\n",
      "Training Epoch: 50, [210/560, 80%], loss is 2.050045\n",
      "Training Epoch: 50, [280/560, 84%], loss is 2.493352\n",
      "Training Epoch: 50, [350/560, 86%], loss is 2.996395\n",
      "Training Epoch: 50, [420/560, 86%], loss is 3.358366\n",
      "Training Epoch: 50, [490/560, 83%], loss is 3.819893\n",
      "Training Epoch: 55, [0/560, 93%], loss is 0.319885\n",
      "Training Epoch: 55, [70/560, 90%], loss is 0.610199\n",
      "Training Epoch: 55, [140/560, 89%], loss is 0.965043\n",
      "Training Epoch: 55, [210/560, 96%], loss is 1.246563\n",
      "Training Epoch: 55, [280/560, 83%], loss is 1.751288\n",
      "Training Epoch: 55, [350/560, 81%], loss is 2.269322\n",
      "Training Epoch: 55, [420/560, 86%], loss is 2.651314\n",
      "Training Epoch: 55, [490/560, 84%], loss is 3.014402\n",
      "Training Epoch: 60, [0/560, 80%], loss is 0.550649\n",
      "Training Epoch: 60, [70/560, 89%], loss is 0.857583\n",
      "Training Epoch: 60, [140/560, 90%], loss is 1.155124\n",
      "Training Epoch: 60, [210/560, 89%], loss is 1.565975\n",
      "Training Epoch: 60, [280/560, 84%], loss is 2.001570\n",
      "Training Epoch: 60, [350/560, 87%], loss is 2.360337\n",
      "Training Epoch: 60, [420/560, 86%], loss is 2.653409\n",
      "Training Epoch: 60, [490/560, 84%], loss is 3.026621\n",
      "Training Epoch: 65, [0/560, 87%], loss is 0.314369\n",
      "Best Epoch is 65\n",
      "Training Epoch: 65, [70/560, 90%], loss is 0.652770\n",
      "Training Epoch: 65, [140/560, 74%], loss is 1.371265\n",
      "Training Epoch: 65, [210/560, 83%], loss is 1.814612\n",
      "Training Epoch: 65, [280/560, 81%], loss is 2.323326\n",
      "Training Epoch: 65, [350/560, 90%], loss is 2.621308\n",
      "Training Epoch: 65, [420/560, 84%], loss is 3.035504\n",
      "Training Epoch: 65, [490/560, 81%], loss is 3.635301\n",
      "Training Epoch: 70, [0/560, 73%], loss is 0.758633\n",
      "Training Epoch: 70, [70/560, 97%], loss is 0.965203\n",
      "Training Epoch: 70, [140/560, 86%], loss is 1.356426\n",
      "Training Epoch: 70, [210/560, 80%], loss is 1.915384\n",
      "Training Epoch: 70, [280/560, 80%], loss is 2.355051\n",
      "Training Epoch: 70, [350/560, 83%], loss is 2.775730\n",
      "Training Epoch: 70, [420/560, 83%], loss is 3.259274\n",
      "Training Epoch: 70, [490/560, 73%], loss is 3.863914\n",
      "Training Epoch: 75, [0/560, 86%], loss is 0.313022\n",
      "Best Epoch is 75\n",
      "Training Epoch: 75, [70/560, 96%], loss is 0.549324\n",
      "Training Epoch: 75, [140/560, 94%], loss is 0.826586\n",
      "Training Epoch: 75, [210/560, 91%], loss is 1.096476\n",
      "Training Epoch: 75, [280/560, 94%], loss is 1.341292\n",
      "Training Epoch: 75, [350/560, 93%], loss is 1.552842\n",
      "Training Epoch: 75, [420/560, 93%], loss is 1.838773\n",
      "Training Epoch: 75, [490/560, 89%], loss is 2.188644\n",
      "Training Epoch: 80, [0/560, 83%], loss is 0.421272\n",
      "Training Epoch: 80, [70/560, 86%], loss is 0.772078\n",
      "Training Epoch: 80, [140/560, 77%], loss is 1.418368\n",
      "Training Epoch: 80, [210/560, 87%], loss is 1.765537\n",
      "Training Epoch: 80, [280/560, 93%], loss is 2.020491\n",
      "Training Epoch: 80, [350/560, 83%], loss is 2.434980\n",
      "Training Epoch: 80, [420/560, 84%], loss is 2.819919\n",
      "Training Epoch: 80, [490/560, 91%], loss is 3.138178\n",
      "Training Epoch: 85, [0/560, 93%], loss is 0.194732\n",
      "Best Epoch is 85\n",
      "Training Epoch: 85, [70/560, 86%], loss is 0.540238\n",
      "Training Epoch: 85, [140/560, 80%], loss is 0.960672\n",
      "Training Epoch: 85, [210/560, 91%], loss is 1.212063\n",
      "Training Epoch: 85, [280/560, 89%], loss is 1.564304\n",
      "Training Epoch: 85, [350/560, 96%], loss is 1.855100\n",
      "Training Epoch: 85, [420/560, 91%], loss is 2.179366\n",
      "Training Epoch: 85, [490/560, 89%], loss is 2.435487\n",
      "Training Epoch: 90, [0/560, 87%], loss is 0.266955\n",
      "Training Epoch: 90, [70/560, 83%], loss is 0.708437\n",
      "Training Epoch: 90, [140/560, 83%], loss is 1.107037\n",
      "Training Epoch: 90, [210/560, 90%], loss is 1.430647\n",
      "Training Epoch: 90, [280/560, 93%], loss is 1.662896\n",
      "Training Epoch: 90, [350/560, 87%], loss is 1.990865\n",
      "Training Epoch: 90, [420/560, 84%], loss is 2.446621\n",
      "Training Epoch: 90, [490/560, 86%], loss is 2.818537\n",
      "Training Epoch: 95, [0/560, 84%], loss is 0.358016\n",
      "Training Epoch: 95, [70/560, 89%], loss is 0.730000\n",
      "Training Epoch: 95, [140/560, 89%], loss is 1.037057\n",
      "Training Epoch: 95, [210/560, 93%], loss is 1.305849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 95, [280/560, 89%], loss is 1.630116\n",
      "Training Epoch: 95, [350/560, 90%], loss is 2.016837\n",
      "Training Epoch: 95, [420/560, 94%], loss is 2.175394\n",
      "Training Epoch: 95, [490/560, 93%], loss is 2.487713\n",
      "Training Epoch: 100, [0/560, 87%], loss is 0.347997\n",
      "Training Epoch: 100, [70/560, 84%], loss is 0.777391\n",
      "Training Epoch: 100, [140/560, 90%], loss is 1.108390\n",
      "Training Epoch: 100, [210/560, 94%], loss is 1.283901\n",
      "Training Epoch: 100, [280/560, 83%], loss is 1.797235\n",
      "Training Epoch: 100, [350/560, 84%], loss is 2.172142\n",
      "Training Epoch: 100, [420/560, 87%], loss is 2.528275\n",
      "Training Epoch: 100, [490/560, 89%], loss is 2.804524\n",
      "Training Epoch: 105, [0/560, 83%], loss is 0.447395\n",
      "Training Epoch: 105, [70/560, 87%], loss is 0.752128\n",
      "Training Epoch: 105, [140/560, 87%], loss is 1.022023\n",
      "Training Epoch: 105, [210/560, 86%], loss is 1.333490\n",
      "Training Epoch: 105, [280/560, 89%], loss is 1.610326\n",
      "Training Epoch: 105, [350/560, 91%], loss is 1.868347\n",
      "Training Epoch: 105, [420/560, 94%], loss is 2.088348\n",
      "Training Epoch: 105, [490/560, 91%], loss is 2.347465\n",
      "Training Epoch: 110, [0/560, 81%], loss is 0.484871\n",
      "Training Epoch: 110, [70/560, 87%], loss is 0.913950\n",
      "Training Epoch: 110, [140/560, 90%], loss is 1.256928\n",
      "Training Epoch: 110, [210/560, 79%], loss is 1.752049\n",
      "Training Epoch: 110, [280/560, 74%], loss is 2.373922\n",
      "Training Epoch: 110, [350/560, 84%], loss is 2.858326\n",
      "Training Epoch: 110, [420/560, 80%], loss is 3.437228\n",
      "Training Epoch: 110, [490/560, 84%], loss is 3.888139\n",
      "Training Epoch: 115, [0/560, 89%], loss is 0.314601\n",
      "Training Epoch: 115, [70/560, 80%], loss is 0.789319\n",
      "Training Epoch: 115, [140/560, 93%], loss is 1.100760\n",
      "Training Epoch: 115, [210/560, 96%], loss is 1.285697\n",
      "Training Epoch: 115, [280/560, 80%], loss is 1.652288\n",
      "Training Epoch: 115, [350/560, 99%], loss is 1.884636\n",
      "Training Epoch: 115, [420/560, 87%], loss is 2.291110\n",
      "Training Epoch: 115, [490/560, 91%], loss is 2.601078\n",
      "Training Epoch: 120, [0/560, 90%], loss is 0.329019\n",
      "Training Epoch: 120, [70/560, 93%], loss is 0.627747\n",
      "Training Epoch: 120, [140/560, 86%], loss is 1.044208\n",
      "Training Epoch: 120, [210/560, 93%], loss is 1.315548\n",
      "Training Epoch: 120, [280/560, 90%], loss is 1.667905\n",
      "Training Epoch: 120, [350/560, 90%], loss is 1.968373\n",
      "Training Epoch: 120, [420/560, 91%], loss is 2.176775\n",
      "Training Epoch: 120, [490/560, 93%], loss is 2.420058\n",
      "Training Epoch: 125, [0/560, 90%], loss is 0.244324\n",
      "Training Epoch: 125, [70/560, 89%], loss is 0.574304\n",
      "Training Epoch: 125, [140/560, 89%], loss is 0.847888\n",
      "Training Epoch: 125, [210/560, 96%], loss is 1.044791\n",
      "Training Epoch: 125, [280/560, 91%], loss is 1.273333\n",
      "Training Epoch: 125, [350/560, 91%], loss is 1.556761\n",
      "Training Epoch: 125, [420/560, 89%], loss is 1.778251\n",
      "Training Epoch: 125, [490/560, 89%], loss is 2.049046\n",
      "Training Epoch: 130, [0/560, 96%], loss is 0.128421\n",
      "Best Epoch is 130\n",
      "Training Epoch: 130, [70/560, 91%], loss is 0.375055\n",
      "Training Epoch: 130, [140/560, 96%], loss is 0.588305\n",
      "Training Epoch: 130, [210/560, 89%], loss is 0.931365\n",
      "Training Epoch: 130, [280/560, 91%], loss is 1.154775\n",
      "Training Epoch: 130, [350/560, 87%], loss is 1.423295\n",
      "Training Epoch: 130, [420/560, 96%], loss is 1.598034\n",
      "Training Epoch: 130, [490/560, 93%], loss is 1.790100\n",
      "Training Epoch: 135, [0/560, 91%], loss is 0.270284\n",
      "Training Epoch: 135, [70/560, 89%], loss is 0.579315\n",
      "Training Epoch: 135, [140/560, 90%], loss is 0.819263\n",
      "Training Epoch: 135, [210/560, 91%], loss is 1.061697\n",
      "Training Epoch: 135, [280/560, 77%], loss is 1.658296\n",
      "Training Epoch: 135, [350/560, 83%], loss is 1.998509\n",
      "Training Epoch: 135, [420/560, 90%], loss is 2.279128\n",
      "Training Epoch: 135, [490/560, 86%], loss is 2.614013\n",
      "Training Epoch: 140, [0/560, 89%], loss is 0.221951\n",
      "Training Epoch: 140, [70/560, 87%], loss is 0.614305\n",
      "Training Epoch: 140, [140/560, 89%], loss is 0.908573\n",
      "Training Epoch: 140, [210/560, 89%], loss is 1.187774\n",
      "Training Epoch: 140, [280/560, 87%], loss is 1.499997\n",
      "Training Epoch: 140, [350/560, 91%], loss is 1.724613\n",
      "Training Epoch: 140, [420/560, 89%], loss is 2.032372\n",
      "Training Epoch: 140, [490/560, 87%], loss is 2.321638\n",
      "Training Epoch: 145, [0/560, 87%], loss is 0.348872\n",
      "Training Epoch: 145, [70/560, 91%], loss is 0.592503\n",
      "Training Epoch: 145, [140/560, 91%], loss is 0.890256\n",
      "Training Epoch: 145, [210/560, 93%], loss is 1.082269\n",
      "Training Epoch: 145, [280/560, 94%], loss is 1.305021\n",
      "Training Epoch: 145, [350/560, 89%], loss is 1.658256\n",
      "Training Epoch: 145, [420/560, 84%], loss is 2.039678\n",
      "Training Epoch: 145, [490/560, 91%], loss is 2.323888\n",
      "Training Epoch: 150, [0/560, 89%], loss is 0.292173\n",
      "Training Epoch: 150, [70/560, 90%], loss is 0.542740\n",
      "Training Epoch: 150, [140/560, 89%], loss is 0.856900\n",
      "Training Epoch: 150, [210/560, 89%], loss is 1.141439\n",
      "Training Epoch: 150, [280/560, 90%], loss is 1.459288\n",
      "Training Epoch: 150, [350/560, 97%], loss is 1.608826\n",
      "Training Epoch: 150, [420/560, 93%], loss is 1.841847\n",
      "Training Epoch: 150, [490/560, 93%], loss is 2.142984\n",
      "Training Epoch: 155, [0/560, 93%], loss is 0.339897\n",
      "Training Epoch: 155, [70/560, 93%], loss is 0.583288\n",
      "Training Epoch: 155, [140/560, 87%], loss is 0.875258\n",
      "Training Epoch: 155, [210/560, 76%], loss is 1.396094\n",
      "Training Epoch: 155, [280/560, 89%], loss is 1.718521\n",
      "Training Epoch: 155, [350/560, 87%], loss is 2.164291\n",
      "Training Epoch: 155, [420/560, 86%], loss is 2.616995\n",
      "Training Epoch: 155, [490/560, 89%], loss is 2.923207\n",
      "Training Epoch: 160, [0/560, 90%], loss is 0.273288\n",
      "Training Epoch: 160, [70/560, 87%], loss is 0.577203\n",
      "Training Epoch: 160, [140/560, 80%], loss is 0.965970\n",
      "Training Epoch: 160, [210/560, 91%], loss is 1.316754\n",
      "Training Epoch: 160, [280/560, 81%], loss is 1.702314\n",
      "Training Epoch: 160, [350/560, 89%], loss is 2.041095\n",
      "Training Epoch: 160, [420/560, 90%], loss is 2.330771\n",
      "Training Epoch: 160, [490/560, 90%], loss is 2.595908\n",
      "Training Epoch: 165, [0/560, 94%], loss is 0.215481\n",
      "Training Epoch: 165, [70/560, 91%], loss is 0.499927\n",
      "Training Epoch: 165, [140/560, 93%], loss is 0.767878\n",
      "Training Epoch: 165, [210/560, 90%], loss is 1.075863\n",
      "Training Epoch: 165, [280/560, 93%], loss is 1.308413\n",
      "Training Epoch: 165, [350/560, 93%], loss is 1.505221\n",
      "Training Epoch: 165, [420/560, 90%], loss is 1.726636\n",
      "Training Epoch: 165, [490/560, 97%], loss is 1.851218\n",
      "Training Epoch: 170, [0/560, 86%], loss is 0.371279\n",
      "Training Epoch: 170, [70/560, 87%], loss is 0.704485\n",
      "Training Epoch: 170, [140/560, 86%], loss is 1.044602\n",
      "Training Epoch: 170, [210/560, 93%], loss is 1.283918\n",
      "Training Epoch: 170, [280/560, 94%], loss is 1.519729\n",
      "Training Epoch: 170, [350/560, 83%], loss is 2.077710\n",
      "Training Epoch: 170, [420/560, 79%], loss is 2.538475\n",
      "Training Epoch: 170, [490/560, 86%], loss is 2.855227\n",
      "Training Epoch: 175, [0/560, 93%], loss is 0.173352\n",
      "Training Epoch: 175, [70/560, 91%], loss is 0.449989\n",
      "Training Epoch: 175, [140/560, 93%], loss is 0.626760\n",
      "Training Epoch: 175, [210/560, 97%], loss is 0.763247\n",
      "Training Epoch: 175, [280/560, 93%], loss is 1.010784\n",
      "Training Epoch: 175, [350/560, 91%], loss is 1.176882\n",
      "Training Epoch: 175, [420/560, 81%], loss is 1.596309\n",
      "Training Epoch: 175, [490/560, 89%], loss is 1.838796\n",
      "Training Epoch: 180, [0/560, 94%], loss is 0.153302\n",
      "Training Epoch: 180, [70/560, 89%], loss is 0.451565\n",
      "Training Epoch: 180, [140/560, 87%], loss is 0.754472\n",
      "Training Epoch: 180, [210/560, 89%], loss is 1.106245\n",
      "Training Epoch: 180, [280/560, 86%], loss is 1.630056\n",
      "Training Epoch: 180, [350/560, 91%], loss is 1.813926\n",
      "Training Epoch: 180, [420/560, 90%], loss is 2.072369\n",
      "Training Epoch: 180, [490/560, 91%], loss is 2.313903\n",
      "Training Epoch: 185, [0/560, 91%], loss is 0.267713\n",
      "Training Epoch: 185, [70/560, 96%], loss is 0.441674\n",
      "Training Epoch: 185, [140/560, 89%], loss is 0.750664\n",
      "Training Epoch: 185, [210/560, 81%], loss is 1.223796\n",
      "Training Epoch: 185, [280/560, 86%], loss is 1.685672\n",
      "Training Epoch: 185, [350/560, 86%], loss is 2.060533\n",
      "Training Epoch: 185, [420/560, 91%], loss is 2.348924\n",
      "Training Epoch: 185, [490/560, 87%], loss is 2.670294\n",
      "Training Epoch: 190, [0/560, 84%], loss is 0.380501\n",
      "Training Epoch: 190, [70/560, 91%], loss is 0.630785\n",
      "Training Epoch: 190, [140/560, 80%], loss is 1.139160\n",
      "Training Epoch: 190, [210/560, 87%], loss is 1.448164\n",
      "Training Epoch: 190, [280/560, 91%], loss is 1.710653\n",
      "Training Epoch: 190, [350/560, 90%], loss is 1.944246\n",
      "Training Epoch: 190, [420/560, 90%], loss is 2.277195\n",
      "Training Epoch: 190, [490/560, 89%], loss is 2.608908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 195, [0/560, 84%], loss is 0.358196\n",
      "Training Epoch: 195, [70/560, 93%], loss is 0.629723\n",
      "Training Epoch: 195, [140/560, 89%], loss is 0.990233\n",
      "Training Epoch: 195, [210/560, 93%], loss is 1.198888\n",
      "Training Epoch: 195, [280/560, 86%], loss is 1.562058\n",
      "Training Epoch: 195, [350/560, 90%], loss is 1.848939\n",
      "Training Epoch: 195, [420/560, 91%], loss is 2.127327\n",
      "Training Epoch: 195, [490/560, 84%], loss is 2.522222\n",
      "Training Epoch: 200, [0/560, 93%], loss is 0.240352\n",
      "Training Epoch: 200, [70/560, 90%], loss is 0.529191\n",
      "Training Epoch: 200, [140/560, 97%], loss is 0.723074\n",
      "Training Epoch: 200, [210/560, 91%], loss is 0.910237\n",
      "Training Epoch: 200, [280/560, 89%], loss is 1.274455\n",
      "Training Epoch: 200, [350/560, 91%], loss is 1.510691\n",
      "Training Epoch: 200, [420/560, 89%], loss is 1.753767\n",
      "Training Epoch: 200, [490/560, 91%], loss is 1.977586\n",
      "Training Epoch: 205, [0/560, 80%], loss is 0.378871\n",
      "Training Epoch: 205, [70/560, 87%], loss is 0.676513\n",
      "Training Epoch: 205, [140/560, 94%], loss is 0.889039\n",
      "Training Epoch: 205, [210/560, 91%], loss is 1.190173\n",
      "Training Epoch: 205, [280/560, 94%], loss is 1.375717\n",
      "Training Epoch: 205, [350/560, 90%], loss is 1.673786\n",
      "Training Epoch: 205, [420/560, 91%], loss is 1.931269\n",
      "Training Epoch: 205, [490/560, 90%], loss is 2.213413\n",
      "Training Epoch: 210, [0/560, 91%], loss is 0.210242\n",
      "Training Epoch: 210, [70/560, 89%], loss is 0.485061\n",
      "Training Epoch: 210, [140/560, 91%], loss is 0.714811\n",
      "Training Epoch: 210, [210/560, 94%], loss is 0.861902\n",
      "Training Epoch: 210, [280/560, 93%], loss is 1.081021\n",
      "Training Epoch: 210, [350/560, 94%], loss is 1.272411\n",
      "Training Epoch: 210, [420/560, 91%], loss is 1.486881\n",
      "Training Epoch: 210, [490/560, 91%], loss is 1.759175\n",
      "Training Epoch: 215, [0/560, 89%], loss is 0.328693\n",
      "Training Epoch: 215, [70/560, 89%], loss is 0.699892\n",
      "Training Epoch: 215, [140/560, 90%], loss is 0.985624\n",
      "Training Epoch: 215, [210/560, 90%], loss is 1.332841\n",
      "Training Epoch: 215, [280/560, 93%], loss is 1.690621\n",
      "Training Epoch: 215, [350/560, 99%], loss is 1.790791\n",
      "Training Epoch: 215, [420/560, 84%], loss is 2.150286\n",
      "Training Epoch: 215, [490/560, 87%], loss is 2.409368\n",
      "Training Epoch: 220, [0/560, 86%], loss is 0.390367\n",
      "Training Epoch: 220, [70/560, 94%], loss is 0.587369\n",
      "Training Epoch: 220, [140/560, 89%], loss is 0.910598\n",
      "Training Epoch: 220, [210/560, 83%], loss is 1.318376\n",
      "Training Epoch: 220, [280/560, 84%], loss is 1.750461\n",
      "Training Epoch: 220, [350/560, 86%], loss is 2.056111\n",
      "Training Epoch: 220, [420/560, 84%], loss is 2.434685\n",
      "Training Epoch: 220, [490/560, 86%], loss is 2.702549\n",
      "Training Epoch: 225, [0/560, 94%], loss is 0.130753\n",
      "Training Epoch: 225, [70/560, 97%], loss is 0.315501\n",
      "Training Epoch: 225, [140/560, 87%], loss is 0.583376\n",
      "Training Epoch: 225, [210/560, 96%], loss is 0.744257\n",
      "Training Epoch: 225, [280/560, 91%], loss is 0.962000\n",
      "Training Epoch: 225, [350/560, 89%], loss is 1.238500\n",
      "Training Epoch: 225, [420/560, 94%], loss is 1.397014\n",
      "Training Epoch: 225, [490/560, 89%], loss is 1.586740\n",
      "Training Epoch: 230, [0/560, 91%], loss is 0.210107\n",
      "Training Epoch: 230, [70/560, 93%], loss is 0.347155\n",
      "Training Epoch: 230, [140/560, 89%], loss is 0.661698\n",
      "Training Epoch: 230, [210/560, 89%], loss is 0.970098\n",
      "Training Epoch: 230, [280/560, 94%], loss is 1.160570\n",
      "Training Epoch: 230, [350/560, 89%], loss is 1.479587\n",
      "Training Epoch: 230, [420/560, 86%], loss is 1.819525\n",
      "Training Epoch: 230, [490/560, 87%], loss is 2.092222\n",
      "Training Epoch: 235, [0/560, 94%], loss is 0.137416\n",
      "Training Epoch: 235, [70/560, 93%], loss is 0.348084\n",
      "Training Epoch: 235, [140/560, 93%], loss is 0.501792\n",
      "Training Epoch: 235, [210/560, 96%], loss is 0.617984\n",
      "Training Epoch: 235, [280/560, 90%], loss is 0.848264\n",
      "Training Epoch: 235, [350/560, 89%], loss is 1.070685\n",
      "Training Epoch: 235, [420/560, 94%], loss is 1.296212\n",
      "Training Epoch: 235, [490/560, 89%], loss is 1.551441\n",
      "Training Epoch: 240, [0/560, 89%], loss is 0.233251\n",
      "Training Epoch: 240, [70/560, 86%], loss is 0.670822\n",
      "Training Epoch: 240, [140/560, 87%], loss is 0.981813\n",
      "Training Epoch: 240, [210/560, 93%], loss is 1.198410\n",
      "Training Epoch: 240, [280/560, 91%], loss is 1.452789\n",
      "Training Epoch: 240, [350/560, 90%], loss is 1.680315\n",
      "Training Epoch: 240, [420/560, 87%], loss is 2.019885\n",
      "Training Epoch: 240, [490/560, 91%], loss is 2.191120\n",
      "Training Epoch: 245, [0/560, 97%], loss is 0.123296\n",
      "Best Epoch is 245\n",
      "Training Epoch: 245, [70/560, 96%], loss is 0.329970\n",
      "Training Epoch: 245, [140/560, 94%], loss is 0.540082\n",
      "Training Epoch: 245, [210/560, 89%], loss is 0.824105\n",
      "Training Epoch: 245, [280/560, 93%], loss is 1.035903\n",
      "Training Epoch: 245, [350/560, 90%], loss is 1.323332\n",
      "Training Epoch: 245, [420/560, 94%], loss is 1.530132\n",
      "Training Epoch: 245, [490/560, 90%], loss is 1.767677\n",
      "Training Epoch: 250, [0/560, 94%], loss is 0.167350\n",
      "Training Epoch: 250, [70/560, 87%], loss is 0.519721\n",
      "Training Epoch: 250, [140/560, 84%], loss is 0.900001\n",
      "Training Epoch: 250, [210/560, 93%], loss is 1.131519\n",
      "Training Epoch: 250, [280/560, 96%], loss is 1.297219\n",
      "Training Epoch: 250, [350/560, 87%], loss is 1.702214\n",
      "Training Epoch: 250, [420/560, 90%], loss is 1.958454\n",
      "Training Epoch: 250, [490/560, 87%], loss is 2.392215\n",
      "Training Epoch: 255, [0/560, 87%], loss is 0.346854\n",
      "Training Epoch: 255, [70/560, 90%], loss is 0.605180\n",
      "Training Epoch: 255, [140/560, 89%], loss is 0.933263\n",
      "Training Epoch: 255, [210/560, 90%], loss is 1.146662\n",
      "Training Epoch: 255, [280/560, 86%], loss is 1.521171\n",
      "Training Epoch: 255, [350/560, 76%], loss is 2.149744\n",
      "Training Epoch: 255, [420/560, 90%], loss is 2.480186\n",
      "Training Epoch: 255, [490/560, 90%], loss is 2.791499\n",
      "Training Epoch: 260, [0/560, 93%], loss is 0.288129\n",
      "Training Epoch: 260, [70/560, 87%], loss is 0.609666\n",
      "Training Epoch: 260, [140/560, 90%], loss is 0.911287\n",
      "Training Epoch: 260, [210/560, 90%], loss is 1.199658\n",
      "Training Epoch: 260, [280/560, 87%], loss is 1.509138\n",
      "Training Epoch: 260, [350/560, 84%], loss is 1.880128\n",
      "Training Epoch: 260, [420/560, 84%], loss is 2.193221\n",
      "Training Epoch: 260, [490/560, 89%], loss is 2.456310\n",
      "Training Epoch: 265, [0/560, 93%], loss is 0.233668\n",
      "Training Epoch: 265, [70/560, 89%], loss is 0.631041\n",
      "Training Epoch: 265, [140/560, 91%], loss is 0.865705\n",
      "Training Epoch: 265, [210/560, 93%], loss is 1.028223\n",
      "Training Epoch: 265, [280/560, 97%], loss is 1.150275\n",
      "Training Epoch: 265, [350/560, 97%], loss is 1.311534\n",
      "Training Epoch: 265, [420/560, 93%], loss is 1.580512\n",
      "Training Epoch: 265, [490/560, 96%], loss is 1.707958\n",
      "Training Epoch: 270, [0/560, 89%], loss is 0.476429\n",
      "Training Epoch: 270, [70/560, 90%], loss is 1.058665\n",
      "Training Epoch: 270, [140/560, 87%], loss is 1.454374\n",
      "Training Epoch: 270, [210/560, 90%], loss is 1.784024\n",
      "Training Epoch: 270, [280/560, 91%], loss is 2.020102\n",
      "Training Epoch: 270, [350/560, 83%], loss is 2.372273\n",
      "Training Epoch: 270, [420/560, 87%], loss is 2.716162\n",
      "Training Epoch: 270, [490/560, 90%], loss is 3.063677\n",
      "Training Epoch: 275, [0/560, 89%], loss is 0.296727\n",
      "Training Epoch: 275, [70/560, 84%], loss is 0.572836\n",
      "Training Epoch: 275, [140/560, 94%], loss is 0.712321\n",
      "Training Epoch: 275, [210/560, 91%], loss is 0.992369\n",
      "Training Epoch: 275, [280/560, 91%], loss is 1.278541\n",
      "Training Epoch: 275, [350/560, 84%], loss is 1.671182\n",
      "Training Epoch: 275, [420/560, 90%], loss is 1.975401\n",
      "Training Epoch: 275, [490/560, 93%], loss is 2.171493\n",
      "Training Epoch: 280, [0/560, 96%], loss is 0.152543\n",
      "Training Epoch: 280, [70/560, 93%], loss is 0.343946\n",
      "Training Epoch: 280, [140/560, 93%], loss is 0.541673\n",
      "Training Epoch: 280, [210/560, 91%], loss is 0.754983\n",
      "Training Epoch: 280, [280/560, 96%], loss is 0.874287\n",
      "Training Epoch: 280, [350/560, 93%], loss is 1.080139\n",
      "Training Epoch: 280, [420/560, 90%], loss is 1.253125\n",
      "Training Epoch: 280, [490/560, 90%], loss is 1.490834\n",
      "Training Epoch: 285, [0/560, 93%], loss is 0.198576\n",
      "Training Epoch: 285, [70/560, 90%], loss is 0.549213\n",
      "Training Epoch: 285, [140/560, 87%], loss is 0.866026\n",
      "Training Epoch: 285, [210/560, 87%], loss is 1.093488\n",
      "Training Epoch: 285, [280/560, 99%], loss is 1.178296\n",
      "Training Epoch: 285, [350/560, 96%], loss is 1.276293\n",
      "Training Epoch: 285, [420/560, 89%], loss is 1.521699\n",
      "Training Epoch: 285, [490/560, 94%], loss is 1.679943\n",
      "Training Epoch: 290, [0/560, 91%], loss is 0.231779\n",
      "Training Epoch: 290, [70/560, 94%], loss is 0.389930\n",
      "Training Epoch: 290, [140/560, 89%], loss is 0.772437\n",
      "Training Epoch: 290, [210/560, 89%], loss is 1.014020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 290, [280/560, 89%], loss is 1.258438\n",
      "Training Epoch: 290, [350/560, 97%], loss is 1.363312\n",
      "Training Epoch: 290, [420/560, 91%], loss is 1.660851\n",
      "Training Epoch: 290, [490/560, 90%], loss is 1.982271\n",
      "Training Epoch: 295, [0/560, 90%], loss is 0.254792\n",
      "Training Epoch: 295, [70/560, 91%], loss is 0.476457\n",
      "Training Epoch: 295, [140/560, 83%], loss is 0.830497\n",
      "Training Epoch: 295, [210/560, 84%], loss is 1.232788\n",
      "Training Epoch: 295, [280/560, 96%], loss is 1.356076\n",
      "Training Epoch: 295, [350/560, 91%], loss is 1.537360\n",
      "Training Epoch: 295, [420/560, 93%], loss is 1.808366\n",
      "Training Epoch: 295, [490/560, 89%], loss is 2.191907\n",
      "Training Epoch: 300, [0/560, 93%], loss is 0.185356\n",
      "Training Epoch: 300, [70/560, 90%], loss is 0.379850\n",
      "Training Epoch: 300, [140/560, 93%], loss is 0.583175\n",
      "Training Epoch: 300, [210/560, 93%], loss is 0.812672\n",
      "Training Epoch: 300, [280/560, 97%], loss is 0.878889\n",
      "Training Epoch: 300, [350/560, 91%], loss is 1.126042\n",
      "Training Epoch: 300, [420/560, 90%], loss is 1.399896\n",
      "Training Epoch: 300, [490/560, 91%], loss is 1.647758\n",
      "Training Epoch: 305, [0/560, 91%], loss is 0.285771\n",
      "Training Epoch: 305, [70/560, 89%], loss is 0.528968\n",
      "Training Epoch: 305, [140/560, 93%], loss is 0.724342\n",
      "Training Epoch: 305, [210/560, 91%], loss is 1.024928\n",
      "Training Epoch: 305, [280/560, 93%], loss is 1.204716\n",
      "Training Epoch: 305, [350/560, 94%], loss is 1.356045\n",
      "Training Epoch: 305, [420/560, 94%], loss is 1.503682\n",
      "Training Epoch: 305, [490/560, 94%], loss is 1.707206\n",
      "Training Epoch: 310, [0/560, 89%], loss is 0.208687\n",
      "Training Epoch: 310, [70/560, 93%], loss is 0.419025\n",
      "Training Epoch: 310, [140/560, 93%], loss is 0.571579\n",
      "Training Epoch: 310, [210/560, 89%], loss is 0.870863\n",
      "Training Epoch: 310, [280/560, 93%], loss is 1.110406\n",
      "Training Epoch: 310, [350/560, 90%], loss is 1.339580\n",
      "Training Epoch: 310, [420/560, 91%], loss is 1.529121\n",
      "Training Epoch: 310, [490/560, 96%], loss is 1.675713\n",
      "Training Epoch: 315, [0/560, 99%], loss is 0.093646\n",
      "Best Epoch is 315\n",
      "Training Epoch: 315, [70/560, 93%], loss is 0.293759\n",
      "Training Epoch: 315, [140/560, 93%], loss is 0.472105\n",
      "Training Epoch: 315, [210/560, 93%], loss is 0.730157\n",
      "Training Epoch: 315, [280/560, 90%], loss is 1.060819\n",
      "Training Epoch: 315, [350/560, 90%], loss is 1.349708\n",
      "Training Epoch: 315, [420/560, 93%], loss is 1.500370\n",
      "Training Epoch: 315, [490/560, 93%], loss is 1.709396\n",
      "Training Epoch: 320, [0/560, 94%], loss is 0.157080\n",
      "Training Epoch: 320, [70/560, 99%], loss is 0.261947\n",
      "Training Epoch: 320, [140/560, 94%], loss is 0.431643\n",
      "Training Epoch: 320, [210/560, 90%], loss is 0.664883\n",
      "Training Epoch: 320, [280/560, 83%], loss is 1.042752\n",
      "Training Epoch: 320, [350/560, 93%], loss is 1.267871\n",
      "Training Epoch: 320, [420/560, 99%], loss is 1.354622\n",
      "Training Epoch: 320, [490/560, 86%], loss is 1.619076\n",
      "Training Epoch: 325, [0/560, 89%], loss is 0.320105\n",
      "Training Epoch: 325, [70/560, 93%], loss is 0.502408\n",
      "Training Epoch: 325, [140/560, 93%], loss is 0.623058\n",
      "Training Epoch: 325, [210/560, 89%], loss is 0.864778\n",
      "Training Epoch: 325, [280/560, 96%], loss is 1.019901\n",
      "Training Epoch: 325, [350/560, 94%], loss is 1.222621\n",
      "Training Epoch: 325, [420/560, 90%], loss is 1.517339\n",
      "Training Epoch: 325, [490/560, 91%], loss is 1.788062\n",
      "Training Epoch: 330, [0/560, 89%], loss is 0.311193\n",
      "Training Epoch: 330, [70/560, 94%], loss is 0.400491\n",
      "Training Epoch: 330, [140/560, 89%], loss is 0.672269\n",
      "Training Epoch: 330, [210/560, 97%], loss is 0.822602\n",
      "Training Epoch: 330, [280/560, 86%], loss is 1.129281\n",
      "Training Epoch: 330, [350/560, 91%], loss is 1.320253\n",
      "Training Epoch: 330, [420/560, 94%], loss is 1.526490\n",
      "Training Epoch: 330, [490/560, 91%], loss is 1.722771\n",
      "Training Epoch: 335, [0/560, 93%], loss is 0.194293\n",
      "Training Epoch: 335, [70/560, 90%], loss is 0.416087\n",
      "Training Epoch: 335, [140/560, 94%], loss is 0.588444\n",
      "Training Epoch: 335, [210/560, 91%], loss is 0.784304\n",
      "Training Epoch: 335, [280/560, 96%], loss is 0.886182\n",
      "Training Epoch: 335, [350/560, 93%], loss is 1.071823\n",
      "Training Epoch: 335, [420/560, 91%], loss is 1.256622\n",
      "Training Epoch: 335, [490/560, 93%], loss is 1.448103\n",
      "Training Epoch: 340, [0/560, 94%], loss is 0.149027\n",
      "Training Epoch: 340, [70/560, 87%], loss is 0.470726\n",
      "Training Epoch: 340, [140/560, 87%], loss is 0.839910\n",
      "Training Epoch: 340, [210/560, 90%], loss is 1.095937\n",
      "Training Epoch: 340, [280/560, 93%], loss is 1.300448\n",
      "Training Epoch: 340, [350/560, 93%], loss is 1.518655\n",
      "Training Epoch: 340, [420/560, 90%], loss is 1.720092\n",
      "Training Epoch: 340, [490/560, 89%], loss is 1.943377\n",
      "Training Epoch: 345, [0/560, 96%], loss is 0.118238\n",
      "Training Epoch: 345, [70/560, 93%], loss is 0.325822\n",
      "Training Epoch: 345, [140/560, 93%], loss is 0.483636\n",
      "Training Epoch: 345, [210/560, 96%], loss is 0.600292\n",
      "Training Epoch: 345, [280/560, 91%], loss is 0.822957\n",
      "Training Epoch: 345, [350/560, 90%], loss is 1.007557\n",
      "Training Epoch: 345, [420/560, 99%], loss is 1.128399\n",
      "Training Epoch: 345, [490/560, 96%], loss is 1.275198\n",
      "Training Epoch: 350, [0/560, 94%], loss is 0.107921\n",
      "Training Epoch: 350, [70/560, 93%], loss is 0.289712\n",
      "Training Epoch: 350, [140/560, 91%], loss is 0.541670\n",
      "Training Epoch: 350, [210/560, 93%], loss is 0.748583\n",
      "Training Epoch: 350, [280/560, 94%], loss is 0.871829\n",
      "Training Epoch: 350, [350/560, 86%], loss is 1.277512\n",
      "Training Epoch: 350, [420/560, 96%], loss is 1.414934\n",
      "Training Epoch: 350, [490/560, 90%], loss is 1.617019\n",
      "Training Epoch: 355, [0/560, 90%], loss is 0.316671\n",
      "Training Epoch: 355, [70/560, 87%], loss is 0.652430\n",
      "Training Epoch: 355, [140/560, 86%], loss is 1.035086\n",
      "Training Epoch: 355, [210/560, 83%], loss is 1.335240\n",
      "Training Epoch: 355, [280/560, 97%], loss is 1.488817\n",
      "Training Epoch: 355, [350/560, 94%], loss is 1.662418\n",
      "Training Epoch: 355, [420/560, 83%], loss is 2.153995\n",
      "Training Epoch: 355, [490/560, 86%], loss is 2.483638\n",
      "Training Epoch: 360, [0/560, 93%], loss is 0.146194\n",
      "Training Epoch: 360, [70/560, 96%], loss is 0.276820\n",
      "Training Epoch: 360, [140/560, 86%], loss is 0.723578\n",
      "Training Epoch: 360, [210/560, 93%], loss is 0.863012\n",
      "Training Epoch: 360, [280/560, 91%], loss is 1.080857\n",
      "Training Epoch: 360, [350/560, 91%], loss is 1.263109\n",
      "Training Epoch: 360, [420/560, 90%], loss is 1.569031\n",
      "Training Epoch: 360, [490/560, 91%], loss is 1.832632\n",
      "Training Epoch: 365, [0/560, 93%], loss is 0.132379\n",
      "Training Epoch: 365, [70/560, 94%], loss is 0.396673\n",
      "Training Epoch: 365, [140/560, 96%], loss is 0.562683\n",
      "Training Epoch: 365, [210/560, 93%], loss is 0.727419\n",
      "Training Epoch: 365, [280/560, 93%], loss is 0.887339\n",
      "Training Epoch: 365, [350/560, 90%], loss is 1.124442\n",
      "Training Epoch: 365, [420/560, 89%], loss is 1.373529\n",
      "Training Epoch: 365, [490/560, 94%], loss is 1.588169\n",
      "Training Epoch: 370, [0/560, 94%], loss is 0.154925\n",
      "Training Epoch: 370, [70/560, 94%], loss is 0.298449\n",
      "Training Epoch: 370, [140/560, 89%], loss is 0.532567\n",
      "Training Epoch: 370, [210/560, 93%], loss is 0.720850\n",
      "Training Epoch: 370, [280/560, 94%], loss is 0.864149\n",
      "Training Epoch: 370, [350/560, 96%], loss is 0.955448\n",
      "Training Epoch: 370, [420/560, 86%], loss is 1.207782\n",
      "Training Epoch: 370, [490/560, 94%], loss is 1.349324\n",
      "Training Epoch: 375, [0/560, 90%], loss is 0.235231\n",
      "Training Epoch: 375, [70/560, 96%], loss is 0.336343\n",
      "Training Epoch: 375, [140/560, 93%], loss is 0.506353\n",
      "Training Epoch: 375, [210/560, 93%], loss is 0.686028\n",
      "Training Epoch: 375, [280/560, 93%], loss is 0.856456\n",
      "Training Epoch: 375, [350/560, 87%], loss is 1.174179\n",
      "Training Epoch: 375, [420/560, 91%], loss is 1.471093\n",
      "Training Epoch: 375, [490/560, 91%], loss is 1.754658\n",
      "Training Epoch: 380, [0/560, 93%], loss is 0.158035\n",
      "Training Epoch: 380, [70/560, 84%], loss is 0.492723\n",
      "Training Epoch: 380, [140/560, 96%], loss is 0.697821\n",
      "Training Epoch: 380, [210/560, 93%], loss is 0.868238\n",
      "Training Epoch: 380, [280/560, 87%], loss is 1.199566\n",
      "Training Epoch: 380, [350/560, 90%], loss is 1.373492\n",
      "Training Epoch: 380, [420/560, 90%], loss is 1.579385\n",
      "Training Epoch: 380, [490/560, 93%], loss is 1.762411\n",
      "Training Epoch: 385, [0/560, 93%], loss is 0.198596\n",
      "Training Epoch: 385, [70/560, 97%], loss is 0.309358\n",
      "Training Epoch: 385, [140/560, 96%], loss is 0.403504\n",
      "Training Epoch: 385, [210/560, 93%], loss is 0.580962\n",
      "Training Epoch: 385, [280/560, 94%], loss is 0.753299\n",
      "Training Epoch: 385, [350/560, 93%], loss is 0.926032\n",
      "Training Epoch: 385, [420/560, 99%], loss is 1.010365\n",
      "Training Epoch: 385, [490/560, 89%], loss is 1.248814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 390, [0/560, 93%], loss is 0.170459\n",
      "Training Epoch: 390, [70/560, 90%], loss is 0.342867\n",
      "Training Epoch: 390, [140/560, 94%], loss is 0.539894\n",
      "Training Epoch: 390, [210/560, 90%], loss is 0.747378\n",
      "Training Epoch: 390, [280/560, 93%], loss is 0.906161\n",
      "Training Epoch: 390, [350/560, 90%], loss is 1.146312\n",
      "Training Epoch: 390, [420/560, 86%], loss is 1.394982\n",
      "Training Epoch: 390, [490/560, 96%], loss is 1.558121\n",
      "Training Epoch: 395, [0/560, 87%], loss is 0.325963\n",
      "Training Epoch: 395, [70/560, 90%], loss is 0.528606\n",
      "Training Epoch: 395, [140/560, 87%], loss is 0.915777\n",
      "Training Epoch: 395, [210/560, 91%], loss is 1.144340\n",
      "Training Epoch: 395, [280/560, 86%], loss is 1.477746\n",
      "Training Epoch: 395, [350/560, 94%], loss is 1.603118\n",
      "Training Epoch: 395, [420/560, 99%], loss is 1.721637\n",
      "Training Epoch: 395, [490/560, 94%], loss is 1.927827\n",
      "Training Epoch: 400, [0/560, 94%], loss is 0.118120\n",
      "Training Epoch: 400, [70/560, 90%], loss is 0.314401\n",
      "Training Epoch: 400, [140/560, 94%], loss is 0.421444\n",
      "Training Epoch: 400, [210/560, 91%], loss is 0.644711\n",
      "Training Epoch: 400, [280/560, 97%], loss is 0.749087\n",
      "Training Epoch: 400, [350/560, 93%], loss is 0.956806\n",
      "Training Epoch: 400, [420/560, 91%], loss is 1.135697\n",
      "Training Epoch: 400, [490/560, 94%], loss is 1.287380\n",
      "Training Epoch: 405, [0/560, 96%], loss is 0.168513\n",
      "Training Epoch: 405, [70/560, 91%], loss is 0.414548\n",
      "Training Epoch: 405, [140/560, 86%], loss is 0.826715\n",
      "Training Epoch: 405, [210/560, 97%], loss is 0.934396\n",
      "Training Epoch: 405, [280/560, 90%], loss is 1.165691\n",
      "Training Epoch: 405, [350/560, 91%], loss is 1.403488\n",
      "Training Epoch: 405, [420/560, 94%], loss is 1.608194\n",
      "Training Epoch: 405, [490/560, 93%], loss is 1.765129\n",
      "Training Epoch: 410, [0/560, 79%], loss is 0.506094\n",
      "Training Epoch: 410, [70/560, 90%], loss is 0.742078\n",
      "Training Epoch: 410, [140/560, 86%], loss is 0.972524\n",
      "Training Epoch: 410, [210/560, 94%], loss is 1.090817\n",
      "Training Epoch: 410, [280/560, 90%], loss is 1.314287\n",
      "Training Epoch: 410, [350/560, 90%], loss is 1.543051\n",
      "Training Epoch: 410, [420/560, 90%], loss is 1.747188\n",
      "Training Epoch: 410, [490/560, 84%], loss is 2.198420\n",
      "Training Epoch: 415, [0/560, 93%], loss is 0.134066\n",
      "Training Epoch: 415, [70/560, 93%], loss is 0.301141\n",
      "Training Epoch: 415, [140/560, 89%], loss is 0.529436\n",
      "Training Epoch: 415, [210/560, 94%], loss is 0.701612\n",
      "Training Epoch: 415, [280/560, 93%], loss is 0.923533\n",
      "Training Epoch: 415, [350/560, 94%], loss is 1.056290\n",
      "Training Epoch: 415, [420/560, 90%], loss is 1.264240\n",
      "Training Epoch: 415, [490/560, 91%], loss is 1.607551\n",
      "Training Epoch: 420, [0/560, 90%], loss is 0.282983\n",
      "Training Epoch: 420, [70/560, 91%], loss is 0.476373\n",
      "Training Epoch: 420, [140/560, 96%], loss is 0.579615\n",
      "Training Epoch: 420, [210/560, 89%], loss is 0.938179\n",
      "Training Epoch: 420, [280/560, 90%], loss is 1.216734\n",
      "Training Epoch: 420, [350/560, 93%], loss is 1.357405\n",
      "Training Epoch: 420, [420/560, 89%], loss is 1.569824\n",
      "Training Epoch: 420, [490/560, 86%], loss is 1.894808\n",
      "Training Epoch: 425, [0/560, 94%], loss is 0.199005\n",
      "Training Epoch: 425, [70/560, 86%], loss is 0.560378\n",
      "Training Epoch: 425, [140/560, 93%], loss is 0.885618\n",
      "Training Epoch: 425, [210/560, 91%], loss is 1.141591\n",
      "Training Epoch: 425, [280/560, 89%], loss is 1.477376\n",
      "Training Epoch: 425, [350/560, 90%], loss is 1.730661\n",
      "Training Epoch: 425, [420/560, 90%], loss is 1.951868\n",
      "Training Epoch: 425, [490/560, 93%], loss is 2.102417\n",
      "Training Epoch: 430, [0/560, 96%], loss is 0.128939\n",
      "Training Epoch: 430, [70/560, 93%], loss is 0.329635\n",
      "Training Epoch: 430, [140/560, 91%], loss is 0.542228\n",
      "Training Epoch: 430, [210/560, 86%], loss is 0.924244\n",
      "Training Epoch: 430, [280/560, 96%], loss is 1.044696\n",
      "Training Epoch: 430, [350/560, 99%], loss is 1.125396\n",
      "Training Epoch: 430, [420/560, 94%], loss is 1.275794\n",
      "Training Epoch: 430, [490/560, 91%], loss is 1.492530\n",
      "Training Epoch: 435, [0/560, 94%], loss is 0.142289\n",
      "Training Epoch: 435, [70/560, 94%], loss is 0.339112\n",
      "Training Epoch: 435, [140/560, 94%], loss is 0.528820\n",
      "Training Epoch: 435, [210/560, 97%], loss is 0.642536\n",
      "Training Epoch: 435, [280/560, 89%], loss is 0.889103\n",
      "Training Epoch: 435, [350/560, 97%], loss is 0.999806\n",
      "Training Epoch: 435, [420/560, 94%], loss is 1.112945\n",
      "Training Epoch: 435, [490/560, 93%], loss is 1.273430\n",
      "Training Epoch: 440, [0/560, 96%], loss is 0.142346\n",
      "Training Epoch: 440, [70/560, 90%], loss is 0.442583\n",
      "Training Epoch: 440, [140/560, 93%], loss is 0.724118\n",
      "Training Epoch: 440, [210/560, 90%], loss is 0.994123\n",
      "Training Epoch: 440, [280/560, 93%], loss is 1.138825\n",
      "Training Epoch: 440, [350/560, 86%], loss is 1.481428\n",
      "Training Epoch: 440, [420/560, 91%], loss is 1.706999\n",
      "Training Epoch: 440, [490/560, 97%], loss is 1.807840\n",
      "Training Epoch: 445, [0/560, 91%], loss is 0.190083\n",
      "Training Epoch: 445, [70/560, 97%], loss is 0.318001\n",
      "Training Epoch: 445, [140/560, 87%], loss is 0.585162\n",
      "Training Epoch: 445, [210/560, 94%], loss is 0.734705\n",
      "Training Epoch: 445, [280/560, 93%], loss is 0.925498\n",
      "Training Epoch: 445, [350/560, 91%], loss is 1.076663\n",
      "Training Epoch: 445, [420/560, 90%], loss is 1.279405\n",
      "Training Epoch: 445, [490/560, 97%], loss is 1.395359\n",
      "Training Epoch: 450, [0/560, 93%], loss is 0.192135\n",
      "Training Epoch: 450, [70/560, 87%], loss is 0.432391\n",
      "Training Epoch: 450, [140/560, 93%], loss is 0.644220\n",
      "Training Epoch: 450, [210/560, 90%], loss is 0.876306\n",
      "Training Epoch: 450, [280/560, 96%], loss is 1.313724\n",
      "Training Epoch: 450, [350/560, 89%], loss is 1.581151\n",
      "Training Epoch: 450, [420/560, 90%], loss is 1.900235\n",
      "Training Epoch: 450, [490/560, 90%], loss is 2.229030\n",
      "Training Epoch: 455, [0/560, 93%], loss is 0.162598\n",
      "Training Epoch: 455, [70/560, 90%], loss is 0.376907\n",
      "Training Epoch: 455, [140/560, 91%], loss is 0.621871\n",
      "Training Epoch: 455, [210/560, 90%], loss is 0.897308\n",
      "Training Epoch: 455, [280/560, 96%], loss is 1.042179\n",
      "Training Epoch: 455, [350/560, 87%], loss is 1.329605\n",
      "Training Epoch: 455, [420/560, 93%], loss is 1.672529\n",
      "Training Epoch: 455, [490/560, 89%], loss is 1.881270\n",
      "Training Epoch: 460, [0/560, 96%], loss is 0.186545\n",
      "Training Epoch: 460, [70/560, 94%], loss is 0.356229\n",
      "Training Epoch: 460, [140/560, 93%], loss is 0.528343\n",
      "Training Epoch: 460, [210/560, 91%], loss is 0.746559\n",
      "Training Epoch: 460, [280/560, 86%], loss is 1.073820\n",
      "Training Epoch: 460, [350/560, 91%], loss is 1.280191\n",
      "Training Epoch: 460, [420/560, 94%], loss is 1.441262\n",
      "Training Epoch: 460, [490/560, 90%], loss is 1.754870\n",
      "Training Epoch: 465, [0/560, 94%], loss is 0.150468\n",
      "Training Epoch: 465, [70/560, 93%], loss is 0.391941\n",
      "Training Epoch: 465, [140/560, 100%], loss is 0.471665\n",
      "Training Epoch: 465, [210/560, 93%], loss is 0.662673\n",
      "Training Epoch: 465, [280/560, 91%], loss is 0.892997\n",
      "Training Epoch: 465, [350/560, 91%], loss is 1.024322\n",
      "Training Epoch: 465, [420/560, 93%], loss is 1.180337\n",
      "Training Epoch: 465, [490/560, 87%], loss is 1.495705\n",
      "Training Epoch: 470, [0/560, 93%], loss is 0.144188\n",
      "Training Epoch: 470, [70/560, 90%], loss is 0.367033\n",
      "Training Epoch: 470, [140/560, 94%], loss is 0.499908\n",
      "Training Epoch: 470, [210/560, 91%], loss is 0.680826\n",
      "Training Epoch: 470, [280/560, 91%], loss is 0.956040\n",
      "Training Epoch: 470, [350/560, 96%], loss is 1.085276\n",
      "Training Epoch: 470, [420/560, 94%], loss is 1.211681\n",
      "Training Epoch: 470, [490/560, 90%], loss is 1.437347\n",
      "Training Epoch: 475, [0/560, 93%], loss is 0.146506\n",
      "Training Epoch: 475, [70/560, 91%], loss is 0.340688\n",
      "Training Epoch: 475, [140/560, 99%], loss is 0.437992\n",
      "Training Epoch: 475, [210/560, 94%], loss is 0.583835\n",
      "Training Epoch: 475, [280/560, 99%], loss is 0.674713\n",
      "Training Epoch: 475, [350/560, 100%], loss is 0.736551\n",
      "Training Epoch: 475, [420/560, 93%], loss is 0.938438\n",
      "Training Epoch: 475, [490/560, 91%], loss is 1.128891\n",
      "Training Epoch: 480, [0/560, 91%], loss is 0.174671\n",
      "Training Epoch: 480, [70/560, 93%], loss is 0.360886\n",
      "Training Epoch: 480, [140/560, 99%], loss is 0.408512\n",
      "Training Epoch: 480, [210/560, 94%], loss is 0.650004\n",
      "Training Epoch: 480, [280/560, 96%], loss is 0.756746\n",
      "Training Epoch: 480, [350/560, 91%], loss is 0.928615\n",
      "Training Epoch: 480, [420/560, 100%], loss is 0.993419\n",
      "Training Epoch: 480, [490/560, 96%], loss is 1.182083\n",
      "Training Epoch: 485, [0/560, 89%], loss is 0.197677\n",
      "Training Epoch: 485, [70/560, 91%], loss is 0.425612\n",
      "Training Epoch: 485, [140/560, 96%], loss is 0.567287\n",
      "Training Epoch: 485, [210/560, 91%], loss is 0.823133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 485, [280/560, 96%], loss is 0.914539\n",
      "Training Epoch: 485, [350/560, 89%], loss is 1.230431\n",
      "Training Epoch: 485, [420/560, 96%], loss is 1.389185\n",
      "Training Epoch: 485, [490/560, 91%], loss is 1.613165\n",
      "Training Epoch: 490, [0/560, 87%], loss is 0.308515\n",
      "Training Epoch: 490, [70/560, 87%], loss is 0.514246\n",
      "Training Epoch: 490, [140/560, 91%], loss is 0.725469\n",
      "Training Epoch: 490, [210/560, 93%], loss is 0.939022\n",
      "Training Epoch: 490, [280/560, 93%], loss is 1.126212\n",
      "Training Epoch: 490, [350/560, 87%], loss is 1.641490\n",
      "Training Epoch: 490, [420/560, 87%], loss is 1.953849\n",
      "Training Epoch: 490, [490/560, 90%], loss is 2.328290\n",
      "Training Epoch: 495, [0/560, 91%], loss is 0.199546\n",
      "Training Epoch: 495, [70/560, 93%], loss is 0.399698\n",
      "Training Epoch: 495, [140/560, 87%], loss is 0.725117\n",
      "Training Epoch: 495, [210/560, 89%], loss is 1.019694\n",
      "Training Epoch: 495, [280/560, 90%], loss is 1.237485\n",
      "Training Epoch: 495, [350/560, 91%], loss is 1.454561\n",
      "Training Epoch: 495, [420/560, 91%], loss is 1.700794\n",
      "Training Epoch: 495, [490/560, 94%], loss is 1.860235\n",
      "Training Epoch: 500, [0/560, 94%], loss is 0.128377\n",
      "Training Epoch: 500, [70/560, 99%], loss is 0.242596\n",
      "Training Epoch: 500, [140/560, 97%], loss is 0.334639\n",
      "Training Epoch: 500, [210/560, 96%], loss is 0.427742\n",
      "Training Epoch: 500, [280/560, 90%], loss is 0.606402\n",
      "Training Epoch: 500, [350/560, 97%], loss is 0.711662\n",
      "Training Epoch: 500, [420/560, 94%], loss is 0.843001\n",
      "Training Epoch: 500, [490/560, 97%], loss is 0.964845\n",
      "Training Epoch: 505, [0/560, 90%], loss is 0.272481\n",
      "Training Epoch: 505, [70/560, 91%], loss is 0.505196\n",
      "Training Epoch: 505, [140/560, 99%], loss is 0.583349\n",
      "Training Epoch: 505, [210/560, 93%], loss is 0.835541\n",
      "Training Epoch: 505, [280/560, 94%], loss is 0.968992\n",
      "Training Epoch: 505, [350/560, 99%], loss is 1.062786\n",
      "Training Epoch: 505, [420/560, 96%], loss is 1.198904\n",
      "Training Epoch: 505, [490/560, 96%], loss is 1.288883\n",
      "Training Epoch: 510, [0/560, 96%], loss is 0.097825\n",
      "Training Epoch: 510, [70/560, 93%], loss is 0.295949\n",
      "Training Epoch: 510, [140/560, 100%], loss is 0.398490\n",
      "Training Epoch: 510, [210/560, 94%], loss is 0.533843\n",
      "Training Epoch: 510, [280/560, 93%], loss is 0.693138\n",
      "Training Epoch: 510, [350/560, 97%], loss is 0.787293\n",
      "Training Epoch: 510, [420/560, 94%], loss is 0.980770\n",
      "Training Epoch: 510, [490/560, 93%], loss is 1.168698\n",
      "Training Epoch: 515, [0/560, 94%], loss is 0.178862\n",
      "Training Epoch: 515, [70/560, 93%], loss is 0.356371\n",
      "Training Epoch: 515, [140/560, 93%], loss is 0.532458\n",
      "Training Epoch: 515, [210/560, 94%], loss is 0.675141\n",
      "Training Epoch: 515, [280/560, 96%], loss is 0.812847\n",
      "Training Epoch: 515, [350/560, 94%], loss is 0.950881\n",
      "Training Epoch: 515, [420/560, 96%], loss is 1.020003\n",
      "Training Epoch: 515, [490/560, 96%], loss is 1.141314\n",
      "Training Epoch: 520, [0/560, 94%], loss is 0.184335\n",
      "Training Epoch: 520, [70/560, 93%], loss is 0.413937\n",
      "Training Epoch: 520, [140/560, 86%], loss is 1.048693\n",
      "Training Epoch: 520, [210/560, 93%], loss is 1.208951\n",
      "Training Epoch: 520, [280/560, 91%], loss is 1.535136\n",
      "Training Epoch: 520, [350/560, 90%], loss is 1.767745\n",
      "Training Epoch: 520, [420/560, 93%], loss is 1.920667\n",
      "Training Epoch: 520, [490/560, 90%], loss is 2.277936\n",
      "Training Epoch: 525, [0/560, 81%], loss is 0.457194\n",
      "Training Epoch: 525, [70/560, 86%], loss is 0.679006\n",
      "Training Epoch: 525, [140/560, 90%], loss is 0.933385\n",
      "Training Epoch: 525, [210/560, 90%], loss is 1.168611\n",
      "Training Epoch: 525, [280/560, 96%], loss is 1.302242\n",
      "Training Epoch: 525, [350/560, 90%], loss is 1.537534\n",
      "Training Epoch: 525, [420/560, 89%], loss is 1.836370\n",
      "Training Epoch: 525, [490/560, 90%], loss is 2.030634\n",
      "Training Epoch: 530, [0/560, 93%], loss is 0.141880\n",
      "Training Epoch: 530, [70/560, 91%], loss is 0.322626\n",
      "Training Epoch: 530, [140/560, 97%], loss is 0.398566\n",
      "Training Epoch: 530, [210/560, 96%], loss is 0.525080\n",
      "Training Epoch: 530, [280/560, 94%], loss is 0.629360\n",
      "Training Epoch: 530, [350/560, 91%], loss is 0.822085\n",
      "Training Epoch: 530, [420/560, 96%], loss is 0.963570\n",
      "Training Epoch: 530, [490/560, 94%], loss is 1.269917\n",
      "Training Epoch: 535, [0/560, 96%], loss is 0.089214\n",
      "Best Epoch is 535\n",
      "Training Epoch: 535, [70/560, 96%], loss is 0.216213\n",
      "Training Epoch: 535, [140/560, 91%], loss is 0.424237\n",
      "Training Epoch: 535, [210/560, 90%], loss is 0.638300\n",
      "Training Epoch: 535, [280/560, 91%], loss is 0.877870\n",
      "Training Epoch: 535, [350/560, 93%], loss is 1.038539\n",
      "Training Epoch: 535, [420/560, 94%], loss is 1.191193\n",
      "Training Epoch: 535, [490/560, 83%], loss is 1.470910\n",
      "Training Epoch: 540, [0/560, 94%], loss is 0.136804\n",
      "Training Epoch: 540, [70/560, 96%], loss is 0.240570\n",
      "Training Epoch: 540, [140/560, 97%], loss is 0.380039\n",
      "Training Epoch: 540, [210/560, 94%], loss is 0.525670\n",
      "Training Epoch: 540, [280/560, 87%], loss is 0.898245\n",
      "Training Epoch: 540, [350/560, 96%], loss is 1.025476\n",
      "Training Epoch: 540, [420/560, 94%], loss is 1.171106\n",
      "Training Epoch: 540, [490/560, 93%], loss is 1.290938\n",
      "Training Epoch: 545, [0/560, 93%], loss is 0.178967\n",
      "Training Epoch: 545, [70/560, 94%], loss is 0.338827\n",
      "Training Epoch: 545, [140/560, 99%], loss is 0.443658\n",
      "Training Epoch: 545, [210/560, 87%], loss is 0.709402\n",
      "Training Epoch: 545, [280/560, 96%], loss is 0.842555\n",
      "Training Epoch: 545, [350/560, 94%], loss is 0.977055\n",
      "Training Epoch: 545, [420/560, 91%], loss is 1.228564\n",
      "Training Epoch: 545, [490/560, 91%], loss is 1.497222\n",
      "Training Epoch: 550, [0/560, 96%], loss is 0.114817\n",
      "Training Epoch: 550, [70/560, 97%], loss is 0.237140\n",
      "Training Epoch: 550, [140/560, 89%], loss is 0.493551\n",
      "Training Epoch: 550, [210/560, 97%], loss is 0.598458\n",
      "Training Epoch: 550, [280/560, 93%], loss is 0.726380\n",
      "Training Epoch: 550, [350/560, 93%], loss is 1.025835\n",
      "Training Epoch: 550, [420/560, 91%], loss is 1.411883\n",
      "Training Epoch: 550, [490/560, 97%], loss is 1.534531\n",
      "Training Epoch: 555, [0/560, 89%], loss is 0.411340\n",
      "Training Epoch: 555, [70/560, 90%], loss is 0.638079\n",
      "Training Epoch: 555, [140/560, 93%], loss is 0.852260\n",
      "Training Epoch: 555, [210/560, 84%], loss is 1.176841\n",
      "Training Epoch: 555, [280/560, 90%], loss is 1.519033\n",
      "Training Epoch: 555, [350/560, 93%], loss is 1.777476\n",
      "Training Epoch: 555, [420/560, 89%], loss is 2.071123\n",
      "Training Epoch: 555, [490/560, 99%], loss is 2.158846\n",
      "Training Epoch: 560, [0/560, 93%], loss is 0.147039\n",
      "Training Epoch: 560, [70/560, 96%], loss is 0.324609\n",
      "Training Epoch: 560, [140/560, 90%], loss is 0.642792\n",
      "Training Epoch: 560, [210/560, 89%], loss is 0.907303\n",
      "Training Epoch: 560, [280/560, 90%], loss is 1.115549\n",
      "Training Epoch: 560, [350/560, 93%], loss is 1.296101\n",
      "Training Epoch: 560, [420/560, 91%], loss is 1.485734\n",
      "Training Epoch: 560, [490/560, 97%], loss is 1.597606\n",
      "Training Epoch: 565, [0/560, 100%], loss is 0.042484\n",
      "Best Epoch is 565\n",
      "Training Epoch: 565, [70/560, 93%], loss is 0.189668\n",
      "Training Epoch: 565, [140/560, 97%], loss is 0.264595\n",
      "Training Epoch: 565, [210/560, 99%], loss is 0.373071\n",
      "Training Epoch: 565, [280/560, 94%], loss is 0.529047\n",
      "Training Epoch: 565, [350/560, 89%], loss is 0.707874\n",
      "Training Epoch: 565, [420/560, 93%], loss is 0.848445\n",
      "Training Epoch: 565, [490/560, 94%], loss is 1.010888\n",
      "Training Epoch: 570, [0/560, 96%], loss is 0.138120\n",
      "Training Epoch: 570, [70/560, 93%], loss is 0.352843\n",
      "Training Epoch: 570, [140/560, 91%], loss is 0.537824\n",
      "Training Epoch: 570, [210/560, 89%], loss is 0.790485\n",
      "Training Epoch: 570, [280/560, 93%], loss is 0.992692\n",
      "Training Epoch: 570, [350/560, 96%], loss is 1.123306\n",
      "Training Epoch: 570, [420/560, 94%], loss is 1.244022\n",
      "Training Epoch: 570, [490/560, 87%], loss is 1.526759\n",
      "Training Epoch: 575, [0/560, 84%], loss is 0.286413\n",
      "Training Epoch: 575, [70/560, 93%], loss is 0.434653\n",
      "Training Epoch: 575, [140/560, 94%], loss is 0.561981\n",
      "Training Epoch: 575, [210/560, 93%], loss is 0.764905\n",
      "Training Epoch: 575, [280/560, 86%], loss is 1.008086\n",
      "Training Epoch: 575, [350/560, 91%], loss is 1.180270\n",
      "Training Epoch: 575, [420/560, 96%], loss is 1.309007\n",
      "Training Epoch: 575, [490/560, 91%], loss is 1.506320\n",
      "Training Epoch: 580, [0/560, 89%], loss is 0.304631\n",
      "Training Epoch: 580, [70/560, 90%], loss is 0.529098\n",
      "Training Epoch: 580, [140/560, 89%], loss is 0.738126\n",
      "Training Epoch: 580, [210/560, 93%], loss is 0.946296\n",
      "Training Epoch: 580, [280/560, 93%], loss is 1.129709\n",
      "Training Epoch: 580, [350/560, 90%], loss is 1.397720\n",
      "Training Epoch: 580, [420/560, 90%], loss is 1.607150\n",
      "Training Epoch: 580, [490/560, 84%], loss is 1.862890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 585, [0/560, 91%], loss is 0.168264\n",
      "Training Epoch: 585, [70/560, 93%], loss is 0.286253\n",
      "Training Epoch: 585, [140/560, 90%], loss is 0.522859\n",
      "Training Epoch: 585, [210/560, 90%], loss is 0.689842\n",
      "Training Epoch: 585, [280/560, 94%], loss is 0.945497\n",
      "Training Epoch: 585, [350/560, 90%], loss is 1.139355\n",
      "Training Epoch: 585, [420/560, 93%], loss is 1.292773\n",
      "Training Epoch: 585, [490/560, 91%], loss is 1.498464\n",
      "Training Epoch: 590, [0/560, 97%], loss is 0.105133\n",
      "Training Epoch: 590, [70/560, 89%], loss is 0.404273\n",
      "Training Epoch: 590, [140/560, 94%], loss is 0.594297\n",
      "Training Epoch: 590, [210/560, 99%], loss is 0.701813\n",
      "Training Epoch: 590, [280/560, 99%], loss is 0.793814\n",
      "Training Epoch: 590, [350/560, 90%], loss is 1.010219\n",
      "Training Epoch: 590, [420/560, 93%], loss is 1.174804\n",
      "Training Epoch: 590, [490/560, 96%], loss is 1.279569\n",
      "Training Epoch: 595, [0/560, 89%], loss is 0.267817\n",
      "Training Epoch: 595, [70/560, 89%], loss is 0.496545\n",
      "Training Epoch: 595, [140/560, 90%], loss is 0.710279\n",
      "Training Epoch: 595, [210/560, 93%], loss is 0.874237\n",
      "Training Epoch: 595, [280/560, 94%], loss is 1.113568\n",
      "Training Epoch: 595, [350/560, 96%], loss is 1.310255\n",
      "Training Epoch: 595, [420/560, 90%], loss is 1.541947\n",
      "Training Epoch: 595, [490/560, 90%], loss is 1.757553\n",
      "Training Epoch: 600, [0/560, 83%], loss is 0.228144\n",
      "Training Epoch: 600, [70/560, 96%], loss is 0.439763\n",
      "Training Epoch: 600, [140/560, 91%], loss is 0.603500\n",
      "Training Epoch: 600, [210/560, 90%], loss is 0.964274\n",
      "Training Epoch: 600, [280/560, 94%], loss is 1.120537\n",
      "Training Epoch: 600, [350/560, 90%], loss is 1.329112\n",
      "Training Epoch: 600, [420/560, 94%], loss is 1.465685\n",
      "Training Epoch: 600, [490/560, 91%], loss is 1.641915\n",
      "Training Epoch: 605, [0/560, 93%], loss is 0.146502\n",
      "Training Epoch: 605, [70/560, 89%], loss is 0.362980\n",
      "Training Epoch: 605, [140/560, 90%], loss is 0.530609\n",
      "Training Epoch: 605, [210/560, 89%], loss is 0.773651\n",
      "Training Epoch: 605, [280/560, 96%], loss is 0.932717\n",
      "Training Epoch: 605, [350/560, 91%], loss is 1.160252\n",
      "Training Epoch: 605, [420/560, 96%], loss is 1.274336\n",
      "Training Epoch: 605, [490/560, 90%], loss is 1.565069\n",
      "Training Epoch: 610, [0/560, 97%], loss is 0.065052\n",
      "Training Epoch: 610, [70/560, 97%], loss is 0.194792\n",
      "Training Epoch: 610, [140/560, 96%], loss is 0.330132\n",
      "Training Epoch: 610, [210/560, 97%], loss is 0.430926\n",
      "Training Epoch: 610, [280/560, 91%], loss is 0.650181\n",
      "Training Epoch: 610, [350/560, 86%], loss is 0.918406\n",
      "Training Epoch: 610, [420/560, 90%], loss is 1.148121\n",
      "Training Epoch: 610, [490/560, 96%], loss is 1.282378\n",
      "Training Epoch: 615, [0/560, 94%], loss is 0.149236\n",
      "Training Epoch: 615, [70/560, 94%], loss is 0.342978\n",
      "Training Epoch: 615, [140/560, 94%], loss is 0.468350\n",
      "Training Epoch: 615, [210/560, 99%], loss is 0.543789\n",
      "Training Epoch: 615, [280/560, 89%], loss is 0.857749\n",
      "Training Epoch: 615, [350/560, 94%], loss is 0.999588\n",
      "Training Epoch: 615, [420/560, 89%], loss is 1.313503\n",
      "Training Epoch: 615, [490/560, 96%], loss is 1.441222\n",
      "Training Epoch: 620, [0/560, 87%], loss is 0.364378\n",
      "Training Epoch: 620, [70/560, 90%], loss is 0.597322\n",
      "Training Epoch: 620, [140/560, 91%], loss is 0.799104\n",
      "Training Epoch: 620, [210/560, 93%], loss is 0.975293\n",
      "Training Epoch: 620, [280/560, 93%], loss is 1.143076\n",
      "Training Epoch: 620, [350/560, 96%], loss is 1.235705\n",
      "Training Epoch: 620, [420/560, 90%], loss is 1.490814\n",
      "Training Epoch: 620, [490/560, 90%], loss is 1.735964\n",
      "Training Epoch: 625, [0/560, 93%], loss is 0.144500\n",
      "Training Epoch: 625, [70/560, 93%], loss is 0.288998\n",
      "Training Epoch: 625, [140/560, 93%], loss is 0.487836\n",
      "Training Epoch: 625, [210/560, 90%], loss is 0.631679\n",
      "Training Epoch: 625, [280/560, 96%], loss is 0.743276\n",
      "Training Epoch: 625, [350/560, 99%], loss is 0.830102\n",
      "Training Epoch: 625, [420/560, 91%], loss is 0.954979\n",
      "Training Epoch: 625, [490/560, 94%], loss is 1.074994\n",
      "Training Epoch: 630, [0/560, 100%], loss is 0.055480\n",
      "Training Epoch: 630, [70/560, 91%], loss is 0.223321\n",
      "Training Epoch: 630, [140/560, 93%], loss is 0.351927\n",
      "Training Epoch: 630, [210/560, 97%], loss is 0.437815\n",
      "Training Epoch: 630, [280/560, 94%], loss is 0.631336\n",
      "Training Epoch: 630, [350/560, 87%], loss is 0.898556\n",
      "Training Epoch: 630, [420/560, 99%], loss is 0.962308\n",
      "Training Epoch: 630, [490/560, 89%], loss is 1.196397\n",
      "Training Epoch: 635, [0/560, 90%], loss is 0.171378\n",
      "Training Epoch: 635, [70/560, 100%], loss is 0.283560\n",
      "Training Epoch: 635, [140/560, 89%], loss is 0.474069\n",
      "Training Epoch: 635, [210/560, 96%], loss is 0.575616\n",
      "Training Epoch: 635, [280/560, 90%], loss is 0.814541\n",
      "Training Epoch: 635, [350/560, 94%], loss is 0.966287\n",
      "Training Epoch: 635, [420/560, 94%], loss is 1.088430\n",
      "Training Epoch: 635, [490/560, 94%], loss is 1.267300\n",
      "Training Epoch: 640, [0/560, 83%], loss is 0.327557\n",
      "Training Epoch: 640, [70/560, 86%], loss is 0.636499\n",
      "Training Epoch: 640, [140/560, 93%], loss is 0.790293\n",
      "Training Epoch: 640, [210/560, 94%], loss is 0.995019\n",
      "Training Epoch: 640, [280/560, 93%], loss is 1.201441\n",
      "Training Epoch: 640, [350/560, 94%], loss is 1.350151\n",
      "Training Epoch: 640, [420/560, 93%], loss is 1.550973\n",
      "Training Epoch: 640, [490/560, 91%], loss is 1.797453\n",
      "Training Epoch: 645, [0/560, 91%], loss is 0.178799\n",
      "Training Epoch: 645, [70/560, 94%], loss is 0.298090\n",
      "Training Epoch: 645, [140/560, 90%], loss is 0.488704\n",
      "Training Epoch: 645, [210/560, 93%], loss is 0.687533\n",
      "Training Epoch: 645, [280/560, 94%], loss is 0.887371\n",
      "Training Epoch: 645, [350/560, 96%], loss is 0.988813\n",
      "Training Epoch: 645, [420/560, 96%], loss is 1.142853\n",
      "Training Epoch: 645, [490/560, 99%], loss is 1.229171\n",
      "Training Epoch: 650, [0/560, 93%], loss is 0.163581\n",
      "Training Epoch: 650, [70/560, 86%], loss is 0.543725\n",
      "Training Epoch: 650, [140/560, 87%], loss is 0.831197\n",
      "Training Epoch: 650, [210/560, 89%], loss is 1.069153\n",
      "Training Epoch: 650, [280/560, 81%], loss is 1.466597\n",
      "Training Epoch: 650, [350/560, 93%], loss is 1.633616\n",
      "Training Epoch: 650, [420/560, 93%], loss is 1.891424\n",
      "Training Epoch: 650, [490/560, 87%], loss is 2.195617\n",
      "Training Epoch: 655, [0/560, 96%], loss is 0.230413\n",
      "Training Epoch: 655, [70/560, 89%], loss is 0.603963\n",
      "Training Epoch: 655, [140/560, 90%], loss is 0.879987\n",
      "Training Epoch: 655, [210/560, 96%], loss is 1.019885\n",
      "Training Epoch: 655, [280/560, 93%], loss is 1.151889\n",
      "Training Epoch: 655, [350/560, 94%], loss is 1.298361\n",
      "Training Epoch: 655, [420/560, 91%], loss is 1.490320\n",
      "Training Epoch: 655, [490/560, 84%], loss is 2.268767\n",
      "Training Epoch: 660, [0/560, 91%], loss is 0.182907\n",
      "Training Epoch: 660, [70/560, 97%], loss is 0.310487\n",
      "Training Epoch: 660, [140/560, 93%], loss is 0.496076\n",
      "Training Epoch: 660, [210/560, 96%], loss is 0.599124\n",
      "Training Epoch: 660, [280/560, 87%], loss is 0.849989\n",
      "Training Epoch: 660, [350/560, 91%], loss is 1.030671\n",
      "Training Epoch: 660, [420/560, 96%], loss is 1.179439\n",
      "Training Epoch: 660, [490/560, 94%], loss is 1.384345\n",
      "Training Epoch: 665, [0/560, 94%], loss is 0.183583\n",
      "Training Epoch: 665, [70/560, 96%], loss is 0.372363\n",
      "Training Epoch: 665, [140/560, 90%], loss is 0.689858\n",
      "Training Epoch: 665, [210/560, 84%], loss is 1.053518\n",
      "Training Epoch: 665, [280/560, 94%], loss is 1.225714\n",
      "Training Epoch: 665, [350/560, 94%], loss is 1.344180\n",
      "Training Epoch: 665, [420/560, 96%], loss is 1.466326\n",
      "Training Epoch: 665, [490/560, 94%], loss is 1.637257\n",
      "Training Epoch: 670, [0/560, 96%], loss is 0.111511\n",
      "Training Epoch: 670, [70/560, 93%], loss is 0.268083\n",
      "Training Epoch: 670, [140/560, 94%], loss is 0.474392\n",
      "Training Epoch: 670, [210/560, 99%], loss is 0.554659\n",
      "Training Epoch: 670, [280/560, 93%], loss is 0.720625\n",
      "Training Epoch: 670, [350/560, 90%], loss is 0.925402\n",
      "Training Epoch: 670, [420/560, 91%], loss is 1.136575\n",
      "Training Epoch: 670, [490/560, 86%], loss is 1.411747\n",
      "Training Epoch: 675, [0/560, 91%], loss is 0.225612\n",
      "Training Epoch: 675, [70/560, 90%], loss is 0.415333\n",
      "Training Epoch: 675, [140/560, 90%], loss is 0.629232\n",
      "Training Epoch: 675, [210/560, 97%], loss is 0.809950\n",
      "Training Epoch: 675, [280/560, 89%], loss is 1.173037\n",
      "Training Epoch: 675, [350/560, 93%], loss is 1.535204\n",
      "Training Epoch: 675, [420/560, 96%], loss is 1.703807\n",
      "Training Epoch: 675, [490/560, 99%], loss is 1.784582\n",
      "Training Epoch: 680, [0/560, 91%], loss is 0.173194\n",
      "Training Epoch: 680, [70/560, 99%], loss is 0.303033\n",
      "Training Epoch: 680, [140/560, 86%], loss is 0.847660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 680, [210/560, 97%], loss is 0.953287\n",
      "Training Epoch: 680, [280/560, 94%], loss is 1.113856\n",
      "Training Epoch: 680, [350/560, 91%], loss is 1.275981\n",
      "Training Epoch: 680, [420/560, 91%], loss is 1.480294\n",
      "Training Epoch: 680, [490/560, 97%], loss is 1.561155\n",
      "Training Epoch: 685, [0/560, 94%], loss is 0.123836\n",
      "Training Epoch: 685, [70/560, 96%], loss is 0.227818\n",
      "Training Epoch: 685, [140/560, 91%], loss is 0.453365\n",
      "Training Epoch: 685, [210/560, 93%], loss is 0.621996\n",
      "Training Epoch: 685, [280/560, 93%], loss is 0.789878\n",
      "Training Epoch: 685, [350/560, 90%], loss is 1.026976\n",
      "Training Epoch: 685, [420/560, 97%], loss is 1.151777\n",
      "Training Epoch: 685, [490/560, 90%], loss is 1.347301\n",
      "Training Epoch: 690, [0/560, 99%], loss is 0.045339\n",
      "Training Epoch: 690, [70/560, 97%], loss is 0.122313\n",
      "Training Epoch: 690, [140/560, 89%], loss is 0.388517\n",
      "Training Epoch: 690, [210/560, 99%], loss is 0.498273\n",
      "Training Epoch: 690, [280/560, 94%], loss is 0.682917\n",
      "Training Epoch: 690, [350/560, 97%], loss is 0.781474\n",
      "Training Epoch: 690, [420/560, 93%], loss is 1.003701\n",
      "Training Epoch: 690, [490/560, 96%], loss is 1.152233\n",
      "Training Epoch: 695, [0/560, 97%], loss is 0.093195\n",
      "Training Epoch: 695, [70/560, 96%], loss is 0.226130\n",
      "Training Epoch: 695, [140/560, 96%], loss is 0.333590\n",
      "Training Epoch: 695, [210/560, 96%], loss is 0.496932\n",
      "Training Epoch: 695, [280/560, 93%], loss is 0.692607\n",
      "Training Epoch: 695, [350/560, 91%], loss is 0.869642\n",
      "Training Epoch: 695, [420/560, 94%], loss is 1.043958\n",
      "Training Epoch: 695, [490/560, 91%], loss is 1.210105\n",
      "Training Epoch: 700, [0/560, 97%], loss is 0.083463\n",
      "Training Epoch: 700, [70/560, 94%], loss is 0.257825\n",
      "Training Epoch: 700, [140/560, 93%], loss is 0.529135\n",
      "Training Epoch: 700, [210/560, 94%], loss is 0.716266\n",
      "Training Epoch: 700, [280/560, 96%], loss is 0.836114\n",
      "Training Epoch: 700, [350/560, 96%], loss is 0.997691\n",
      "Training Epoch: 700, [420/560, 94%], loss is 1.145071\n",
      "Training Epoch: 700, [490/560, 97%], loss is 1.224954\n",
      "Training Epoch: 705, [0/560, 93%], loss is 0.173826\n",
      "Training Epoch: 705, [70/560, 99%], loss is 0.252183\n",
      "Training Epoch: 705, [140/560, 94%], loss is 0.373190\n",
      "Training Epoch: 705, [210/560, 90%], loss is 0.568755\n",
      "Training Epoch: 705, [280/560, 96%], loss is 0.680238\n",
      "Training Epoch: 705, [350/560, 87%], loss is 0.969810\n",
      "Training Epoch: 705, [420/560, 94%], loss is 1.090090\n",
      "Training Epoch: 705, [490/560, 96%], loss is 1.201295\n",
      "Training Epoch: 710, [0/560, 89%], loss is 0.346824\n",
      "Training Epoch: 710, [70/560, 94%], loss is 0.488221\n",
      "Training Epoch: 710, [140/560, 89%], loss is 0.755920\n",
      "Training Epoch: 710, [210/560, 89%], loss is 1.164111\n",
      "Training Epoch: 710, [280/560, 93%], loss is 1.321186\n",
      "Training Epoch: 710, [350/560, 89%], loss is 1.617465\n",
      "Training Epoch: 710, [420/560, 96%], loss is 1.765434\n",
      "Training Epoch: 710, [490/560, 93%], loss is 1.965681\n",
      "Training Epoch: 715, [0/560, 90%], loss is 0.258666\n",
      "Training Epoch: 715, [70/560, 93%], loss is 0.473825\n",
      "Training Epoch: 715, [140/560, 96%], loss is 0.599320\n",
      "Training Epoch: 715, [210/560, 91%], loss is 0.765954\n",
      "Training Epoch: 715, [280/560, 97%], loss is 0.882287\n",
      "Training Epoch: 715, [350/560, 93%], loss is 1.069587\n",
      "Training Epoch: 715, [420/560, 94%], loss is 1.208942\n",
      "Training Epoch: 715, [490/560, 90%], loss is 1.445830\n",
      "Training Epoch: 720, [0/560, 96%], loss is 0.105890\n",
      "Training Epoch: 720, [70/560, 96%], loss is 0.190173\n",
      "Training Epoch: 720, [140/560, 96%], loss is 0.328206\n",
      "Training Epoch: 720, [210/560, 97%], loss is 0.454178\n",
      "Training Epoch: 720, [280/560, 96%], loss is 0.573188\n",
      "Training Epoch: 720, [350/560, 96%], loss is 0.690033\n",
      "Training Epoch: 720, [420/560, 93%], loss is 0.886922\n",
      "Training Epoch: 720, [490/560, 91%], loss is 1.123565\n",
      "Training Epoch: 725, [0/560, 99%], loss is 0.069754\n",
      "Training Epoch: 725, [70/560, 99%], loss is 0.125965\n",
      "Training Epoch: 725, [140/560, 100%], loss is 0.179802\n",
      "Training Epoch: 725, [210/560, 94%], loss is 0.354090\n",
      "Training Epoch: 725, [280/560, 94%], loss is 0.568207\n",
      "Training Epoch: 725, [350/560, 94%], loss is 0.716846\n",
      "Training Epoch: 725, [420/560, 99%], loss is 0.767730\n",
      "Training Epoch: 725, [490/560, 96%], loss is 0.937779\n",
      "Training Epoch: 730, [0/560, 94%], loss is 0.163806\n",
      "Training Epoch: 730, [70/560, 96%], loss is 0.298746\n",
      "Training Epoch: 730, [140/560, 99%], loss is 0.387714\n",
      "Training Epoch: 730, [210/560, 94%], loss is 0.485123\n",
      "Training Epoch: 730, [280/560, 94%], loss is 0.621373\n",
      "Training Epoch: 730, [350/560, 94%], loss is 0.784428\n",
      "Training Epoch: 730, [420/560, 97%], loss is 0.890077\n",
      "Training Epoch: 730, [490/560, 93%], loss is 1.069657\n",
      "Training Epoch: 735, [0/560, 87%], loss is 0.354072\n",
      "Training Epoch: 735, [70/560, 91%], loss is 0.577936\n",
      "Training Epoch: 735, [140/560, 96%], loss is 0.681893\n",
      "Training Epoch: 735, [210/560, 94%], loss is 0.819841\n",
      "Training Epoch: 735, [280/560, 91%], loss is 1.042587\n",
      "Training Epoch: 735, [350/560, 93%], loss is 1.231875\n",
      "Training Epoch: 735, [420/560, 90%], loss is 1.613643\n",
      "Training Epoch: 735, [490/560, 90%], loss is 1.842343\n",
      "Training Epoch: 740, [0/560, 93%], loss is 0.204505\n",
      "Training Epoch: 740, [70/560, 96%], loss is 0.404040\n",
      "Training Epoch: 740, [140/560, 87%], loss is 0.703394\n",
      "Training Epoch: 740, [210/560, 90%], loss is 0.942059\n",
      "Training Epoch: 740, [280/560, 90%], loss is 1.125417\n",
      "Training Epoch: 740, [350/560, 93%], loss is 1.263117\n",
      "Training Epoch: 740, [420/560, 90%], loss is 1.435124\n",
      "Training Epoch: 740, [490/560, 90%], loss is 1.725876\n",
      "Training Epoch: 745, [0/560, 87%], loss is 0.462258\n",
      "Training Epoch: 745, [70/560, 81%], loss is 0.941536\n",
      "Training Epoch: 745, [140/560, 87%], loss is 1.159223\n",
      "Training Epoch: 745, [210/560, 89%], loss is 1.473431\n",
      "Training Epoch: 745, [280/560, 83%], loss is 1.855795\n",
      "Training Epoch: 745, [350/560, 89%], loss is 2.230196\n",
      "Training Epoch: 745, [420/560, 89%], loss is 2.649755\n",
      "Training Epoch: 745, [490/560, 93%], loss is 2.850915\n",
      "Training Epoch: 750, [0/560, 93%], loss is 0.259671\n",
      "Training Epoch: 750, [70/560, 96%], loss is 0.395745\n",
      "Training Epoch: 750, [140/560, 94%], loss is 0.576948\n",
      "Training Epoch: 750, [210/560, 94%], loss is 0.765358\n",
      "Training Epoch: 750, [280/560, 93%], loss is 0.937360\n",
      "Training Epoch: 750, [350/560, 93%], loss is 1.107887\n",
      "Training Epoch: 750, [420/560, 94%], loss is 1.252362\n",
      "Training Epoch: 750, [490/560, 90%], loss is 1.598895\n",
      "Training Epoch: 755, [0/560, 91%], loss is 0.241742\n",
      "Training Epoch: 755, [70/560, 96%], loss is 0.430023\n",
      "Training Epoch: 755, [140/560, 87%], loss is 0.736144\n",
      "Training Epoch: 755, [210/560, 90%], loss is 0.917936\n",
      "Training Epoch: 755, [280/560, 94%], loss is 1.105231\n",
      "Training Epoch: 755, [350/560, 96%], loss is 1.235926\n",
      "Training Epoch: 755, [420/560, 93%], loss is 1.445386\n",
      "Training Epoch: 755, [490/560, 94%], loss is 1.639266\n",
      "Training Epoch: 760, [0/560, 93%], loss is 0.237313\n",
      "Training Epoch: 760, [70/560, 96%], loss is 0.359536\n",
      "Training Epoch: 760, [140/560, 96%], loss is 0.540061\n",
      "Training Epoch: 760, [210/560, 91%], loss is 0.681985\n",
      "Training Epoch: 760, [280/560, 96%], loss is 0.822224\n",
      "Training Epoch: 760, [350/560, 89%], loss is 1.058525\n",
      "Training Epoch: 760, [420/560, 93%], loss is 1.234868\n",
      "Training Epoch: 760, [490/560, 97%], loss is 1.351697\n",
      "Training Epoch: 765, [0/560, 90%], loss is 0.267929\n",
      "Training Epoch: 765, [70/560, 96%], loss is 0.460839\n",
      "Training Epoch: 765, [140/560, 90%], loss is 0.638755\n",
      "Training Epoch: 765, [210/560, 91%], loss is 0.827814\n",
      "Training Epoch: 765, [280/560, 97%], loss is 1.019541\n",
      "Training Epoch: 765, [350/560, 91%], loss is 1.206531\n",
      "Training Epoch: 765, [420/560, 91%], loss is 1.539299\n",
      "Training Epoch: 765, [490/560, 96%], loss is 1.659301\n",
      "Training Epoch: 770, [0/560, 97%], loss is 0.089408\n",
      "Training Epoch: 770, [70/560, 96%], loss is 0.200723\n",
      "Training Epoch: 770, [140/560, 99%], loss is 0.293006\n",
      "Training Epoch: 770, [210/560, 93%], loss is 0.437593\n",
      "Training Epoch: 770, [280/560, 96%], loss is 0.532090\n",
      "Training Epoch: 770, [350/560, 97%], loss is 0.624076\n",
      "Training Epoch: 770, [420/560, 97%], loss is 0.772060\n",
      "Training Epoch: 770, [490/560, 97%], loss is 0.888880\n",
      "Training Epoch: 775, [0/560, 97%], loss is 0.100024\n",
      "Training Epoch: 775, [70/560, 94%], loss is 0.256111\n",
      "Training Epoch: 775, [140/560, 97%], loss is 0.358287\n",
      "Training Epoch: 775, [210/560, 90%], loss is 0.508148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 775, [280/560, 97%], loss is 0.568484\n",
      "Training Epoch: 775, [350/560, 94%], loss is 0.747463\n",
      "Training Epoch: 775, [420/560, 99%], loss is 0.811011\n",
      "Training Epoch: 775, [490/560, 94%], loss is 0.977799\n",
      "Training Epoch: 780, [0/560, 94%], loss is 0.112678\n",
      "Training Epoch: 780, [70/560, 99%], loss is 0.204653\n",
      "Training Epoch: 780, [140/560, 94%], loss is 0.323031\n",
      "Training Epoch: 780, [210/560, 94%], loss is 0.416788\n",
      "Training Epoch: 780, [280/560, 94%], loss is 0.549755\n",
      "Training Epoch: 780, [350/560, 94%], loss is 0.746919\n",
      "Training Epoch: 780, [420/560, 91%], loss is 0.939085\n",
      "Training Epoch: 780, [490/560, 90%], loss is 1.215989\n",
      "Training Epoch: 785, [0/560, 93%], loss is 0.178856\n",
      "Training Epoch: 785, [70/560, 94%], loss is 0.330162\n",
      "Training Epoch: 785, [140/560, 96%], loss is 0.449090\n",
      "Training Epoch: 785, [210/560, 96%], loss is 0.554580\n",
      "Training Epoch: 785, [280/560, 96%], loss is 0.655169\n",
      "Training Epoch: 785, [350/560, 93%], loss is 0.877325\n",
      "Training Epoch: 785, [420/560, 96%], loss is 0.973492\n",
      "Training Epoch: 785, [490/560, 97%], loss is 1.055410\n",
      "Training Epoch: 790, [0/560, 90%], loss is 0.235527\n",
      "Training Epoch: 790, [70/560, 91%], loss is 0.449500\n",
      "Training Epoch: 790, [140/560, 91%], loss is 0.828014\n",
      "Training Epoch: 790, [210/560, 97%], loss is 0.941756\n",
      "Training Epoch: 790, [280/560, 93%], loss is 1.150151\n",
      "Training Epoch: 790, [350/560, 84%], loss is 1.584999\n",
      "Training Epoch: 790, [420/560, 89%], loss is 1.885879\n",
      "Training Epoch: 790, [490/560, 96%], loss is 2.121151\n",
      "Training Epoch: 795, [0/560, 93%], loss is 0.267466\n",
      "Training Epoch: 795, [70/560, 93%], loss is 0.431233\n",
      "Training Epoch: 795, [140/560, 94%], loss is 0.635484\n",
      "Training Epoch: 795, [210/560, 93%], loss is 0.815843\n",
      "Training Epoch: 795, [280/560, 93%], loss is 0.984720\n",
      "Training Epoch: 795, [350/560, 97%], loss is 1.058275\n",
      "Training Epoch: 795, [420/560, 86%], loss is 1.383722\n",
      "Training Epoch: 795, [490/560, 90%], loss is 1.621625\n",
      "Test set results: loss= 0.0895 accuracy= 97.5000 1-hop accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_loss, train_best, model_test = main(seed, dim_input, dim_hidden, up_limit, down_limit, batch_size, alpha, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_sigPQ_perturb_1_percentage.mat\n",
      "Test set results: loss= 0.1126 accuracy= 96.2500 1-hop accuracy = 1.0000\n",
      "testing_sigPQ_perturb_1.5_percentage.mat\n",
      "Test set results: loss= 0.4213 accuracy= 85.0000 1-hop accuracy = 0.9643\n",
      "testing_sigPQ_perturb_2_percentage.mat\n",
      "Test set results: loss= 1.0435 accuracy= 74.4643 1-hop accuracy = 0.9089\n",
      "testing_sigPQ_perturb_3_percentage.mat\n",
      "Test set results: loss= 2.5868 accuracy= 61.9643 1-hop accuracy = 0.8161\n",
      "[[96.25 85.   74.46 61.96]]\n"
     ]
    }
   ],
   "source": [
    "model_test = Net(dim_input, dim_hidden)\n",
    "model_test.load_state_dict(torch.load( os.path.join(model_dir,   savename + '.pt')))\n",
    "   \n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "mag = [  '1','1.5','2', '3'  ] \n",
    "acc_list = np.zeros((len(mag),1))\n",
    "acc_hop_list = np.zeros(acc_list.shape)\n",
    "for i in range(len(mag)):\n",
    "    testName = 'testing_sigPQ_perturb_' + mag[i] + '_percentage.mat'# 'Test_perturb_'+ mag[i] + 'pu_type_' + str(fault_type+1) + '_impedance_' \\+ str(impe_type+1)+ '_sigNew' \n",
    "    print(testName)\n",
    "    test_x,    test_labels,test_num= load_data_VI_new(w,rootPath, testName)  \n",
    "    acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "    acc_list[ i] = float(\"{:.2f}\".format(acc)) \n",
    "    acc_hop_list[  i] = float(\"{:.2f}\".format(acc_hop))  \n",
    "print( (acc_list.T ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_SigControl_allgener_0.1_pu.mat\n",
      "Test set results: loss= 0.0972 accuracy= 97.5000 1-hop accuracy = 0.9964\n",
      "testing_SigControl_allgener_0.2_pu.mat\n",
      "Test set results: loss= 0.1580 accuracy= 95.5357 1-hop accuracy = 0.9911\n",
      "testing_SigControl_allgener_0.3_pu.mat\n",
      "Test set results: loss= 0.2568 accuracy= 92.5000 1-hop accuracy = 0.9786\n",
      "testing_SigControl_allgener_0.4_pu.mat\n",
      "Test set results: loss= 0.3735 accuracy= 88.7500 1-hop accuracy = 0.9625\n",
      "testing_SigControl_allgener_0.5_pu.mat\n",
      "Test set results: loss= 0.5471 accuracy= 85.3571 1-hop accuracy = 0.9286\n",
      "[[97.5  95.54 92.5  88.75 85.36]]\n"
     ]
    }
   ],
   "source": [
    "model_test = Net(dim_input, dim_hidden)\n",
    "model_test.load_state_dict(torch.load( os.path.join(model_dir,   savename + '.pt')))   \n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "mag_control = ['0.1','0.2','0.3','0.4' , '0.5']\n",
    "acc_list = np.zeros((len(mag_control),1))\n",
    "acc_hop_list = np.zeros(acc_list.shape)\n",
    "for i in range(len(mag_control)):\n",
    "    testName = 'testing_SigControl_allgener_' +  mag_control[i]  + '_pu.mat'\n",
    "    print(testName)\n",
    "    test_x,   test_labels,test_num= load_data_VI_new(w,rootPath, testName)  \n",
    "    acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "    acc_list[ i] = float(\"{:.2f}\".format(acc)) \n",
    "    acc_hop_list[  i] = float(\"{:.2f}\".format(acc_hop))  \n",
    "print( (acc_list.T ))    \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PGD_cur_vol_limit'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = 552//70*300\n",
    "train_loss_noregu =[4.745625972747803, 4.456696510314941, 4.357154846191406, 4.352932453155518, 4.121169090270996, 4.168258190155029, 4.0213117599487305, 4.002157211303711, 3.9735639095306396, 3.883394718170166, 3.852123498916626, 3.754178285598755, 3.6294469833374023, 3.5256497859954834, 3.5135412216186523, 3.511547088623047, 3.524271011352539, 3.4840140342712402, 3.357811689376831, 3.240104913711548, 2.951444149017334, 2.877629041671753, 3.2644052505493164, 2.9462192058563232, 2.9429829120635986, 2.8862850666046143, 3.029782295227051, 2.730166435241699, 2.74981951713562, 2.7309086322784424, 2.5644900798797607, 2.4680681228637695, 2.5348339080810547, 2.4822540283203125, 2.482224225997925, 2.3806493282318115, 2.3882579803466797, 2.3677587509155273, 2.2208054065704346, 2.169201612472534, 2.214273452758789, 2.115992307662964, 1.9817023277282715, 1.984116792678833, 1.9552161693572998, 1.9631586074829102, 1.8889025449752808, 1.881063461303711, 1.9232300519943237, 1.7737377882003784, 1.8902654647827148, 1.9031257629394531, 1.8507863283157349, 1.7808926105499268, 1.6713825464248657, 1.5913234949111938, 1.5057812929153442, 1.8063828945159912, 1.5853183269500732, 1.4316246509552002, 1.6397202014923096, 1.5285996198654175, 1.5735265016555786, 1.3973426818847656, 1.2722923755645752, 1.5791321992874146, 1.3888357877731323, 1.6936458349227905, 1.5535122156143188, 1.5106998682022095, 1.2514206171035767, 1.3893612623214722, 1.2139276266098022, 1.3898402452468872, 1.3412725925445557, 1.4109824895858765, 1.3102518320083618, 1.3015387058258057, 1.2962557077407837, 1.221261978149414, 1.176613450050354, 1.1471909284591675, 1.2550231218338013, 1.1464475393295288, 1.1591686010360718, 1.2740442752838135, 1.1237143278121948, 1.3017354011535645, 1.1372549533843994, 1.1001884937286377, 1.078153133392334, 0.9972777962684631, 0.958417534828186, 1.0767502784729004, 0.9930189847946167, 1.0285550355911255, 1.0878558158874512, 0.9038298726081848, 1.0346252918243408, 0.8888367414474487, 1.0329818725585938, 0.8083903193473816, 1.047920823097229, 0.9852449893951416, 0.9197216629981995, 0.8537733554840088, 0.8486788272857666, 0.8451921343803406, 0.7917031645774841, 0.9271080493927002, 0.8330649733543396, 0.885593593120575, 0.7543959617614746, 0.736476719379425, 0.9533486366271973, 0.8817031979560852, 0.7748497128486633, 0.8707833886146545, 0.8396646976470947, 0.9045959711074829, 0.7217587828636169, 0.7213079333305359, 0.724692702293396, 0.81397545337677, 0.8684503436088562, 0.6405384540557861, 0.7067212462425232, 0.6717466711997986, 0.6640275716781616, 0.8074582815170288, 0.6995357871055603, 0.7374575138092041, 0.7773217558860779, 0.6669643521308899, 0.5895241498947144, 0.8358684182167053, 0.9191528558731079, 0.6642494201660156, 0.730644941329956, 0.6732279658317566, 0.6265103816986084, 0.7192863821983337, 0.6721801161766052, 0.5859752297401428, 0.6770429611206055, 0.7395886778831482, 0.6318424940109253, 0.5706590414047241, 0.6096128225326538, 0.8204998970031738, 0.5903738141059875, 0.7667896747589111, 0.7873279452323914, 0.761865496635437, 0.5575388073921204, 0.730779767036438, 0.6842268109321594, 0.5121209025382996, 0.6615317463874817, 0.5769963264465332, 0.6308512091636658, 0.5564888119697571, 0.6348657608032227, 0.5830143094062805, 0.5458860993385315, 0.6514995098114014, 0.5930621027946472, 0.5502113103866577, 0.6521579027175903, 0.5795019268989563, 0.646162748336792, 0.7116018533706665, 0.5525330305099487, 0.45431020855903625, 0.5522657036781311, 0.49051761627197266, 0.4553261697292328, 0.5146001577377319, 0.5133762359619141, 0.5002856850624084, 0.5817666053771973, 0.4947103261947632, 0.49289804697036743, 0.6058309674263, 0.5334698557853699, 0.4541029632091522, 0.49439698457717896, 0.6084842085838318, 0.5001153945922852, 0.5529870986938477, 0.5984950661659241, 0.4732738733291626, 0.6126233339309692, 0.6488263607025146, 0.4971179962158203, 0.46544697880744934, 0.43052637577056885, 0.5830774307250977, 0.6463861465454102, 0.562667191028595, 0.4816012978553772, 0.654923141002655, 0.49109333753585815, 0.3924477994441986, 0.4988666772842407, 0.5139084458351135, 0.4737192988395691, 0.42545366287231445, 0.46772995591163635, 0.5563077926635742, 0.3777056634426117, 0.5255182981491089, 0.6575437784194946, 0.5354505777359009, 0.40943509340286255, 0.46262508630752563, 0.5063302516937256, 0.4791768789291382, 0.4011112451553345, 0.5132416486740112, 0.48909467458724976, 0.4356824457645416, 0.3230058550834656, 0.5330284237861633, 0.47534891963005066, 0.4317167103290558, 0.4557165801525116, 0.4152212142944336, 0.5389747619628906, 0.4991667568683624, 0.42264455556869507, 0.4940876364707947, 0.4448555111885071, 0.4495790898799896, 0.4701458215713501, 0.4374001622200012, 0.4924028217792511, 0.46709781885147095, 0.4020656943321228, 0.4582594037055969, 0.4437086880207062, 0.43995893001556396, 0.45755043625831604, 0.5516196489334106, 0.45290613174438477, 0.440842866897583, 0.32673129439353943, 0.41268572211265564, 0.4259812533855438, 0.4611772894859314, 0.3880811333656311, 0.4992108643054962, 0.40932008624076843, 0.38320696353912354, 0.41235873103141785, 0.4304876923561096, 0.44723090529441833, 0.4073272943496704, 0.3795734941959381, 0.34030380845069885, 0.4653940200805664, 0.4475490152835846, 0.4361332654953003, 0.396023690700531, 0.36964160203933716, 0.3614920377731323, 0.3898649513721466, 0.45219141244888306, 0.5246117115020752, 0.4826352298259735, 0.4933983087539673, 0.34605205059051514, 0.38348832726478577, 0.3651370704174042, 0.422693133354187, 0.45907294750213623, 0.3404679000377655, 0.3608716130256653, 0.44645553827285767, 0.36850613355636597, 0.3195309042930603, 0.39913561940193176, 0.48339366912841797, 0.37085503339767456, 0.3878006339073181, 0.44835221767425537, 0.3340212106704712, 0.3625069260597229, 0.4383232891559601, 0.31566867232322693, 0.41082146763801575, 0.3881300389766693, 0.3732621669769287, 0.484624445438385, 0.3360592722892761, 0.36697643995285034, 0.3375265896320343, 0.35180357098579407, 0.37742117047309875, 0.30015724897384644, 0.3995770812034607, 0.30456700921058655, 0.38383156061172485, 0.36081165075302124, 0.25400716066360474, 0.30583855509757996, 0.3117940127849579, 0.3692505359649658, 0.2715931534767151, 0.4409260153770447, 0.32378771901130676, 0.3633459508419037, 0.3768215775489807, 0.2993875741958618, 0.37519919872283936, 0.39551815390586853, 0.2635549008846283, 0.3930019736289978, 0.2658346891403198, 0.3083277940750122, 0.30371415615081787, 0.3384823203086853, 0.4028274416923523, 0.3147197365760803, 0.3066392242908478, 0.36061638593673706, 0.3136817514896393, 0.43201810121536255, 0.3394601047039032, 0.3391419053077698, 0.32376980781555176, 0.3554590344429016, 0.3384166657924652, 0.31978002190589905, 0.42889875173568726, 0.31470105051994324, 0.3249585032463074, 0.39953023195266724, 0.4233056902885437, 0.2968738377094269, 0.401172399520874, 0.41206446290016174, 0.3802061676979065, 0.33188462257385254, 0.45912450551986694, 0.34321361780166626, 0.3662393093109131, 0.34656408429145813, 0.3246064782142639, 0.30986911058425903, 0.37226980924606323, 0.3438881039619446, 0.4041655659675598, 0.3902307450771332, 0.30415087938308716, 0.28883394598960876, 0.3503490388393402, 0.35029372572898865, 0.28899988532066345, 0.31503188610076904, 0.3602548837661743, 0.4169408082962036, 0.3272361159324646, 0.3686062693595886, 0.3753862679004669, 0.3167569637298584, 0.26928219199180603, 0.3316333591938019, 0.33107325434684753, 0.29814910888671875, 0.32143720984458923, 0.3061191439628601, 0.27768903970718384, 0.3280285894870758, 0.2926395535469055, 0.42270249128341675, 0.31718552112579346, 0.29623571038246155, 0.3171809911727905, 0.29596102237701416, 0.2984737753868103, 0.26300331950187683, 0.2582225799560547, 0.3481685519218445, 0.28351378440856934, 0.2919612526893616, 0.33213528990745544, 0.31774014234542847, 0.33730387687683105, 0.34487664699554443, 0.2922511398792267, 0.30367332696914673, 0.2544962465763092, 0.30791258811950684, 0.3192288875579834, 0.2518429160118103, 0.23672710359096527, 0.34653857350349426, 0.27501749992370605, 0.26018327474594116, 0.3571816086769104, 0.2674747109413147, 0.2788747251033783, 0.2719871401786804, 0.28422293066978455, 0.25003328919410706, 0.23494037985801697, 0.3018840551376343, 0.33377963304519653, 0.27539196610450745, 0.25090864300727844, 0.3020976185798645, 0.2815520167350769, 0.2491336464881897, 0.31845617294311523, 0.26578080654144287, 0.28852829337120056, 0.27739763259887695, 0.3784862160682678, 0.35844355821609497, 0.3332573175430298, 0.34113702178001404, 0.2431982457637787, 0.26647505164146423, 0.25017017126083374, 0.22959107160568237, 0.21019995212554932, 0.27975165843963623, 0.2578694522380829, 0.2531067132949829, 0.31862324476242065, 0.3241739571094513, 0.260998398065567, 0.3495502769947052, 0.2971629202365875, 0.22282397747039795, 0.23032474517822266, 0.22788548469543457, 0.28991276025772095, 0.23893508315086365, 0.42226454615592957, 0.25363409519195557, 0.29915571212768555, 0.2645730674266815, 0.317699134349823, 0.27109891176223755, 0.303844690322876, 0.276266872882843, 0.23601996898651123, 0.4321078658103943, 0.30753451585769653, 0.29743170738220215, 0.3087676465511322, 0.3275778889656067, 0.3101160526275635, 0.317378431558609, 0.21532854437828064, 0.3259642422199249, 0.2356470823287964, 0.2549136281013489, 0.3123067617416382, 0.2681051790714264, 0.2790820300579071, 0.24212007224559784, 0.35355454683303833, 0.333065927028656, 0.15830601751804352, 0.26575836539268494, 0.29575401544570923, 0.22395741939544678, 0.2313576340675354, 0.32337474822998047, 0.331512451171875, 0.22971515357494354, 0.2570936679840088, 0.18634441494941711, 0.44168397784233093, 0.2918521761894226, 0.35068169236183167, 0.3681826591491699, 0.27722078561782837, 0.2523055672645569, 0.2756650447845459, 0.26706719398498535, 0.3263901174068451, 0.31282877922058105, 0.25714752078056335, 0.26831620931625366, 0.32060497999191284, 0.24742752313613892, 0.2308797836303711, 0.293154239654541, 0.29417771100997925, 0.27934303879737854, 0.2516210079193115, 0.3358502686023712, 0.2991858124732971, 0.4255516529083252, 0.2991947531700134, 0.31157973408699036, 0.33617103099823, 0.2955487370491028, 0.389223575592041, 0.40910908579826355, 0.24055835604667664, 0.24823477864265442, 0.32076895236968994, 0.3832072913646698, 0.34577643871307373, 0.21250499784946442, 0.3769359886646271, 0.33842769265174866, 0.3508821129798889, 0.4481438398361206, 0.28880399465560913, 0.39607298374176025, 0.3862956762313843, 0.39424026012420654, 0.25622475147247314, 0.2123504877090454, 0.3005650043487549, 0.38037919998168945, 0.2593722939491272, 0.3420240879058838, 0.43913596868515015, 0.24174699187278748, 0.27648186683654785, 0.40423139929771423, 0.34539395570755005, 0.29752317070961, 0.3197273910045624, 0.2837180495262146, 0.28011998534202576, 0.2602372169494629, 0.32951560616493225, 0.30365175008773804, 0.24880178272724152, 0.3114336133003235, 0.21484681963920593, 0.2890891432762146, 0.24487483501434326, 0.3043200373649597, 0.3912012577056885, 0.2516552209854126, 0.4059615731239319, 0.22850975394248962, 0.31910884380340576, 0.271999716758728, 0.3609413802623749, 0.23329730331897736, 0.24340660870075226, 0.29828208684921265, 0.2987121343612671, 0.3573665916919708, 0.30429503321647644, 0.24745482206344604, 0.2671503722667694, 0.2533862292766571, 0.29402899742126465, 0.295828640460968, 0.24387094378471375, 0.3123572766780853, 0.28945088386535645, 0.16070139408111572, 0.2927072048187256, 0.3375946283340454, 0.241378515958786, 0.25145381689071655, 0.22545978426933289, 0.23887230455875397, 0.2572730779647827, 0.28785213828086853, 0.1752760112285614, 0.2608170509338379, 0.18745838105678558, 0.22321301698684692, 0.2763218283653259, 0.2886003851890564, 0.30199918150901794, 0.22971931099891663, 0.22339507937431335, 0.21274614334106445, 0.2925374507904053, 0.19977246224880219, 0.21814437210559845, 0.24911029636859894, 0.2597535252571106, 0.2943524122238159, 0.20898157358169556, 0.22510980069637299, 0.2509816586971283, 0.2832266688346863, 0.1713750958442688, 0.20090144872665405, 0.22578376531600952, 0.19224956631660461, 0.20942974090576172, 0.23276811838150024, 0.16266217827796936, 0.2251843810081482, 0.23468366265296936, 0.2597544491291046, 0.228120818734169, 0.3208269476890564, 0.25262683629989624, 0.2599206566810608, 0.16062653064727783, 0.2834988832473755, 0.21262240409851074, 0.209095299243927, 0.21141809225082397, 0.32054126262664795, 0.23552390933036804, 0.23546189069747925, 0.20099809765815735, 0.16645373404026031, 0.19065284729003906, 0.31484559178352356, 0.24682538211345673, 0.20079554617404938, 0.21360570192337036, 0.28010669350624084, 0.24698542058467865, 0.4395103454589844, 0.2217196524143219, 0.19284123182296753, 0.27120721340179443, 0.34576883912086487, 0.25727277994155884, 0.20853765308856964, 0.19959530234336853, 0.2231387495994568, 0.30113136768341064, 0.32839274406433105, 0.2318851798772812, 0.23823337256908417, 0.23946024477481842, 0.2006710171699524, 0.1730905920267105, 0.2910539209842682, 0.41176924109458923, 0.25533032417297363, 0.22470292448997498, 0.31162580847740173, 0.29840803146362305, 0.2741273045539856, 0.26334285736083984, 0.21894489228725433, 0.2617282569408417, 0.20979687571525574, 0.1813143789768219, 0.21426090598106384, 0.287161648273468, 0.2340514063835144, 0.2530955672264099, 0.2075069546699524, 0.2383265495300293, 0.29277706146240234, 0.2585148215293884, 0.3580446243286133, 0.27174052596092224, 0.22940894961357117, 0.19378331303596497, 0.2864968776702881, 0.18565335869789124, 0.19161148369312286, 0.2633703947067261, 0.2086859494447708, 0.2831187844276428, 0.258736252784729, 0.2045799195766449, 0.2589965760707855, 0.31159543991088867, 0.328622043132782, 0.27694135904312134, 0.22387048602104187, 0.19823716580867767, 0.24756571650505066, 0.26377245783805847, 0.2811926305294037, 0.17363181710243225, 0.17754074931144714, 0.21402031183242798, 0.24919292330741882, 0.2524609863758087, 0.3183521628379822, 0.3677509129047394, 0.18496090173721313, 0.19387146830558777, 0.2557925283908844, 0.2723628282546997, 0.2510155141353607, 0.26644161343574524, 0.23031049966812134, 0.2484557330608368, 0.20407646894454956, 0.19570639729499817, 0.14988771080970764, 0.21000558137893677, 0.13384753465652466, 0.24658742547035217, 0.1577543169260025, 0.28106793761253357, 0.26294174790382385, 0.20447617769241333, 0.22044767439365387, 0.3065113425254822, 0.17722702026367188, 0.16158932447433472, 0.1988251805305481, 0.21616223454475403, 0.15259794890880585, 0.30091145634651184, 0.21451881527900696, 0.22224120795726776, 0.1304636299610138, 0.1763506531715393, 0.23748977482318878, 0.18351280689239502, 0.25340741872787476, 0.16908049583435059, 0.20898982882499695, 0.29862236976623535, 0.1961374580860138, 0.3111453056335449, 0.23276343941688538, 0.2134237289428711, 0.1854190230369568, 0.2114543616771698, 0.2041134536266327, 0.20384183526039124, 0.22141483426094055, 0.23882460594177246, 0.170393168926239, 0.20840255916118622, 0.2615855932235718, 0.18355533480644226, 0.36461710929870605, 0.1591426283121109, 0.183224618434906, 0.12571758031845093, 0.2600359320640564, 0.17848685383796692, 0.26234230399131775, 0.18465733528137207, 0.1770423948764801, 0.19686955213546753, 0.25892770290374756, 0.161482036113739, 0.21850118041038513, 0.23738138377666473, 0.174992173910141, 0.15764959156513214, 0.27437594532966614, 0.1642238199710846, 0.2761017382144928, 0.1697794795036316, 0.2021561563014984, 0.16759732365608215, 0.18341955542564392, 0.18938025832176208, 0.1937989741563797, 0.2326931357383728, 0.1545715481042862, 0.2708216607570648, 0.17507454752922058, 0.14933165907859802, 0.22634464502334595, 0.20444688200950623, 0.23302975296974182, 0.16258762776851654, 0.15670761466026306, 0.28840139508247375, 0.2251097708940506, 0.1497500091791153, 0.20370709896087646, 0.22955487668514252, 0.1902121901512146, 0.20629429817199707, 0.23194631934165955, 0.18460211157798767, 0.16500845551490784, 0.20041346549987793, 0.2076006531715393, 0.23506569862365723, 0.1875646412372589, 0.19169166684150696, 0.16778412461280823, 0.20504316687583923, 0.19247213006019592, 0.1949065625667572, 0.18366645276546478, 0.2018280327320099, 0.19960051774978638, 0.24042971432209015, 0.151809960603714, 0.16615080833435059, 0.18123556673526764, 0.1520569920539856, 0.17564618587493896, 0.1438738852739334, 0.22348996996879578, 0.19035604596138, 0.19915446639060974, 0.1906922608613968, 0.18868428468704224, 0.18173524737358093, 0.21980339288711548, 0.18883267045021057, 0.17013972997665405, 0.18683503568172455, 0.21164382994174957, 0.12078613042831421, 0.21739482879638672, 0.16084584593772888, 0.1781194806098938, 0.19029273092746735, 0.25932013988494873, 0.21462124586105347, 0.2346593290567398, 0.2065226137638092, 0.1492299884557724, 0.17848461866378784, 0.19395238161087036, 0.169629767537117, 0.19099201261997223, 0.171796977519989, 0.15380121767520905, 0.1894749402999878, 0.173786461353302, 0.1624947488307953, 0.15333986282348633, 0.25521010160446167, 0.13747453689575195, 0.17540115118026733, 0.189353808760643, 0.20800243318080902, 0.22999149560928345, 0.19249442219734192, 0.23768475651741028, 0.1874961405992508, 0.19823837280273438, 0.16613033413887024, 0.215397447347641, 0.21709616482257843, 0.18845438957214355, 0.17010857164859772, 0.20726129412651062, 0.2645883858203888, 0.2217799425125122, 0.29322707653045654, 0.18574203550815582, 0.18751651048660278, 0.34468328952789307, 0.24180147051811218, 0.20830169320106506, 0.183218315243721, 0.19228586554527283, 0.21993762254714966, 0.19646736979484558, 0.2224033921957016, 0.30738115310668945, 0.1962634027004242, 0.08564354479312897, 0.24191814661026, 0.20330557227134705, 0.2153560370206833, 0.1753169596195221, 0.24730446934700012, 0.1495896577835083, 0.171847864985466, 0.1985698640346527, 0.13431978225708008, 0.18710941076278687, 0.18627285957336426, 0.14031429588794708, 0.21536847949028015, 0.19273173809051514, 0.1386066973209381, 0.24848683178424835, 0.18923509120941162, 0.20919401943683624, 0.12741298973560333, 0.2082548588514328, 0.1905437707901001, 0.1759144365787506, 0.1666644662618637, 0.1649562418460846, 0.1710657775402069, 0.16960474848747253, 0.23377792537212372, 0.18762406706809998, 0.21789801120758057, 0.1862579882144928, 0.24396446347236633, 0.2155223786830902, 0.3321189880371094, 0.2682495713233948, 0.3102368712425232, 0.13632752001285553, 0.22970867156982422, 0.1937919408082962, 0.2510274350643158, 0.1568993628025055, 0.23169642686843872, 0.24352294206619263, 0.17771904170513153, 0.2322387397289276, 0.22148433327674866, 0.16674116253852844, 0.18522807955741882, 0.24665138125419617, 0.20626616477966309, 0.2177392989397049, 0.203688383102417, 0.1995793581008911, 0.16014635562896729, 0.17248941957950592, 0.2588191032409668, 0.13908281922340393, 0.17629894614219666, 0.19591158628463745, 0.22421756386756897, 0.16044017672538757, 0.19752418994903564, 0.18054577708244324, 0.2049325704574585, 0.20506784319877625, 0.1502131223678589, 0.21923229098320007, 0.1804034411907196, 0.1675170361995697, 0.20850259065628052, 0.2050151526927948, 0.2875722050666809, 0.18653452396392822, 0.2072376161813736, 0.1457797884941101, 0.23263749480247498, 0.20403632521629333, 0.30959394574165344, 0.2107255905866623, 0.2256385236978531, 0.15314343571662903, 0.19024395942687988, 0.20764584839344025, 0.18889129161834717, 0.23994861543178558, 0.19271253049373627, 0.19215434789657593, 0.18697631359100342, 0.18259352445602417, 0.24197927117347717, 0.17156969010829926, 0.21808183193206787, 0.23368538916110992, 0.2257446050643921, 0.2092563807964325, 0.1910201907157898, 0.18258632719516754, 0.3298221230506897, 0.19496500492095947, 0.19426700472831726, 0.23297905921936035, 0.15855756402015686, 0.1852424293756485, 0.253670871257782, 0.17798849940299988, 0.21187657117843628, 0.20562048256397247, 0.16582846641540527, 0.17836102843284607, 0.206109881401062, 0.22167053818702698, 0.2003154158592224, 0.15629853308200836, 0.1521952599287033, 0.2037852704524994, 0.16676907241344452, 0.23668795824050903, 0.24059730768203735, 0.17978882789611816, 0.2108238935470581, 0.19912230968475342, 0.2003600001335144, 0.18451453745365143, 0.13383962213993073, 0.1341174989938736, 0.18405112624168396, 0.21960797905921936, 0.14697512984275818, 0.15666931867599487, 0.16834422945976257, 0.2251463234424591, 0.1875876933336258, 0.15864931046962738, 0.22661137580871582, 0.13635395467281342, 0.21329522132873535, 0.2150726020336151, 0.15925511717796326, 0.16541177034378052, 0.1805747151374817, 0.1580614149570465, 0.162927508354187, 0.16718736290931702, 0.15126711130142212, 0.19460749626159668, 0.15701380372047424, 0.251149445772171, 0.16865947842597961, 0.15364596247673035, 0.2776832580566406, 0.1442907750606537, 0.21134531497955322, 0.16712138056755066, 0.1353810727596283, 0.14554357528686523, 0.15699437260627747, 0.17990216612815857, 0.129336416721344, 0.16893544793128967, 0.14435991644859314, 0.12819014489650726, 0.13724713027477264, 0.17356765270233154, 0.16770589351654053, 0.12420099973678589, 0.16490358114242554, 0.16849301755428314, 0.12149610370397568, 0.24041923880577087, 0.17350800335407257, 0.12667623162269592, 0.13267523050308228, 0.14363715052604675, 0.18927200138568878, 0.1658223569393158, 0.1625780612230301, 0.1536216139793396, 0.15762127935886383, 0.17180097103118896, 0.1815544068813324, 0.19388312101364136, 0.1484730839729309, 0.13301058113574982, 0.16201496124267578, 0.15602093935012817, 0.1951034963130951, 0.16516385972499847, 0.11090092360973358, 0.17468976974487305, 0.1539759337902069, 0.19022737443447113, 0.1392901986837387, 0.2042117416858673, 0.18923074007034302, 0.16394557058811188, 0.20913395285606384, 0.21391484141349792, 0.14627154171466827, 0.1743411421775818, 0.13178318738937378, 0.19335100054740906, 0.1707773208618164, 0.1623956263065338, 0.1822521984577179, 0.1486840397119522, 0.1713988482952118, 0.13988527655601501, 0.17321717739105225, 0.17134489119052887, 0.18162375688552856, 0.16676115989685059, 0.11251711845397949, 0.21780434250831604, 0.14836403727531433, 0.1307952105998993, 0.1582086682319641, 0.11148452758789062, 0.14664599299430847, 0.20723628997802734, 0.1602526158094406, 0.20246167480945587, 0.2530284523963928, 0.19180691242218018, 0.16137748956680298, 0.2376861274242401, 0.19784781336784363, 0.178923100233078, 0.17510153353214264, 0.1922704577445984, 0.16003218293190002, 0.186112642288208, 0.17473864555358887, 0.16724099218845367, 0.1659155786037445, 0.1852942705154419, 0.18702375888824463, 0.16786272823810577, 0.16152530908584595, 0.17771683633327484, 0.17322197556495667, 0.17804524302482605, 0.22406920790672302, 0.16168132424354553, 0.17659685015678406, 0.2369508445262909, 0.22854620218276978, 0.2515947222709656, 0.24379760026931763, 0.15178485214710236, 0.19735784828662872, 0.157182976603508, 0.3929890990257263, 0.19340436160564423, 0.36863380670547485, 0.22663314640522003, 0.2506982088088989, 0.20315685868263245, 0.33714357018470764, 0.2546854019165039, 0.19446486234664917, 0.22348123788833618, 0.26252302527427673, 0.34744179248809814, 0.4166359603404999, 0.27925294637680054, 0.2573070526123047, 0.3612704873085022, 0.2577466070652008, 0.3833528757095337, 0.2614169716835022, 0.22544646263122559, 0.21002796292304993, 0.307411253452301, 0.31542104482650757, 0.2878153324127197, 0.7566683292388916, 0.34620118141174316, 0.6083210706710815, 0.24647080898284912, 0.2797798812389374, 0.31446370482444763, 0.3800159990787506, 0.3007061779499054, 0.44041359424591064, 0.8384516835212708, 0.3077883720397949, 0.289261132478714, 0.48400354385375977, 0.3106881380081177, 0.35624080896377563, 0.5787644386291504, 0.5080582499504089, 0.4601324796676636, 0.40271610021591187, 0.2682324945926666, 0.4481912851333618, 0.4486027956008911, 0.4229297637939453, 0.25247666239738464, 0.4112855792045593, 0.607181191444397, 0.3603973090648651, 0.31453099846839905, 0.39696529507637024, 0.5613085031509399, 0.4423871636390686, 0.49568837881088257, 0.30609452724456787, 0.311266154050827, 0.345685750246048, 0.31831443309783936, 0.29012906551361084, 0.2753315567970276, 0.49644210934638977, 0.3219943940639496, 0.4349712133407593, 0.3009856045246124, 0.27788567543029785, 0.3963789641857147, 0.3806755840778351, 0.4364762306213379, 0.18265333771705627, 0.37431392073631287, 0.4628639221191406, 0.31615909934043884, 0.3233693838119507, 0.3003932237625122, 0.3748939335346222, 0.527681827545166, 0.28119608759880066, 0.2868261933326721, 0.5809482336044312, 0.28869691491127014, 0.23715519905090332, 0.46000105142593384, 0.2831836938858032, 0.22276991605758667, 0.28890639543533325, 0.24152183532714844, 0.2083428055047989, 0.2651672065258026, 0.21845021843910217, 0.3756036162376404, 0.22799861431121826, 0.24541231989860535, 0.2063124179840088, 0.19104886054992676, 0.20960184931755066, 0.2619343101978302, 0.22639945149421692, 0.2532832622528076, 0.17411355674266815, 0.14130952954292297, 0.223935067653656, 0.19963310658931732, 0.17659235000610352, 0.19800584018230438, 0.19578483700752258, 0.21440881490707397, 0.1981809437274933, 0.20926707983016968, 0.16903111338615417, 0.2313266545534134, 0.19066496193408966, 0.16051709651947021, 0.1777035892009735, 0.2172214835882187, 0.18638190627098083, 0.15340298414230347, 0.16350898146629333, 0.1800192892551422, 0.16222696006298065, 0.20192134380340576, 0.15609365701675415, 0.21397477388381958, 0.11421921849250793, 0.1634775996208191, 0.1390531063079834, 0.1562911570072174, 0.18398448824882507, 0.17803514003753662, 0.17176738381385803, 0.157565176486969, 0.19681710004806519, 0.1858992576599121, 0.14881004393100739, 0.1803811639547348, 0.1904779076576233, 0.20547950267791748, 0.11771216988563538, 0.15641316771507263, 0.15739023685455322, 0.18558095395565033, 0.20258809626102448, 0.16475580632686615, 0.13970080018043518, 0.20095089077949524, 0.15629304945468903, 0.19143758714199066, 0.1293259859085083, 0.14113137125968933, 0.21058189868927002, 0.18594226241111755, 0.14262880384922028, 0.16039282083511353, 0.187188982963562, 0.1463620513677597, 0.19023346900939941, 0.270359605550766, 0.21357953548431396, 0.14882920682430267, 0.13867680728435516, 0.13665184378623962, 0.18453210592269897, 0.13363677263259888, 0.17448264360427856, 0.19469088315963745, 0.1408834606409073, 0.15850970149040222, 0.174041748046875, 0.16398227214813232, 0.11259284615516663, 0.13229750096797943, 0.11725358664989471, 0.16780705749988556, 0.12369334697723389, 0.1873662918806076, 0.14359909296035767, 0.13735419511795044, 0.19735978543758392, 0.10423386842012405, 0.13756392896175385, 0.08076030015945435, 0.19629627466201782, 0.22469472885131836, 0.13534663617610931, 0.12165969610214233, 0.15263661742210388, 0.14086899161338806, 0.17678585648536682, 0.09450343251228333, 0.19985780119895935, 0.20371320843696594, 0.1481868028640747, 0.18121680617332458, 0.15543153882026672, 0.1577359437942505, 0.20825991034507751, 0.20624032616615295, 0.15747755765914917, 0.2513987421989441, 0.16448426246643066, 0.21525084972381592, 0.18664102256298065, 0.14186140894889832, 0.1800229549407959, 0.1896347999572754, 0.15934345126152039, 0.15641455352306366, 0.13256466388702393, 0.138281911611557, 0.1645497977733612, 0.12465168535709381, 0.16198468208312988, 0.14523720741271973, 0.14777329564094543, 0.1616986095905304, 0.20114178955554962, 0.16790977120399475, 0.1503039300441742, 0.12008421868085861, 0.159682959318161, 0.21557408571243286, 0.13498398661613464, 0.11709144711494446, 0.17252829670906067, 0.08983151614665985, 0.21268489956855774, 0.15417343378067017, 0.2039228081703186, 0.16666346788406372, 0.11421939730644226, 0.13836340606212616, 0.18305149674415588, 0.16836506128311157, 0.18088695406913757, 0.12671589851379395, 0.1582127809524536, 0.11080145835876465, 0.13436678051948547, 0.1388111263513565, 0.1935276985168457, 0.145514577627182, 0.1364784836769104, 0.14071205258369446, 0.20142772793769836, 0.12196749448776245, 0.16120770573616028, 0.10905802249908447, 0.14121244847774506, 0.14334885776042938, 0.15930704772472382, 0.19594323635101318, 0.14020079374313354, 0.10643289983272552, 0.12638351321220398, 0.17613451182842255, 0.1354537308216095, 0.15855181217193604, 0.1143345981836319, 0.18318957090377808, 0.22047948837280273, 0.11140228807926178, 0.15070855617523193, 0.16137900948524475, 0.16935652494430542, 0.15319512784481049, 0.09660328924655914, 0.17272432148456573, 0.17909085750579834, 0.13128423690795898, 0.16127200424671173, 0.16603565216064453, 0.17513945698738098, 0.11130498349666595, 0.13144613802433014, 0.15966002643108368, 0.13601894676685333, 0.11009141802787781, 0.15798763930797577, 0.1017855554819107, 0.1672053039073944, 0.17671936750411987, 0.13497863709926605, 0.11321140825748444, 0.14283207058906555, 0.11853741109371185, 0.1505080759525299, 0.12416225671768188, 0.16187579929828644, 0.14518681168556213, 0.17433404922485352, 0.16915734112262726, 0.14192655682563782, 0.1923040747642517, 0.16226935386657715, 0.1565965712070465, 0.1373957097530365, 0.14671656489372253, 0.141363263130188, 0.16676080226898193, 0.10840099304914474, 0.14570960402488708, 0.13471901416778564, 0.17976996302604675, 0.1423693150281906, 0.12115944921970367, 0.17081546783447266, 0.08312501013278961, 0.1654565930366516, 0.1791449785232544, 0.15593695640563965, 0.17630675435066223, 0.13336636126041412, 0.1659022867679596, 0.10015571117401123, 0.12595272064208984, 0.14575740694999695, 0.13268812000751495, 0.14322713017463684, 0.10978478193283081, 0.11457391083240509, 0.14160580933094025, 0.1578705608844757, 0.11423289775848389, 0.07914645969867706, 0.1744006723165512, 0.17760872840881348, 0.09030140936374664, 0.14029690623283386, 0.1360495686531067, 0.15226870775222778, 0.13453856110572815, 0.11640620976686478, 0.12016257643699646, 0.1154576987028122, 0.1829029619693756, 0.10382585227489471, 0.13469888269901276, 0.16098669171333313, 0.11536924540996552, 0.1628476083278656, 0.17722530663013458, 0.1744125932455063, 0.09852414578199387, 0.1050812304019928, 0.19867412745952606, 0.14416596293449402, 0.10589899867773056, 0.22682535648345947, 0.17108124494552612, 0.129414901137352, 0.24184690415859222, 0.23418690264225006, 0.13372702896595, 0.18835604190826416, 0.14207439124584198, 0.16508710384368896, 0.12767982482910156, 0.13603509962558746, 0.17804370820522308, 0.13362134993076324, 0.11413167417049408, 0.1740049421787262, 0.21279165148735046, 0.1446356475353241, 0.1267501562833786, 0.11935576051473618, 0.23370447754859924, 0.11813540756702423, 0.14950141310691833, 0.1575218290090561, 0.17865273356437683, 0.12551863491535187, 0.14340490102767944, 0.14116552472114563, 0.10052186995744705, 0.12375818192958832, 0.10308749973773956, 0.12084324657917023, 0.23246721923351288, 0.1263085901737213, 0.19605469703674316, 0.1442311406135559, 0.13995224237442017, 0.1969822496175766, 0.16659453511238098, 0.15013785660266876, 0.1157093495130539, 0.15514743328094482, 0.150576651096344, 0.15101562440395355, 0.187723308801651, 0.15451733767986298, 0.15875473618507385, 0.11901314556598663, 0.15200528502464294, 0.16381987929344177, 0.13304701447486877, 0.13514092564582825, 0.17520739138126373, 0.12854702770709991, 0.1418013572692871, 0.1493515819311142, 0.16483789682388306, 0.22386126220226288, 0.11993694305419922, 0.13904723525047302, 0.18843665719032288, 0.13225869834423065, 0.12142616510391235, 0.22478723526000977, 0.2031204253435135, 0.33662688732147217, 0.17266768217086792, 0.20471146702766418, 0.16380132734775543, 0.1952434480190277, 0.1691095232963562, 0.18748319149017334, 0.24857833981513977, 0.25133180618286133, 0.18450719118118286, 0.21155068278312683, 0.1839628368616104, 0.13677576184272766, 0.15430948138237, 0.1933986246585846, 0.2006969451904297, 0.18132995069026947, 0.11323241889476776, 0.15720072388648987, 0.15545868873596191, 0.19714787602424622, 0.1679133176803589, 0.2391255497932434, 0.1716427505016327, 0.19047227501869202, 0.1542457640171051, 0.1811978965997696, 0.23557260632514954, 0.13605037331581116, 0.14908695220947266, 0.14716458320617676, 0.14692358672618866, 0.14234226942062378, 0.15555933117866516, 0.16287511587142944, 0.1212749034166336, 0.12668989598751068, 0.09108585119247437, 0.11236988753080368, 0.15362697839736938, 0.15378670394420624, 0.1437980830669403, 0.15776371955871582, 0.14546632766723633, 0.13665840029716492, 0.12789183855056763, 0.09784609824419022, 0.14128324389457703, 0.15950727462768555, 0.19100740551948547, 0.14414727687835693, 0.14304372668266296, 0.15468132495880127, 0.16283835470676422, 0.14342042803764343, 0.14688646793365479, 0.1960778534412384, 0.1367795318365097, 0.13607612252235413, 0.17432855069637299, 0.20378151535987854, 0.10165181010961533, 0.11444161087274551, 0.1575866937637329, 0.11700663715600967, 0.1594858169555664, 0.164738267660141, 0.23548585176467896, 0.19025494158267975, 0.153657928109169, 0.13780422508716583, 0.1484115719795227, 0.1559484452009201, 0.2278972864151001, 0.24166783690452576, 0.18892398476600647, 0.13050763309001923, 0.23387332260608673, 0.15177014470100403, 0.18291312456130981, 0.11752693355083466, 0.14540211856365204, 0.32283344864845276, 0.19905677437782288, 0.13173580169677734, 0.1391284465789795, 0.14211811125278473, 0.15621864795684814, 0.12311108410358429, 0.11610221117734909, 0.11697077751159668, 0.15240825712680817, 0.1452884078025818, 0.17048917710781097, 0.09716690331697464, 0.13104036450386047, 0.2029232680797577, 0.09831713140010834, 0.11590327322483063, 0.17300210893154144, 0.17237944900989532, 0.1152908205986023, 0.14521275460720062, 0.123070627450943, 0.1931072175502777, 0.14146943390369415, 0.14398914575576782, 0.11783448606729507, 0.11667066067457199, 0.09960610419511795, 0.1610652655363083, 0.1366969645023346, 0.1849389374256134, 0.1727747619152069, 0.1395014524459839, 0.18587416410446167, 0.0891507938504219, 0.11271724104881287, 0.13486480712890625, 0.15329492092132568, 0.15608486533164978, 0.14450979232788086, 0.15745370090007782, 0.11960166692733765, 0.15466563403606415, 0.15286383032798767, 0.16179388761520386, 0.14169855415821075, 0.12803372740745544, 0.19276005029678345, 0.17099922895431519, 0.15316611528396606, 0.13518790900707245, 0.10836309939622879, 0.14210771024227142, 0.14415951073169708, 0.16508495807647705, 0.15945225954055786, 0.09278604388237, 0.171329066157341, 0.08649766445159912, 0.15076500177383423, 0.10358652472496033, 0.14480143785476685, 0.15686923265457153, 0.13324497640132904, 0.13077063858509064, 0.12464462965726852, 0.1604282557964325, 0.13928526639938354, 0.11804550141096115, 0.10789729654788971, 0.12732581794261932, 0.12393920123577118, 0.121296226978302, 0.1281069666147232, 0.14697948098182678, 0.13812661170959473, 0.125965878367424, 0.1552916169166565, 0.1346825659275055, 0.11774338036775589, 0.13910695910453796, 0.12167680263519287, 0.08852198719978333, 0.10140956938266754, 0.11588852107524872, 0.14262980222702026, 0.14332859218120575, 0.10998563468456268, 0.14899824559688568, 0.12092384696006775, 0.1331833004951477, 0.18804562091827393, 0.1395464390516281, 0.13585330545902252, 0.13563647866249084, 0.1486309915781021, 0.1524217128753662, 0.19663697481155396, 0.11157311499118805, 0.13071560859680176, 0.07853469252586365, 0.10831388831138611, 0.1496693342924118, 0.10397516191005707, 0.13321411609649658, 0.14846935868263245, 0.10960298776626587, 0.14805766940116882, 0.13501834869384766, 0.11300913989543915, 0.15338072180747986, 0.12749169766902924, 0.15684333443641663, 0.1647060066461563, 0.1442117989063263, 0.12029287219047546, 0.1592007279396057, 0.11575092375278473, 0.17779392004013062, 0.1319512575864792, 0.12287037074565887, 0.12080160528421402, 0.11652995645999908, 0.4035957455635071, 0.10748328268527985, 0.20757099986076355, 0.14218127727508545, 0.1340741068124771, 0.1800326257944107, 0.23405010998249054, 0.26015084981918335, 0.15318776667118073, 0.13321146368980408, 0.10977191478013992, 0.09733779728412628, 0.2838801443576813, 0.1363058239221573, 0.22308388352394104, 0.15823876857757568, 0.1425229012966156, 0.12650536000728607, 0.19861364364624023, 0.11084889620542526, 0.2078189253807068, 0.20771408081054688, 0.13195565342903137, 0.2349766194820404, 0.17034168541431427, 0.13119402527809143, 0.1443571150302887, 0.14855627715587616, 0.21122172474861145, 0.20140962302684784, 0.17025825381278992, 0.1397925764322281, 0.13541395962238312, 0.23801547288894653, 0.1674303412437439, 0.2636977732181549, 0.18392908573150635, 0.20377640426158905, 0.11825307458639145, 0.08267416805028915, 0.17760781943798065, 0.13856497406959534, 0.16914719343185425, 0.16717562079429626, 0.15857654809951782, 0.1557924896478653, 0.3831038475036621, 0.16119523346424103, 0.16839227080345154, 0.27491751313209534, 0.27851402759552, 0.19751229882240295, 0.17943626642227173, 0.33575817942619324, 0.40365201234817505, 0.30091097950935364, 0.26418009400367737, 0.20258980989456177, 0.2176622450351715, 0.4358598589897156, 0.5558712482452393, 0.1505809873342514, 0.3799227476119995, 0.2225181758403778, 0.40329504013061523, 0.3809118866920471, 0.3946641683578491, 0.35021278262138367, 0.2388782501220703, 0.25423872470855713, 0.26848626136779785, 0.22778292000293732, 0.2786717116832733, 0.24201560020446777, 0.29706746339797974, 0.2243155539035797, 0.28281813859939575, 0.2697423994541168, 0.12931308150291443, 0.18250004947185516, 0.1406630575656891, 0.23663625121116638, 0.1673736870288849, 0.2024572789669037, 0.2006358504295349, 0.17567062377929688, 0.18772412836551666, 0.2377953976392746, 0.20565356314182281, 0.19113804399967194, 0.14983424544334412, 0.15309956669807434, 0.3011769652366638, 0.16929177939891815, 0.13473457098007202, 0.2273971289396286, 0.2091657519340515, 0.2869586646556854, 0.1893613040447235, 0.1944712996482849, 0.36104264855384827, 0.28912100195884705, 0.23623046278953552, 0.23149065673351288, 0.4057806432247162, 0.33902543783187866, 0.27214083075523376, 0.2315470278263092, 0.14104551076889038, 0.22550374269485474, 0.3757362365722656, 0.18396660685539246, 0.32663995027542114, 0.25581130385398865, 0.47937220335006714, 0.3602263033390045, 0.24346423149108887, 0.1942199319601059, 0.16507798433303833, 0.5086200833320618, 0.30288878083229065, 0.5197147130966187, 0.32886171340942383, 0.4025726914405823, 0.4252374768257141, 0.40338778495788574, 0.13790994882583618, 0.2032012641429901, 0.28163638710975647, 0.2914988100528717, 0.2669684886932373, 0.21915385127067566, 0.3791928291320801, 0.1884390115737915, 0.27609825134277344, 0.3212822377681732, 0.22567182779312134, 0.21091139316558838, 0.3365423381328583, 0.21283774077892303, 0.27947068214416504, 0.1974060982465744, 0.23589929938316345, 0.21442854404449463, 0.2643011510372162, 0.20283932983875275, 0.2429690808057785, 0.1870305985212326, 0.16606959700584412, 0.2580127716064453, 0.13391701877117157, 0.17571958899497986, 0.2646089196205139, 0.21857517957687378, 0.17441615462303162, 0.14087462425231934, 0.2552827298641205, 0.18972592055797577, 0.1892865002155304, 0.26504915952682495, 0.21818649768829346, 0.13763712346553802, 0.18104609847068787, 0.20297493040561676, 0.21921652555465698, 0.1778460443019867, 0.1729755848646164, 0.1962289810180664, 0.17134083807468414, 0.16956111788749695, 0.15263321995735168, 0.180885910987854, 0.1462811380624771, 0.18988463282585144, 0.16863030195236206, 0.14012739062309265, 0.15684929490089417, 0.13159720599651337, 0.11522930860519409, 0.16307014226913452, 0.1479557603597641, 0.15675896406173706, 0.23054848611354828, 0.2033861130475998, 0.08750756829977036, 0.19681942462921143, 0.18653059005737305, 0.1521594524383545, 0.13570134341716766, 0.14303025603294373, 0.17398199439048767, 0.13486339151859283, 0.17528170347213745, 0.08336183428764343, 0.13464118540287018, 0.1731269210577011, 0.11467675864696503, 0.1041346937417984, 0.15404237806797028, 0.14376558363437653, 0.13206981122493744, 0.09771868586540222, 0.14950920641422272, 0.13183102011680603, 0.1371513456106186, 0.158030167222023, 0.08450094610452652, 0.13896086812019348, 0.12365131080150604, 0.14180949330329895, 0.12498864531517029, 0.11559131741523743, 0.13080468773841858, 0.12958043813705444, 0.13337019085884094, 0.10305994749069214, 0.11365294456481934, 0.12924757599830627, 0.13664385676383972, 0.11124062538146973, 0.1620575189590454, 0.15476346015930176, 0.09116530418395996, 0.12137013673782349, 0.12011240422725677, 0.14210067689418793, 0.14140941202640533, 0.09698319435119629, 0.0871666818857193, 0.12512031197547913, 0.14465098083019257, 0.12731483578681946, 0.11298315227031708, 0.12471427023410797, 0.16794736683368683, 0.09842946380376816, 0.10996322333812714, 0.10392111539840698, 0.12720105051994324, 0.09227988123893738, 0.09083245694637299, 0.1368103176355362, 0.12637095153331757, 0.10943403840065002, 0.11379273235797882, 0.09098315238952637, 0.11587516963481903, 0.1220175251364708, 0.17519786953926086, 0.17232130467891693, 0.12512587010860443, 0.12515804171562195, 0.11408568173646927, 0.12810054421424866, 0.16796749830245972, 0.15653938055038452, 0.1403542459011078, 0.17312851548194885, 0.13031885027885437, 0.09295960515737534, 0.09434163570404053, 0.09675821661949158, 0.14936639368534088, 0.10940896719694138, 0.1497168242931366, 0.10330955684185028, 0.11768653243780136, 0.11013802886009216, 0.1196136623620987, 0.12854023277759552, 0.12163880467414856, 0.10223571956157684, 0.12520718574523926, 0.20372071862220764, 0.17087338864803314, 0.15994322299957275, 0.1260925531387329, 0.10868135839700699, 0.19894877076148987, 0.13194313645362854, 0.11518733948469162, 0.20433774590492249, 0.11840826272964478, 0.14008411765098572, 0.12545426189899445, 0.3946707844734192, 0.1248781830072403, 0.10360180586576462, 0.1493780016899109, 0.16467662155628204, 0.13830220699310303, 0.17711102962493896, 0.09921148419380188, 0.1483445018529892, 0.14520063996315002, 0.11653649061918259, 0.11383974552154541, 0.12690778076648712, 0.14287163317203522, 0.16388966143131256, 0.14650431275367737, 0.0916433036327362, 0.16153264045715332, 0.10044331103563309, 0.12579277157783508, 0.13368728756904602, 0.1117749959230423, 0.1408856213092804, 0.09074655175209045, 0.1352538764476776, 0.10588930547237396, 0.18720455467700958, 0.13303469121456146, 0.1813063621520996, 0.1390794813632965, 0.1454279124736786, 0.10851775109767914, 0.11794266104698181, 0.1255425065755844, 0.18017402291297913, 0.13059477508068085, 0.11463570594787598, 0.088219553232193, 0.19003835320472717, 0.22133144736289978, 0.10578272491693497, 0.19828888773918152, 0.11053329706192017, 0.1082085594534874, 0.12983982264995575, 0.14958694577217102, 0.14494477212429047, 0.11011288315057755, 0.18102553486824036, 0.1411903202533722, 0.1737237274646759, 0.16762736439704895, 0.125789612531662, 0.09223856776952744, 0.128378763794899, 0.12866374850273132, 0.18227581679821014, 0.15280687808990479, 0.12170135974884033, 0.12852799892425537, 0.11481638252735138, 0.13149069249629974, 0.1725589632987976, 0.08923135697841644, 0.11069084703922272, 0.11151233315467834, 0.13794541358947754, 0.13967680931091309, 0.1365593522787094, 0.2117691934108734, 0.1193646565079689, 0.12932470440864563, 0.15168461203575134, 0.11387322843074799, 0.19676430523395538, 0.14325225353240967, 0.11770673096179962, 0.12277968227863312, 0.19473205506801605, 0.1367194801568985, 0.09308524429798126, 0.12916553020477295, 0.11002374440431595, 0.09262911975383759, 0.09651201218366623, 0.17354930937290192, 0.1601494550704956, 0.11075198650360107, 0.12444876879453659, 0.13071748614311218, 0.13922899961471558, 0.1323995441198349, 0.1411549299955368, 0.11079790443181992, 0.15174239873886108, 0.09710700809955597, 0.09262048453092575, 0.10412280261516571, 0.13920798897743225, 0.08470909297466278, 0.12114521116018295, 0.1595858335494995, 0.15024670958518982, 0.10812442004680634, 0.09767157584428787, 0.08231285959482193, 0.07883903384208679, 0.14163020253181458, 0.1528211385011673, 0.10428144037723541, 0.14323894679546356, 0.17981041967868805, 0.1276814341545105, 0.14692997932434082, 0.12707149982452393, 0.1495848000049591, 0.11766007542610168, 0.1361714005470276, 0.13168799877166748, 0.09056193381547928, 0.11015044152736664, 0.19493785500526428, 0.10150618851184845, 0.22956646978855133, 0.22153541445732117, 0.1390867531299591, 0.13327130675315857, 0.1504497528076172, 0.17065593600273132, 0.12653829157352448, 0.14695629477500916, 0.19510608911514282, 0.18601760268211365, 0.16261576116085052, 0.1269107162952423, 0.15924106538295746, 0.11301153898239136, 0.13711583614349365, 0.16516560316085815, 0.12642598152160645, 0.11177694797515869, 0.12140288949012756, 0.23194652795791626, 0.12860973179340363, 0.11139124631881714, 0.124666228890419, 0.10888557881116867, 0.1110503226518631, 0.1693701148033142, 0.0890393927693367, 0.14599844813346863, 0.10413989424705505, 0.14452779293060303, 0.13875353336334229, 0.12767942249774933, 0.11062313616275787, 0.06859651952981949, 0.08897475153207779, 0.09120488166809082, 0.09379133582115173, 0.11043970286846161, 0.128435879945755, 0.1107913926243782, 0.08368908613920212, 0.14342612028121948, 0.13566343486309052, 0.12283430993556976, 0.09902147948741913, 0.11814230680465698, 0.11000307649374008, 0.1332884579896927, 0.16271251440048218, 0.16052652895450592, 0.11227506399154663, 0.14497974514961243, 0.1270744353532791, 0.15390047430992126, 0.1680537760257721, 0.13361993432044983, 0.11572089791297913, 0.13033974170684814, 0.14052732288837433, 0.10808278620243073, 0.1176978051662445, 0.12554217875003815, 0.1359233856201172, 0.12931832671165466, 0.10644160211086273, 0.115762859582901, 0.11394089460372925, 0.12924329936504364, 0.1029278039932251, 0.11749455332756042, 0.1348249316215515, 0.0974406898021698, 0.12364420294761658, 0.08424529433250427, 0.12783172726631165, 0.11913114786148071, 0.09927599877119064, 0.1146506667137146, 0.1247175857424736, 0.1042441800236702, 0.12066277116537094, 0.07670394331216812, 0.16363313794136047, 0.12447894364595413, 0.08230345696210861, 0.1261730045080185, 0.15613211691379547, 0.10193832963705063, 0.09265124797821045, 0.14757898449897766, 0.10705219209194183, 0.08421384543180466, 0.10221836715936661, 0.10080163180828094, 0.10722319781780243, 0.12657926976680756, 0.12559542059898376, 0.13479475677013397, 0.15149222314357758, 0.10114992409944534, 0.13880807161331177, 0.1061328575015068, 0.10897215455770493, 0.09429770708084106, 0.13787585496902466, 0.1217455267906189, 0.08711685240268707, 0.130044624209404, 0.13394992053508759, 0.12624311447143555, 0.13899502158164978, 0.09438508749008179, 0.12765078246593475, 0.10098925977945328, 0.11989317834377289, 0.11885213106870651, 0.11482217907905579, 0.10321719199419022, 0.09919870644807816, 0.07996843010187149, 0.13272957503795624, 0.08565682917833328, 0.08784899115562439, 0.11163260787725449, 0.08486217260360718, 0.12475424259901047, 0.09321833401918411, 0.10716021060943604, 0.05431456118822098, 0.09461355954408646, 0.09012970328330994, 0.13838306069374084, 0.11984477192163467, 0.09893717616796494, 0.14146676659584045, 0.1317708045244217, 0.09545809775590897, 0.0939139649271965, 0.09492494910955429, 0.10026580095291138, 0.1118190810084343, 0.1061248853802681, 0.15867304801940918, 0.15151572227478027, 0.15357159078121185, 0.09230437129735947, 0.13917618989944458, 0.10622760653495789, 0.09241068363189697, 0.11446326971054077, 0.06830358505249023, 0.10338787734508514, 0.09136676043272018, 0.07313007116317749, 0.10785721242427826, 0.13568516075611115, 0.10251788794994354, 0.09416879713535309, 0.11708353459835052, 0.0990394577383995, 0.07386292517185211, 0.11125652492046356, 0.11752776056528091, 0.1233278214931488, 0.09441864490509033, 0.12669718265533447, 0.10458597540855408, 0.10319672524929047, 0.07835302501916885, 0.11023055016994476, 0.09804391860961914, 0.1126803457736969, 0.1296125203371048, 0.12228044867515564, 0.1281212568283081, 0.11165700107812881, 0.12463921308517456, 0.11695264279842377, 0.09004812687635422, 0.08106324821710587, 0.07158458977937698, 0.07996941357851028, 0.10611514747142792, 0.2534218430519104, 0.15817353129386902, 0.1004243865609169, 0.08459827303886414, 0.11373654007911682, 0.08350294828414917, 0.0735066756606102, 0.10365229845046997, 0.09165643155574799, 0.12009210884571075, 0.08632344007492065, 0.10570340603590012, 0.12759578227996826, 0.07305556535720825, 0.06810431182384491, 0.07137705385684967, 0.15107376873493195, 0.08074414730072021, 0.09031888842582703, 0.10421504825353622, 0.10218474268913269, 0.09393276274204254, 0.11680246144533157, 0.11786742508411407, 0.15174293518066406, 0.06944476068019867, 0.08253142982721329, 0.051907237619161606, 0.07915352284908295, 0.1079908162355423, 0.11999816447496414, 0.11140340566635132, 0.11765425652265549, 0.09368374943733215, 0.10413099080324173, 0.11019984632730484, 0.10744466632604599, 0.14342935383319855, 0.08880660682916641, 0.10397416353225708, 0.07396620512008667, 0.08760474622249603, 0.11826035380363464, 0.10025812685489655, 0.17883780598640442, 0.10229098796844482, 0.13227593898773193, 0.1745237410068512, 0.09482762217521667, 0.12100113928318024, 0.08418454229831696, 0.10611678659915924, 0.11731234937906265, 0.13756327331066132, 0.13491617143154144, 0.12256255745887756, 0.11358633637428284, 0.11178722977638245, 0.10313265025615692, 0.15731260180473328, 0.13080528378486633, 0.10055296123027802, 0.0726328045129776, 0.11233240365982056, 0.11856117844581604, 0.07086621969938278, 0.08366619050502777, 0.08693625777959824, 0.11011388897895813, 0.13023196160793304, 0.12495923787355423, 0.11370525509119034, 0.08583617955446243, 0.07548604160547256, 0.09799443185329437, 0.14010290801525116, 0.09939080476760864, 0.10807690024375916, 0.09401246905326843, 0.0802656039595604, 0.12941046059131622, 0.09797894209623337, 0.12851488590240479, 0.09414727985858917, 0.10423082858324051, 0.1345519721508026, 0.09323355555534363, 0.10550616681575775, 0.09826973080635071, 0.13055503368377686, 0.11167098581790924, 0.15494081377983093, 0.0483240932226181, 0.11926066130399704, 0.14340898394584656, 0.16397954523563385, 0.10554181039333344, 0.11621266603469849, 0.08905098587274551, 0.07689295709133148, 0.12684401869773865, 0.1846221685409546, 0.12806010246276855, 0.10383617877960205, 0.08754384517669678, 0.1456083059310913, 0.1323358714580536, 0.0955260843038559, 0.13006307184696198, 0.12501199543476105, 0.11553379893302917, 0.09769453853368759, 0.1231144368648529, 0.14204752445220947, 0.11652243137359619, 0.11400651931762695, 0.17809167504310608, 0.14942282438278198, 0.09837697446346283, 0.19837695360183716, 0.2704910933971405, 0.10277439653873444, 0.13000236451625824, 0.12420141696929932, 0.11731225252151489, 0.148268461227417, 0.11967883259057999, 0.258375346660614, 0.14417949318885803, 0.13143527507781982, 0.09697331488132477, 0.13058681786060333, 0.22475889325141907, 0.25460103154182434, 0.18630456924438477, 0.26788240671157837, 0.3778345286846161, 0.11293234676122665, 0.23039063811302185, 0.18878291547298431, 0.2717757523059845, 0.36613017320632935, 0.12957698106765747, 0.1665194034576416, 0.32166755199432373, 0.2665475010871887, 0.34603002667427063, 0.3097810447216034, 0.34752508997917175, 0.21628335118293762, 0.18167872726917267, 0.42491579055786133, 0.39867615699768066, 0.24591486155986786, 0.3269723653793335, 0.23143655061721802, 0.43993625044822693, 0.2653823494911194, 0.27572691440582275, 0.17784026265144348, 0.1772216558456421, 0.2428254783153534, 0.2272799164056778, 0.21924349665641785, 0.32566291093826294, 0.17032551765441895, 0.23454052209854126, 0.15176966786384583, 0.21996857225894928, 0.19767925143241882, 0.15771973133087158, 0.21584084630012512, 0.21671459078788757, 0.26143568754196167, 0.213797926902771, 0.25524160265922546, 0.2403220534324646, 0.14238713681697845, 0.12430894374847412, 0.18742908537387848, 0.17925068736076355, 0.14927677810192108, 0.21143725514411926, 0.21057245135307312, 0.18403010070323944, 0.18445232510566711, 0.1846827268600464, 0.1510128676891327, 0.23026379942893982, 0.19627881050109863, 0.1403256207704544, 0.18421044945716858, 0.14906233549118042, 0.15298275649547577, 0.176290363073349, 0.13583378493785858, 0.18220986425876617, 0.14980049431324005, 0.13040438294410706, 0.10536204278469086, 0.1343221366405487, 0.1609029471874237, 0.24938777089118958, 0.14896130561828613, 0.12509027123451233, 0.12732504308223724, 0.12660139799118042, 0.13729538023471832, 0.11747948825359344, 0.11441676318645477, 0.10471997410058975, 0.19351065158843994, 0.11119069159030914, 0.13597974181175232, 0.1249583512544632, 0.2167394459247589, 0.14532926678657532, 0.17395569384098053, 0.2192595899105072, 0.12664642930030823, 0.25416699051856995, 0.18201269209384918, 0.13623785972595215, 0.23702740669250488, 0.1443130224943161, 0.22102732956409454, 0.14685426652431488, 0.15011140704154968, 0.09588128328323364, 0.16247130930423737, 0.20380079746246338, 0.16855594515800476, 0.11049249768257141, 0.16064351797103882, 0.1088319718837738, 0.12123502045869827, 0.23157742619514465, 0.1384953260421753, 0.09965378046035767, 0.16390782594680786, 0.1320711225271225, 0.16456778347492218, 0.15427729487419128, 0.15165874361991882, 0.1737491935491562, 0.1455155909061432, 0.08594447374343872, 0.12895467877388, 0.12170723080635071, 0.1368599832057953, 0.10481962561607361, 0.12849374115467072, 0.1800350546836853, 0.1275826096534729, 0.1257539689540863, 0.16008833050727844, 0.17523735761642456, 0.0997544676065445, 0.14550703763961792, 0.1918538212776184, 0.13439694046974182, 0.15698567032814026, 0.13964518904685974, 0.11138944327831268, 0.14182522892951965, 0.16318152844905853, 0.16204829514026642, 0.15446794033050537, 0.09604662656784058, 0.12408675253391266, 0.1027347594499588, 0.16144460439682007, 0.08186037838459015, 0.1603994369506836, 0.10023344308137894, 0.11894680559635162, 0.11248701810836792, 0.12000928819179535, 0.10848992317914963, 0.0816100463271141, 0.13521897792816162, 0.1424093246459961, 0.1400439739227295, 0.09424208104610443, 0.12135982513427734, 0.09279078245162964, 0.10819121450185776, 0.12063510715961456, 0.08533842861652374, 0.1115371361374855, 0.10423862934112549, 0.12634176015853882, 0.07296739518642426, 0.18221428990364075, 0.1071261316537857, 0.13996614515781403, 0.11836446821689606, 0.1568181812763214, 0.1029881089925766, 0.1734207272529602, 0.11179492622613907, 0.11302441358566284, 0.10996116697788239, 0.11797881871461868, 0.12086857110261917, 0.15235267579555511, 0.13920378684997559, 0.16574950516223907, 0.06913343816995621, 0.07856009900569916, 0.12142064422369003, 0.08701920509338379, 0.08727344870567322, 0.0948130190372467, 0.1345280557870865, 0.1711997389793396, 0.06459078192710876, 0.11620137095451355, 0.12058953940868378, 0.09799760580062866, 0.09328969568014145, 0.16562595963478088, 0.08925479650497437, 0.11804170906543732, 0.1047324538230896, 0.14140447974205017, 0.12393412739038467, 0.11965727806091309, 0.08986727893352509, 0.12249365448951721, 0.0797131136059761, 0.09339249134063721, 0.14953859150409698, 0.06365542858839035, 0.12317684292793274, 0.1019030213356018, 0.07924337685108185, 0.17328590154647827, 0.160760298371315, 0.12735119462013245, 0.12117858976125717, 0.12741363048553467, 0.10649999976158142, 0.10321398079395294, 0.08751696348190308, 0.09817904978990555, 0.17579397559165955, 0.11646226048469543, 0.15082626044750214, 0.14380094408988953, 0.14594492316246033, 0.11119851469993591, 0.157783642411232, 0.20381641387939453, 0.14519058167934418, 0.12482964992523193, 0.1094907596707344, 0.15814918279647827, 0.15970765054225922, 0.16063210368156433, 0.10925597697496414, 0.14363902807235718, 0.10051142424345016, 0.13763855397701263, 0.14774833619594574, 0.1512208878993988, 0.13052256405353546, 0.16067734360694885, 0.25977402925491333, 0.17184171080589294, 0.15542954206466675, 0.16652347147464752, 0.1347169280052185, 0.10828590393066406, 0.11367735266685486, 0.12320128083229065, 0.16120898723602295, 0.10333137214183807, 0.14199703931808472, 0.145480215549469, 0.08347971737384796, 0.12517747282981873, 0.14414694905281067, 0.15834149718284607, 0.11360198259353638, 0.10206975787878036, 0.14411315321922302, 0.13869944214820862, 0.10783005505800247, 0.08886805921792984, 0.08873867243528366, 0.09432615339756012, 0.1674441695213318, 0.1588679850101471, 0.15149015188217163, 0.11261366307735443, 0.07674900442361832, 0.1465529501438141, 0.14850153028964996, 0.07841398566961288, 0.1755271553993225, 0.17605164647102356, 0.11907079815864563, 0.14278267323970795, 0.12187698483467102, 0.21549242734909058, 0.2676298916339874, 0.11701831221580505, 0.14970804750919342, 0.11062857508659363, 0.22398585081100464, 0.26403993368148804, 0.4199107885360718, 0.2429797649383545, 0.14380773901939392, 0.2627592980861664, 0.654637336730957, 0.9977423548698425, 0.23570644855499268, 0.21700423955917358, 0.14655324816703796, 0.4792235493659973, 0.5913019776344299, 0.2356828898191452, 0.2555980086326599, 0.9458914399147034, 0.2162473499774933, 0.40330928564071655, 0.39121362566947937, 0.2987157106399536, 0.17183908820152283, 0.47878584265708923, 0.3562171757221222, 0.34273838996887207, 0.32100290060043335, 0.3641004264354706, 0.23283880949020386, 0.29297253489494324, 0.3783726096153259, 0.25754284858703613, 0.16712048649787903, 0.22726598381996155, 0.5228936672210693, 0.1492280662059784, 0.2706223726272583, 0.32678112387657166, 0.2280365377664566, 0.2654401957988739, 0.16677486896514893, 0.19083251059055328, 0.29765546321868896, 0.3603075444698334, 0.24574699997901917, 0.1716616004705429, 0.1948564648628235, 0.22528384625911713, 0.22421808540821075, 0.2246987670660019, 0.23752768337726593, 0.3048223853111267, 0.20551320910453796, 0.19056743383407593, 0.15528808534145355, 0.20345383882522583, 0.2260468304157257, 0.17117714881896973, 0.1783079355955124, 0.1645059585571289, 0.11722704768180847, 0.13641603291034698, 0.14512301981449127, 0.12400364875793457, 0.1320626437664032, 0.11891166865825653, 0.1798119843006134, 0.11529752612113953, 0.13140088319778442, 0.11194276809692383, 0.12433311343193054, 0.16806074976921082, 0.12486986070871353, 0.13151025772094727, 0.12252342700958252, 0.29302310943603516, 0.13271737098693848, 0.2061789631843567, 0.18144501745700836, 0.3750002384185791, 0.13348278403282166, 0.1617628037929535, 0.120025634765625, 0.17174986004829407, 0.2465115189552307, 0.1678142547607422, 0.1595870405435562, 0.19928482174873352, 0.10953651368618011, 0.14574065804481506, 0.13789156079292297, 0.11476439237594604, 0.13175281882286072, 0.11495158821344376, 0.1558886021375656, 0.12446717917919159, 0.10788394510746002, 0.1515447199344635, 0.12473209947347641, 0.1516483575105667, 0.12491485476493835, 0.16260629892349243, 0.11504319310188293, 0.16096852719783783, 0.13135838508605957, 0.11562405526638031, 0.1243801862001419, 0.12403592467308044, 0.10864806175231934, 0.1407410353422165, 0.08614031225442886, 0.09228388965129852, 0.08568071573972702, 0.13594968616962433, 0.10333552956581116, 0.14160838723182678, 0.12812666594982147, 0.10693681985139847, 0.1592331975698471, 0.10859905183315277, 0.0994369387626648, 0.10148970782756805, 0.10410577058792114, 0.12992452085018158, 0.12897194921970367, 0.1026587188243866, 0.12309492379426956, 0.09793082624673843, 0.07520115375518799, 0.2117636501789093, 0.22393956780433655, 0.13638561964035034, 0.2419913113117218, 0.2264140248298645, 0.15925757586956024, 0.17058399319648743, 0.1654886156320572, 0.15084365010261536, 0.40306341648101807, 0.4268227219581604, 0.1468736231327057, 0.12154002487659454, 0.19274307787418365, 0.24029114842414856, 0.20570099353790283, 0.3963727653026581, 0.18577802181243896, 0.2562372386455536, 0.1890152096748352, 0.2923751771450043, 0.3887600898742676, 0.21530675888061523, 0.16536368429660797, 0.22141891717910767, 0.3612910509109497, 0.29547831416130066, 0.18770578503608704, 0.25245600938796997, 0.17861270904541016, 0.13567377626895905, 0.1529308557510376, 0.35670751333236694, 0.24255330860614777, 0.30719417333602905, 0.19770433008670807, 0.2504686713218689, 0.18165606260299683, 0.22901709377765656, 0.15470799803733826, 0.15017452836036682, 0.19251330196857452, 0.20440129935741425, 0.20663520693778992, 0.22139784693717957, 0.20435892045497894, 0.21935299038887024, 0.17662838101387024, 0.17026707530021667, 0.16295713186264038, 0.21045739948749542, 0.13617077469825745, 0.14311492443084717, 0.16807660460472107, 0.13031598925590515, 0.13627631962299347, 0.14358921349048615, 0.18457558751106262, 0.15153548121452332, 0.14956340193748474, 0.15078097581863403, 0.17691442370414734, 0.15807852149009705, 0.1381409764289856, 0.11576787382364273, 0.16407203674316406, 0.14210660755634308, 0.2460050880908966, 0.11975932121276855, 0.13038384914398193, 0.11688639223575592, 0.10130944103002548, 0.16443224251270294, 0.14105340838432312, 0.12502656877040863, 0.1524999439716339, 0.14688655734062195, 0.1578081250190735, 0.11971477419137955, 0.08360940217971802, 0.12908968329429626, 0.11927545070648193, 0.08284889161586761, 0.07897791266441345, 0.11550964415073395, 0.13876032829284668, 0.10129116475582123, 0.11500303447246552, 0.14688406884670258, 0.12035536766052246, 0.13082385063171387, 0.13641926646232605, 0.10177753865718842, 0.11289627850055695, 0.11759644746780396, 0.08856885135173798, 0.09629766643047333, 0.11001026630401611, 0.1443941295146942, 0.1422286033630371, 0.12404313683509827, 0.15800125896930695, 0.13213256001472473, 0.116029754281044, 0.10465368628501892, 0.12159399688243866, 0.11133566498756409, 0.13515996932983398, 0.14964884519577026, 0.07918307930231094, 0.09911338984966278, 0.16049432754516602, 0.10749772191047668, 0.13348372280597687, 0.0956096202135086, 0.09900536388158798, 0.1146509200334549, 0.08331439644098282, 0.09024711698293686, 0.10170867294073105, 0.10643503069877625, 0.11638079583644867, 0.14200888574123383, 0.13301719725131989, 0.116530641913414, 0.09503071755170822, 0.10933887213468552, 0.09098092466592789, 0.13085971772670746, 0.12191764265298843, 0.09937175363302231, 0.07634707540273666, 0.0972243994474411, 0.10492212325334549, 0.08506740629673004, 0.09352771937847137, 0.14359772205352783, 0.1234058290719986, 0.12820957601070404, 0.11260779201984406, 0.10244473069906235, 0.0969049483537674, 0.05735195428133011, 0.07373020052909851, 0.09528720378875732, 0.1511935591697693, 0.10315141081809998, 0.10361278057098389, 0.09874328970909119, 0.14432691037654877, 0.09481144696474075, 0.11663639545440674, 0.0860576406121254, 0.10502365231513977, 0.10932234674692154, 0.09862476587295532, 0.14697372913360596, 0.11277740448713303, 0.08813096582889557, 0.11474868655204773, 0.08626478910446167, 0.12694795429706573, 0.14525854587554932, 0.10397490859031677, 0.08374529331922531, 0.1099030077457428, 0.1132287085056305, 0.08427199721336365, 0.14503923058509827, 0.08323683589696884, 0.11210799962282181, 0.12401838600635529, 0.09336254000663757, 0.07480488717556, 0.08366668224334717, 0.08653713017702103, 0.11655613780021667, 0.12954188883304596, 0.0889340192079544, 0.10154671967029572, 0.08575938642024994, 0.10989484190940857, 0.08632002025842667, 0.14179308712482452, 0.08551855385303497, 0.07249821722507477, 0.06982126832008362, 0.08254937827587128, 0.12253181636333466, 0.10617274045944214, 0.11632287502288818, 0.07902887463569641, 0.16295135021209717, 0.11472748219966888, 0.1008136123418808, 0.09299452602863312, 0.09718713164329529, 0.09259938448667526, 0.12178286910057068, 0.10250696539878845, 0.11445990204811096, 0.10792358219623566, 0.09338986873626709, 0.08821110427379608, 0.14534716308116913, 0.12840166687965393, 0.09684748947620392, 0.10924128443002701, 0.13386081159114838, 0.12432652711868286, 0.1268644481897354, 0.10989726334810257, 0.12426993250846863, 0.14256903529167175, 0.050297483801841736, 0.12433157861232758, 0.1034885123372078, 0.08424830436706543, 0.11514651030302048, 0.11429151147603989, 0.09955935925245285, 0.1093185618519783, 0.1128835529088974, 0.11422629654407501, 0.17448556423187256, 0.08387212455272675, 0.12512385845184326, 0.10394778847694397, 0.1482352614402771, 0.15958434343338013, 0.11375938355922699, 0.13187657296657562, 0.19085679948329926, 0.12719911336898804, 0.14110146462917328, 0.08186466246843338, 0.18052127957344055, 0.14514872431755066, 0.15543359518051147, 0.08155558258295059, 0.07580272108316422, 0.12146752327680588, 0.06815478205680847, 0.09837482869625092, 0.1452617198228836, 0.12365701794624329, 0.16978806257247925, 0.14148253202438354, 0.10780784487724304, 0.08696753531694412, 0.14449985325336456, 0.07015851140022278, 0.12387847900390625, 0.11334038525819778, 0.11427786946296692, 0.12075695395469666, 0.09282593429088593, 0.11252321302890778, 0.12018769979476929, 0.08707854896783829, 0.08462411165237427, 0.12253980338573456, 0.13981439173221588, 0.11198532581329346, 0.11080959439277649, 0.06988922506570816, 0.13420584797859192, 0.19698184728622437, 0.13191455602645874, 0.12253253161907196, 0.19132333993911743, 0.17371827363967896, 0.09791739284992218, 0.13414843380451202, 0.1862516850233078, 0.14804303646087646, 0.1471431851387024, 0.08088570088148117, 0.157774418592453, 0.1254730224609375, 0.16436991095542908, 0.172728031873703, 0.1298597753047943, 0.1606420874595642, 0.09622229635715485, 0.0899832546710968, 0.09138112515211105, 0.11261391639709473, 0.10996303707361221, 0.11483614146709442, 0.14404913783073425, 0.1074102520942688, 0.12398936599493027, 0.13237430155277252, 0.20142701268196106, 0.0959799513220787, 0.1756829470396042, 0.1292681246995926, 0.10518544912338257, 0.10355620086193085, 0.17871952056884766, 0.09326324611902237, 0.16271471977233887, 0.09338729083538055, 0.1495932787656784, 0.09819348901510239, 0.13984480500221252, 0.13090649247169495, 0.13283416628837585, 0.1220582127571106, 0.1250336766242981, 0.11903905868530273, 0.12468727678060532, 0.12834858894348145, 0.07209227234125137, 0.146245077252388, 0.08455473929643631, 0.14386528730392456, 0.10742229223251343, 0.08077966421842575, 0.12333901226520538, 0.0932060033082962, 0.12410479784011841, 0.10619576275348663, 0.13224998116493225, 0.10945948958396912, 0.11102721840143204, 0.09553578495979309, 0.13113771378993988, 0.07826700806617737, 0.11795901507139206, 0.11829957365989685, 0.09065942466259003, 0.14664345979690552, 0.07936696708202362, 0.19930657744407654, 0.1134333610534668, 0.15582790970802307, 0.09253711998462677, 0.14142043888568878, 0.12380553781986237, 0.1098368763923645, 0.11705093085765839, 0.1447671800851822, 0.09724694490432739, 0.12939348816871643, 0.1319824457168579, 0.061190374195575714, 0.1443665772676468, 0.12250512838363647, 0.12134678661823273, 0.09741149097681046, 0.17388826608657837, 0.12383370101451874, 0.14071302115917206, 0.10681263357400894, 0.0983639732003212, 0.15578430891036987, 0.0976884663105011, 0.13446946442127228, 0.11776559799909592, 0.11383286863565445, 0.16804882884025574, 0.107323057949543, 0.11633150279521942, 0.14774681627750397, 0.10703961551189423, 0.11137884110212326, 0.0913608968257904, 0.09584782272577286, 0.04504565894603729, 0.09184505045413971, 0.08601205050945282, 0.10880425572395325, 0.10934115946292877, 0.07927538454532623, 0.12484002858400345, 0.06375544518232346, 0.1253771185874939, 0.09011615812778473, 0.1663360595703125, 0.0877145379781723, 0.0903405100107193, 0.10472260415554047, 0.11064714193344116, 0.10118064284324646, 0.07806552946567535, 0.08733730018138885, 0.13226056098937988, 0.07857424765825272, 0.13332802057266235, 0.06725392490625381, 0.08712217956781387, 0.09153485298156738, 0.1176857128739357, 0.11056101322174072, 0.11429978162050247, 0.0848277285695076, 0.08171725273132324, 0.08182188868522644, 0.1405406892299652, 0.07857902348041534, 0.11822099983692169, 0.09193968772888184, 0.06744226813316345, 0.08406670391559601, 0.0912405475974083, 0.14504502713680267, 0.10302245616912842, 0.09288804978132248, 0.16203993558883667, 0.09839509427547455, 0.12575595080852509, 0.14853106439113617, 0.10599358379840851, 0.07068918645381927, 0.05372047424316406, 0.10961073637008667, 0.1317947506904602, 0.08836784213781357, 0.10398083925247192, 0.09406976401805878, 0.06787024438381195, 0.09739120304584503, 0.139665886759758, 0.08995360136032104, 0.08035697042942047, 0.09329799562692642, 0.08161145448684692, 0.07225065678358078, 0.08060143142938614, 0.10422402620315552, 0.0895206481218338, 0.07600488513708115, 0.10037659108638763, 0.07618575543165207, 0.12233562022447586, 0.09721408784389496, 0.09459839761257172, 0.043384455144405365, 0.11666866391897202, 0.12780064344406128, 0.07171600311994553, 0.10631328821182251, 0.09720439463853836, 0.08601216971874237, 0.09067722409963608, 0.09019425511360168, 0.03835069388151169, 0.10926467925310135, 0.08650241792201996, 0.13313543796539307, 0.09838007390499115, 0.08219854533672333, 0.10876773297786713, 0.09885033965110779, 0.143303781747818, 0.16381727159023285, 0.09318491071462631, 0.12516151368618011, 0.11638946831226349, 0.09601911157369614, 0.19695940613746643, 0.19185175001621246, 0.11410162597894669, 0.0938071757555008, 0.128243088722229, 0.12017565965652466, 0.1142120510339737, 0.1501648724079132, 0.1064324900507927, 0.09067784994840622, 0.07711221277713776, 0.09106862545013428, 0.10050608217716217, 0.257266104221344, 0.12075930833816528, 0.09931228309869766, 0.09503642469644547, 0.14908365905284882, 0.0920061320066452, 0.13144829869270325, 0.08316095918416977, 0.09558863937854767, 0.11433334648609161, 0.10711275041103363, 0.20578446984291077, 0.12072283029556274, 0.14269791543483734, 0.10698623955249786, 0.1337171196937561, 0.16733916103839874, 0.18996167182922363, 0.17717227339744568, 0.20982539653778076, 0.2447102665901184, 0.10288392007350922, 0.173130601644516, 0.15976858139038086, 0.12407679855823517, 0.1602030247449875, 0.25468185544013977, 0.10119123756885529, 0.16230598092079163, 0.08271822333335876, 0.16714967787265778, 0.1581043303012848, 0.1272198110818863, 0.11403535306453705, 0.18603450059890747, 0.1439826637506485, 0.1450788825750351, 0.11920046806335449, 0.08296102285385132, 0.08712518215179443, 0.12967412173748016, 0.12148110568523407, 0.18148644268512726, 0.1515236794948578, 0.18569758534431458, 0.13598160445690155, 0.0954219251871109, 0.13071046769618988, 0.15538887679576874, 0.10763292759656906, 0.12758418917655945, 0.1390523612499237, 0.10582996159791946, 0.11786980926990509, 0.15284617245197296, 0.1434318721294403, 0.2327985167503357, 0.11684355139732361, 0.09563863277435303, 0.20628592371940613, 0.12426336109638214, 0.08332103490829468, 0.11191745847463608, 0.16786080598831177, 0.16070052981376648, 0.09113392978906631, 0.1291288584470749, 0.23613253235816956, 0.1674358993768692, 0.2566518187522888, 0.10224340856075287, 0.17589260637760162, 0.16672399640083313, 0.18013790249824524, 0.24194490909576416, 0.19533997774124146, 0.11335330456495285, 0.10982454568147659, 0.18505345284938812, 0.1393538862466812, 0.11851932853460312, 0.13282352685928345, 0.1382976919412613, 0.43347057700157166, 0.18359026312828064, 0.20810246467590332, 0.3697453439235687, 0.17393776774406433, 0.26843589544296265, 0.16713017225265503, 0.1858213245868683, 0.21525990962982178, 0.2797260880470276, 0.3553367555141449, 0.11442358046770096, 0.18450121581554413, 0.1927371621131897, 0.3581928610801697, 0.20152156054973602, 0.21169406175613403, 0.24684956669807434, 0.1814502477645874, 0.23515917360782623, 0.1581728756427765, 0.24935923516750336, 0.35029757022857666, 0.22484618425369263, 0.2263331413269043, 0.25723016262054443, 0.15098746120929718, 0.21272072196006775, 0.20444121956825256, 0.22003480792045593, 0.21780255436897278, 0.16186246275901794, 0.2197951376438141, 0.2018476128578186, 0.28778478503227234, 0.1452205777168274, 0.17813509702682495, 0.3804755210876465, 0.22681915760040283, 0.10274533927440643, 0.20056578516960144, 0.1953989863395691, 0.15221664309501648, 0.15984749794006348, 0.18499615788459778, 0.16356390714645386, 0.49060335755348206, 0.16321948170661926, 0.12378869205713272, 0.17402034997940063, 0.16266904771327972, 0.17282181978225708, 0.1938498318195343, 0.21057625114917755, 0.14132553339004517, 0.08964814245700836, 0.19836974143981934, 0.18417465686798096, 0.1491132378578186, 0.12692035734653473, 0.1047387570142746, 0.14349733293056488, 0.15998612344264984, 0.27341505885124207, 0.1637866199016571, 0.10055477917194366, 0.11794233322143555, 0.13968198001384735, 0.16804960370063782, 0.2495594322681427, 0.22641178965568542, 0.1305495649576187, 0.10209496319293976, 0.09759223461151123, 0.19052694737911224, 0.14223308861255646, 0.17662256956100464, 0.09084365516901016, 0.14137399196624756, 0.11992713809013367, 0.13000795245170593, 0.11375963687896729, 0.12043075263500214, 0.1243438571691513, 0.12369734048843384, 0.131602942943573, 0.13298694789409637, 0.10762029886245728, 0.08691595494747162, 0.12606345117092133, 0.10233600437641144, 0.13705690205097198, 0.11162737756967545, 0.10125678777694702, 0.09886269271373749, 0.10186189413070679, 0.0962696298956871, 0.13491129875183105, 0.08480364084243774, 0.08944015204906464, 0.12896119058132172, 0.09313545376062393, 0.17939940094947815, 0.07658198475837708, 0.074102483689785, 0.10448519885540009, 0.11607998609542847, 0.07865260541439056, 0.08616985380649567, 0.13874900341033936, 0.15758207440376282, 0.09062127768993378, 0.16334375739097595, 0.12449885904788971, 0.09137623012065887, 0.09836536645889282, 0.1330651491880417, 0.09751333296298981, 0.1136748343706131, 0.18331944942474365, 0.08464383333921432, 0.1672477424144745, 0.09563587605953217, 0.09184180200099945, 0.15587377548217773, 0.06447252631187439, 0.10714847594499588, 0.12477672845125198, 0.10176633298397064, 0.1453157216310501, 0.09183064848184586, 0.08757345378398895, 0.09759293496608734, 0.10625068843364716, 0.10112318396568298, 0.12712371349334717, 0.0798938125371933, 0.11834325641393661, 0.1742027997970581, 0.12783537805080414, 0.1454988270998001, 0.06857308000326157, 0.10010230541229248, 0.1028485894203186, 0.14210177958011627, 0.07570221275091171, 0.12403746694326401, 0.15330082178115845, 0.07968717813491821, 0.10978516936302185, 0.11596660315990448, 0.0997982919216156, 0.14116902649402618, 0.18255329132080078, 0.07640107721090317, 0.08668119460344315, 0.0891684740781784, 0.14605236053466797, 0.15463607013225555, 0.10940048098564148, 0.10333839058876038, 0.0912991389632225, 0.098067507147789, 0.13184858858585358, 0.08887089788913727, 0.12784072756767273, 0.11551046371459961, 0.2507243752479553, 0.3133300244808197, 0.15405091643333435, 0.1694442629814148, 0.20538339018821716, 0.12586669623851776, 0.13610994815826416, 0.15215826034545898, 0.15640896558761597, 0.11538594961166382, 0.14878344535827637, 0.15574508905410767, 0.2897816002368927, 0.12441373616456985, 0.12564778327941895, 0.24312026798725128, 0.1719222366809845, 0.31052088737487793, 0.15666288137435913, 0.14962469041347504, 0.18984249234199524, 0.09812993556261063, 0.12206314504146576, 0.12009882926940918, 0.27576908469200134, 0.17602191865444183, 0.1239934116601944, 0.25979530811309814, 0.129579558968544, 0.13481667637825012, 0.19053110480308533, 0.2834912836551666, 0.2558002769947052, 0.1903652548789978, 0.15620054304599762, 0.16490685939788818, 0.1566219925880432, 0.14044612646102905, 0.14971935749053955, 0.2398737370967865, 0.16062286496162415, 0.12912234663963318, 0.12580953538417816, 0.0932321771979332, 0.16462558507919312, 0.2126421481370926, 0.14174044132232666, 0.1851438283920288, 0.18297238647937775, 0.10293816030025482, 0.10051459074020386, 0.1523984968662262, 0.10557346045970917, 0.14515788853168488, 0.13622738420963287, 0.1194191575050354, 0.1216830164194107, 0.15379159152507782, 0.09778445214033127, 0.10105779767036438, 0.12853440642356873, 0.1281951367855072, 0.14667309820652008, 0.08381321281194687, 0.14228273928165436, 0.17640379071235657, 0.13576556742191315, 0.11412542313337326, 0.15653787553310394, 0.06793715804815292, 0.1238793432712555, 0.09364411234855652, 0.09919938445091248, 0.1257639229297638, 0.1490200161933899, 0.18348154425621033, 0.11055462807416916, 0.1013524979352951, 0.11078639328479767, 0.10999326407909393, 0.054944612085819244, 0.08653949201107025, 0.09049847722053528, 0.12074810266494751, 0.1295606940984726, 0.12022233754396439, 0.13768725097179413, 0.14254464209079742, 0.09340934455394745, 0.07337946444749832, 0.11533451080322266, 0.08450518548488617, 0.09073248505592346, 0.11121198534965515, 0.1341654658317566, 0.16131243109703064, 0.20117133855819702, 0.13434238731861115, 0.1304989606142044, 0.08897242695093155, 0.10466465353965759, 0.08557621389627457, 0.19509705901145935, 0.14323462545871735, 0.13032042980194092, 0.16130276024341583, 0.12534242868423462, 0.07964945584535599, 0.12840627133846283, 0.08407250791788101, 0.16883811354637146, 0.14813369512557983, 0.1101716011762619, 0.0794694647192955, 0.16588333249092102, 0.11808988451957703, 0.08109387755393982, 0.11132147163152695, 0.10804714262485504, 0.0944904014468193, 0.1248776763677597, 0.10391352325677872, 0.11260901391506195, 0.13243307173252106, 0.1601659059524536, 0.09427303075790405, 0.12081548571586609, 0.1302853524684906, 0.11423998326063156, 0.10929788649082184, 0.12993744015693665, 0.09043070673942566, 0.08438563346862793, 0.08240347355604172, 0.12340866774320602, 0.16369912028312683, 0.10335825383663177, 0.12020248919725418, 0.11982443928718567, 0.15661966800689697, 0.09593941271305084, 0.07064616680145264, 0.10486392676830292, 0.15074539184570312, 0.14262494444847107, 0.15546205639839172, 0.13059505820274353, 0.13769933581352234, 0.05884978175163269, 0.11754504591226578, 0.10059519857168198, 0.09762632846832275, 0.08387554436922073, 0.07706630229949951, 0.11172235012054443, 0.10754668712615967, 0.09288524091243744, 0.11397286504507065, 0.07130825519561768, 0.08675815165042877, 0.12087896466255188, 0.09209631383419037, 0.09110060334205627, 0.09140004217624664, 0.1369459331035614, 0.12609082460403442, 0.14623615145683289, 0.12726512551307678, 0.075836181640625, 0.08434909582138062, 0.10159610956907272, 0.09523805975914001, 0.146207794547081, 0.11797462403774261, 0.10853709280490875, 0.09790680557489395, 0.07306081801652908, 0.1165459081530571, 0.16414324939250946, 0.10973332077264786, 0.12225710600614548, 0.1317671537399292, 0.0876709520816803, 0.086038738489151, 0.10245033353567123, 0.08699073642492294, 0.09461867809295654, 0.08342090249061584, 0.07087394595146179, 0.16107657551765442, 0.1018243357539177, 0.13006684184074402, 0.12211231887340546, 0.11135876923799515, 0.14079290628433228, 0.1098710298538208, 0.12988895177841187, 0.06564559787511826, 0.10229448974132538, 0.09369634091854095, 0.11115167289972305, 0.11271826922893524, 0.10987551510334015, 0.11276198923587799, 0.1126231700181961, 0.11856880784034729, 0.0756477415561676, 0.10838502645492554, 0.1431138813495636, 0.13835960626602173, 0.0748596340417862, 0.10964524745941162, 0.17062516510486603, 0.11576537787914276, 0.09797561913728714, 0.11012755334377289, 0.0941595733165741, 0.10548629611730576, 0.18649476766586304, 0.0894574522972107, 0.08814700692892075, 0.09220311045646667, 0.09089019894599915, 0.1337466537952423, 0.10123216360807419, 0.08484542369842529, 0.1041940450668335, 0.07543431222438812, 0.0634903609752655, 0.10653482377529144, 0.11822139471769333, 0.09055209159851074, 0.06724166125059128, 0.11839713156223297, 0.08185646682977676, 0.08186078816652298, 0.12421111762523651, 0.08962417393922806, 0.09316027909517288, 0.15442636609077454, 0.10470643639564514, 0.08009854704141617, 0.11004050821065903, 0.10551635175943375, 0.0857236385345459, 0.10490011423826218, 0.071988046169281, 0.07415788620710373, 0.08262211829423904, 0.07786542177200317, 0.1028919443488121, 0.07281408458948135, 0.1015879362821579, 0.09641702473163605, 0.05182824283838272, 0.09150058031082153, 0.08955910801887512, 0.12243515998125076, 0.11437541246414185, 0.1055992990732193, 0.07816773653030396, 0.05678245425224304, 0.06810708343982697, 0.078937828540802, 0.07780525088310242, 0.09712594747543335, 0.10733821988105774, 0.09267057478427887, 0.08090472221374512, 0.09293991327285767, 0.09545426070690155, 0.08994989842176437, 0.08964170515537262, 0.10056091099977493, 0.07867331802845001, 0.09608200192451477, 0.12215893715620041, 0.10101824253797531, 0.08230708539485931, 0.07661817967891693, 0.1048232764005661, 0.08120414614677429, 0.05957971140742302, 0.09270969033241272, 0.08917073905467987, 0.07263730466365814, 0.10566579550504684, 0.11025853455066681, 0.12472033500671387, 0.0917607992887497, 0.09608891606330872, 0.12385337054729462, 0.08223867416381836, 0.1030460000038147, 0.0890941470861435, 0.11536256968975067, 0.09043936431407928, 0.1287793666124344, 0.09816411137580872, 0.07337632030248642, 0.09517475962638855, 0.07784430682659149, 0.08815914392471313, 0.09157299995422363, 0.1323089301586151, 0.06355923414230347, 0.10050898790359497, 0.08676546812057495, 0.11934305727481842, 0.11750784516334534, 0.06941413134336472, 0.0955638512969017, 0.09015066921710968, 0.07502066344022751, 0.07767735421657562, 0.0946279987692833, 0.09901785105466843, 0.07491983473300934, 0.10181461274623871, 0.1080288290977478, 0.09300516545772552, 0.062263451516628265, 0.09637697041034698, 0.0962175577878952, 0.13131429255008698, 0.07502983510494232, 0.08880484104156494, 0.09470391273498535, 0.07798977196216583, 0.12567555904388428, 0.06203513219952583, 0.0720016360282898, 0.1136966273188591, 0.0828917920589447, 0.09926337003707886, 0.10715578496456146, 0.0862775444984436, 0.15153788030147552, 0.12959595024585724, 0.08638075739145279, 0.1263016015291214, 0.07869666069746017, 0.09406678378582001, 0.09030002355575562, 0.08711981773376465, 0.07109566777944565, 0.09063442051410675, 0.06341112405061722, 0.09018993377685547, 0.14278866350650787, 0.09101284295320511, 0.1029011607170105, 0.0788436084985733, 0.0917668491601944, 0.09057383239269257, 0.11085875332355499, 0.10123845934867859, 0.0959610790014267, 0.11797232180833817, 0.08498544245958328, 0.07569663971662521, 0.10759287327528, 0.08590199053287506, 0.13988935947418213, 0.08720165491104126, 0.079999178647995, 0.10722951591014862, 0.1352258026599884, 0.12435589730739594, 0.10855220258235931, 0.15212313830852509, 0.06625217944383621, 0.07604674994945526, 0.07746943086385727, 0.10547295957803726, 0.10993922501802444, 0.0777190700173378, 0.09524893760681152, 0.08210337162017822, 0.08045586198568344, 0.08627080917358398, 0.07718844711780548, 0.152016282081604, 0.07514647394418716, 0.12149713933467865, 0.1438867300748825, 0.10645619034767151, 0.09934654086828232, 0.15924228727817535, 0.12067010998725891, 0.07395129650831223, 0.1327986717224121, 0.08007996529340744, 0.13497625291347504, 0.10359799861907959, 0.06423734128475189, 0.12372899055480957, 0.1458369642496109, 0.08216790854930878, 0.09265973418951035, 0.09497658163309097, 0.1489323079586029, 0.07076242566108704, 0.12508727610111237, 0.09304719418287277, 0.10498015582561493, 0.13636493682861328, 0.10086147487163544, 0.10826939344406128, 0.050491008907556534, 0.08327994495630264, 0.08846976608037949, 0.14698249101638794, 0.12196579575538635, 0.07693922519683838, 0.16278895735740662, 0.11471329629421234, 0.13933944702148438, 0.09702564775943756, 0.08035659790039062, 0.1368541419506073, 0.12638741731643677, 0.2412763386964798, 0.11109030991792679, 0.13750214874744415, 0.0697542205452919, 0.17598873376846313, 0.15790444612503052, 0.18130671977996826, 0.0966181829571724, 0.18428589403629303, 0.11720985174179077, 0.23082469403743744, 0.10545031726360321, 0.1338939517736435, 0.14262579381465912, 0.08419131487607956, 0.08119046688079834, 0.11272437125444412, 0.1158614307641983, 0.10139013826847076, 0.11572559177875519, 0.13356028497219086, 0.06847827136516571, 0.07820473611354828, 0.12094332277774811, 0.08651454746723175, 0.0790616050362587, 0.09266497939825058, 0.09636233747005463, 0.10208995640277863, 0.11815926432609558, 0.13451603055000305, 0.1818799525499344, 0.13945472240447998, 0.10801924020051956, 0.18971405923366547, 0.08872272819280624, 0.12944921851158142, 0.16748283803462982, 0.17371885478496552, 0.1908324956893921, 0.14227193593978882, 0.13744062185287476, 0.15289823710918427, 0.5159826874732971, 0.08612525463104248, 0.08531656116247177, 0.16586914658546448, 0.586705207824707, 0.4971444010734558, 0.19653403759002686, 0.3225109577178955, 0.25050246715545654, 0.33922910690307617, 0.3277074694633484, 0.2699025273323059, 0.7000924944877625, 0.24258540570735931, 0.3733535408973694, 0.314236581325531, 0.415981650352478, 0.9638767242431641, 0.22780413925647736, 0.17328763008117676, 0.3197651207447052, 0.40091413259506226, 0.3751522898674011, 0.7777432799339294, 0.4396820068359375, 0.31799936294555664, 0.3902515769004822, 0.32970619201660156, 0.3457079231739044, 0.2600760757923126, 0.3668280839920044, 0.2571072280406952, 0.3600514829158783, 0.2536458969116211, 0.4465838074684143, 0.6865167021751404, 0.28038665652275085, 0.34541985392570496, 0.2957778573036194, 0.6511403918266296, 0.6376298666000366, 0.2677985727787018, 0.2629832625389099, 0.3044588267803192, 0.18226340413093567, 0.30093881487846375, 0.3384479880332947, 0.40531620383262634, 0.37749767303466797, 0.3036630153656006, 0.33426451683044434, 0.2661803364753723, 0.18686547875404358, 0.18536856770515442, 0.16799180209636688, 0.22943967580795288, 0.1702997386455536, 0.14746780693531036, 0.17590740323066711, 0.3547554612159729, 0.36650076508522034, 0.2062901258468628, 0.2616370618343353, 0.19379496574401855, 0.2799126207828522, 0.43994563817977905, 0.2988983690738678, 0.21622006595134735, 0.1881333589553833, 0.10061769187450409, 0.2128836214542389, 0.19362656772136688, 0.20889060199260712, 0.30952906608581543, 0.14968280494213104, 0.2763237953186035, 0.1564546823501587, 0.1571611613035202, 0.18280121684074402, 0.29272425174713135, 0.20062477886676788, 0.17392563819885254, 0.20403356850147247, 0.24845053255558014, 0.18109843134880066, 0.17331057786941528, 0.1646127551794052, 0.1937764436006546, 0.1070362776517868, 0.14505577087402344, 0.16562548279762268, 0.12829384207725525, 0.17911174893379211, 0.1853397637605667, 0.16752979159355164, 0.14075332880020142, 0.15391278266906738, 0.150858536362648, 0.17188812792301178, 0.17568689584732056, 0.1705913543701172, 0.24454501271247864, 0.1650664061307907, 0.17668679356575012, 0.12302906811237335, 0.12518393993377686, 0.10472829639911652, 0.13670456409454346, 0.11127675324678421, 0.12936952710151672, 0.15444537997245789, 0.11752033233642578, 0.13329371809959412, 0.13210885226726532, 0.11156214773654938, 0.0920722484588623, 0.1663896143436432, 0.10540269315242767, 0.09567956626415253, 0.18286094069480896, 0.1156049519777298, 0.08903603255748749, 0.16830459237098694, 0.1410611867904663, 0.10909053683280945, 0.10718472301959991, 0.11558079719543457, 0.07964741438627243, 0.11991056799888611, 0.17163598537445068, 0.08413935452699661, 0.15461274981498718, 0.09284502267837524, 0.1258675754070282, 0.1147313266992569, 0.12106770277023315, 0.07503925263881683, 0.10992677509784698, 0.11094312369823456, 0.0992688313126564, 0.07161775976419449, 0.10850026458501816, 0.18579955399036407, 0.0641951635479927, 0.12704646587371826, 0.11231683194637299, 0.1036452054977417, 0.10096999257802963, 0.09497451782226562, 0.1269603818655014, 0.12892866134643555, 0.06350623071193695, 0.10449706017971039, 0.0711091160774231, 0.09380143135786057, 0.11707136034965515, 0.0964510589838028, 0.1528329849243164, 0.13691622018814087, 0.06480585038661957, 0.10908953100442886, 0.10412061959505081, 0.1362646222114563, 0.09997156262397766, 0.11895811557769775, 0.12540319561958313, 0.11225683242082596, 0.06896333396434784, 0.09200523793697357, 0.10731392353773117, 0.12399265170097351, 0.09232158958911896, 0.11248204112052917, 0.1118597462773323, 0.1383575201034546, 0.07449155300855637, 0.08333976566791534, 0.10577546060085297, 0.1542423665523529, 0.14532789587974548, 0.09883619099855423, 0.21598240733146667, 0.0898575410246849, 0.09992137551307678, 0.08173587918281555, 0.1234789714217186, 0.10537942498922348, 0.1341446340084076, 0.07051236927509308, 0.16513310372829437, 0.11000619828701019, 0.11611651629209518, 0.12806469202041626, 0.1213420182466507, 0.10817718505859375, 0.1678975373506546, 0.14427387714385986, 0.22126415371894836, 0.11285778880119324, 0.13420170545578003, 0.06667502224445343, 0.1111033707857132, 0.11167075484991074, 0.1450829654932022, 0.08780921995639801, 0.15045113861560822, 0.22569185495376587, 0.13029253482818604, 0.1516963690519333, 0.15407219529151917, 0.1171727180480957, 0.1540910005569458, 0.10401353240013123, 0.12637163698673248, 0.10584038496017456, 0.10939682275056839, 0.1249934732913971, 0.14891555905342102, 0.11539777368307114, 0.10722853243350983, 0.10727807879447937, 0.12287561595439911, 0.08179503679275513, 0.07497911155223846, 0.15819445252418518, 0.13404038548469543, 0.15811815857887268, 0.15937016904354095, 0.10206048935651779, 0.07934533059597015, 0.14002498984336853, 0.08080024272203445, 0.11258736252784729, 0.0985046923160553, 0.09811647981405258, 0.10831823199987411, 0.11453990638256073, 0.1318475753068924, 0.06586883962154388, 0.1042601615190506, 0.11147251725196838, 0.11330398917198181, 0.12313651293516159, 0.11914888024330139, 0.11038084328174591, 0.11190265417098999, 0.08464035391807556, 0.1425066888332367, 0.15240326523780823, 0.06781931221485138, 0.10442189872264862, 0.08981435000896454, 0.15581682324409485, 0.09346597641706467, 0.12367226928472519, 0.09670674055814743, 0.08390013128519058, 0.10581999272108078, 0.10138056427240372, 0.08576729148626328, 0.09953237324953079, 0.090762197971344, 0.09105832129716873, 0.07431361079216003, 0.07128462195396423, 0.11723321676254272, 0.06955023854970932, 0.08528867363929749, 0.13066041469573975, 0.09225621819496155, 0.11262963712215424, 0.11854629218578339, 0.12237830460071564, 0.13760165870189667, 0.09709831327199936, 0.11970950663089752, 0.11247321218252182, 0.0979732796549797, 0.09395568817853928, 0.09416381269693375, 0.12100417166948318, 0.0844188928604126, 0.06294265389442444, 0.08938109874725342, 0.11218112707138062, 0.13343828916549683, 0.10322071611881256, 0.12620288133621216, 0.1286337822675705, 0.09225970506668091, 0.10004261136054993, 0.10517358779907227, 0.08428522199392319, 0.08716963231563568, 0.09124530106782913, 0.08217674493789673, 0.10450087487697601, 0.09012198448181152, 0.14974722266197205, 0.09468421339988708, 0.10271166265010834, 0.07451416552066803, 0.09938979893922806, 0.10307581722736359, 0.1222841739654541, 0.06222544610500336, 0.1067914217710495, 0.13977356255054474, 0.1525348275899887, 0.08259657025337219, 0.09756086766719818, 0.1105199009180069, 0.13127490878105164, 0.10270511358976364, 0.0550033301115036, 0.17060044407844543, 0.2215496301651001, 0.14221173524856567, 0.09862393140792847, 0.07668596506118774, 0.11322873085737228, 0.0927218496799469, 0.11652788519859314, 0.09776186943054199, 0.09847572445869446, 0.18601784110069275, 0.0819210410118103, 0.13031554222106934, 0.1925104409456253, 0.07473774254322052, 0.0863739550113678, 0.11445742845535278, 0.08153997361660004, 0.09209070354700089, 0.17617660760879517, 0.06696845591068268, 0.08993781358003616, 0.10628213733434677, 0.11855623126029968, 0.09762629866600037, 0.0733407586812973, 0.07764691114425659, 0.0893731564283371, 0.07630271464586258, 0.09225556254386902, 0.09155210852622986, 0.0902114063501358, 0.15411494672298431, 0.07883693277835846, 0.12801037728786469, 0.10812652111053467, 0.11442115902900696, 0.11929072439670563, 0.12926311790943146, 0.09401103109121323, 0.13828502595424652, 0.11573400348424911, 0.13845831155776978, 0.06995022296905518, 0.1258598268032074, 0.10940505564212799, 0.1250954270362854, 0.11265683174133301, 0.08945773541927338, 0.11427472531795502, 0.07210925221443176, 0.12206576019525528, 0.15029555559158325, 0.09071757644414902, 0.07944919914007187, 0.06230422854423523, 0.10793258249759674, 0.08354637026786804, 0.07274957746267319, 0.0996716096997261, 0.08872927725315094, 0.10612915456295013, 0.07662192732095718, 0.06072334200143814, 0.08800522238016129, 0.08007177710533142, 0.10265108942985535, 0.13003316521644592, 0.11785202473402023, 0.0945606455206871, 0.1176726371049881, 0.08993237465620041, 0.09481512755155563, 0.09941157698631287, 0.15386804938316345, 0.08725155144929886, 0.13556008040905, 0.12132231891155243, 0.09654872119426727, 0.12244385480880737, 0.07720330357551575, 0.07949146628379822, 0.07762223482131958, 0.07360652834177017, 0.10245288908481598, 0.0892501026391983, 0.1327393352985382, 0.13976949453353882, 0.09269475191831589, 0.08516029268503189, 0.0766710713505745, 0.09927602857351303, 0.08112744987010956, 0.09875522553920746, 0.09432147443294525, 0.06962418556213379, 0.09446301311254501, 0.08249839395284653, 0.14184993505477905, 0.13770364224910736, 0.10917158424854279, 0.07263091206550598, 0.08971276134252548, 0.11015427112579346, 0.06558845937252045, 0.08848019689321518, 0.12324753403663635, 0.16619868576526642, 0.06885194778442383, 0.12285872548818588, 0.07561151683330536, 0.10442103445529938, 0.06624244153499603, 0.0834018737077713, 0.06501540541648865, 0.10831080377101898, 0.11919872462749481, 0.10768589377403259, 0.09410438686609268, 0.08575914800167084, 0.1385555863380432, 0.08332070708274841, 0.11557713150978088, 0.08340957760810852, 0.12543389201164246, 0.09345392882823944, 0.12173192203044891, 0.10198666155338287, 0.12943054735660553, 0.1638561189174652, 0.1270357072353363, 0.10117902606725693, 0.12365411967039108, 0.10747209191322327, 0.1631365865468979, 0.13893142342567444, 0.07562854886054993, 0.12957249581813812, 0.12626197934150696, 0.19181859493255615, 0.14607177674770355, 0.13215422630310059, 0.16008427739143372, 0.11020985245704651, 0.13717998564243317, 0.1486397683620453, 0.11932510137557983, 0.12284446507692337, 0.12235039472579956, 0.08200450241565704, 0.11988973617553711, 0.11571262776851654, 0.1407451033592224, 0.1326824426651001, 0.07572513818740845, 0.13685664534568787, 0.13835737109184265, 0.09091067314147949, 0.09903379529714584, 0.21664196252822876, 0.13298526406288147, 0.14486971497535706, 0.0705273300409317, 0.11959618330001831, 0.10549651086330414, 0.13278837502002716, 0.1427442729473114, 0.12165255099534988, 0.07650215923786163, 0.10002833604812622, 0.08959861099720001, 0.08473819494247437, 0.10399297624826431, 0.11592169106006622, 0.06293319165706635, 0.09376783668994904, 0.10781294852495193, 0.12101909518241882, 0.09712409228086472, 0.05854051187634468, 0.1422809362411499, 0.08160136640071869, 0.1323719620704651, 0.08174159377813339, 0.08788622915744781, 0.07651142030954361, 0.1060333177447319, 0.1006445363163948, 0.10423250496387482, 0.07058722525835037, 0.10656636953353882, 0.1476851850748062, 0.10419075936079025, 0.07775270193815231, 0.07218880951404572, 0.10546311736106873, 0.089317686855793, 0.0914527177810669, 0.15001949667930603, 0.09628711640834808, 0.08884011209011078, 0.06963594257831573, 0.07078161835670471, 0.07829931378364563, 0.09222950786352158, 0.11700413376092911, 0.15282714366912842, 0.09269838780164719, 0.07445467263460159, 0.0867139995098114, 0.08677039295434952, 0.07385928928852081, 0.06725148856639862, 0.11419814079999924, 0.10989956557750702, 0.0781393051147461, 0.09406516700983047, 0.09393885731697083, 0.10401704162359238, 0.11355344951152802, 0.10516378283500671, 0.07199488580226898, 0.08911144733428955, 0.11036942154169083, 0.08880932629108429, 0.08783179521560669, 0.10867069661617279, 0.07094943523406982, 0.07657862454652786, 0.09052509814500809, 0.11214835941791534, 0.10380128026008606, 0.13021351397037506, 0.0821080207824707, 0.08916153013706207, 0.14726462960243225, 0.11083874106407166, 0.07814921438694, 0.10206472873687744, 0.14271271228790283, 0.08656780421733856, 0.07940836250782013, 0.09271585941314697, 0.1831340789794922, 0.13181747496128082, 0.13042934238910675, 0.0836779922246933, 0.1150176078081131, 0.1127677857875824, 0.09732456505298615, 0.07994186133146286, 0.09219668060541153, 0.1122141107916832, 0.09084481000900269, 0.13268642127513885, 0.08523102849721909, 0.10368350893259048, 0.08066791296005249, 0.08366283029317856, 0.0797320008277893, 0.13761205971240997, 0.06969592720270157, 0.11072218418121338, 0.10222026705741882, 0.10705267637968063, 0.1404995620250702, 0.14808830618858337, 0.10837681591510773, 0.09197178483009338, 0.16263186931610107, 0.08211568742990494, 0.12188763916492462, 0.1467159390449524, 0.144607275724411, 0.2114793211221695, 0.07416687160730362, 0.07679786533117294, 0.11519747227430344, 0.12610304355621338, 0.14611132442951202, 0.09771180897951126, 0.1367461234331131, 0.1375962793827057, 0.07399770617485046, 0.10806054621934891, 0.11774428188800812, 0.0915418267250061, 0.09054659307003021, 0.11627340316772461, 0.09034338593482971, 0.09810775518417358, 0.08857674151659012, 0.07523006200790405, 0.07698778808116913, 0.10310779511928558, 0.07047379016876221, 0.13418695330619812, 0.1437223255634308, 0.11121068894863129, 0.07527881860733032, 0.0675036609172821, 0.08649957925081253, 0.06611854583024979, 0.08869152516126633, 0.10206729173660278, 0.09692373871803284, 0.1072569265961647, 0.17436566948890686, 0.08390606939792633, 0.09835886210203171, 0.07727545499801636, 0.07615359127521515, 0.10818924754858017, 0.06639721989631653, 0.11216835677623749, 0.08932333439588547, 0.10005076974630356, 0.07725569605827332, 0.11963041871786118, 0.09421529620885849, 0.06377270072698593, 0.07341562211513519, 0.08488550782203674, 0.07324673980474472, 0.09738584607839584, 0.09485089778900146, 0.08732151985168457, 0.10328604280948639, 0.10975226759910583, 0.08437459915876389, 0.08902342617511749, 0.08692361414432526, 0.10958311706781387, 0.11504018306732178, 0.07936453819274902, 0.09181896597146988, 0.06636890769004822, 0.10263713449239731, 0.09137550741434097, 0.08185827732086182, 0.08926822245121002, 0.10401055216789246, 0.10621831566095352, 0.10024138540029526, 0.053117785602808, 0.10023162513971329, 0.10682971775531769, 0.10265035182237625, 0.09327380359172821, 0.12305086106061935, 0.11098048835992813, 0.11085112392902374, 0.08723367750644684, 0.09448396414518356, 0.09168675541877747, 0.06683849543333054, 0.10134203732013702, 0.1034889966249466, 0.13149556517601013, 0.1619783341884613, 0.12612883746623993, 0.14220182597637177, 0.10862115025520325, 0.0749158263206482, 0.11541907489299774, 0.07634496688842773, 0.11702024191617966, 0.10342448204755783, 0.06502708792686462, 0.07055766880512238, 0.08922045677900314, 0.08928847312927246, 0.10777995735406876, 0.08206155896186829, 0.07426419854164124, 0.09150978922843933, 0.13187813758850098, 0.07545941323041916, 0.09249216318130493, 0.061771225184202194, 0.07651732861995697, 0.12056633085012436, 0.09018087387084961, 0.07396157085895538, 0.11675140261650085, 0.0922427624464035, 0.09060370922088623, 0.13361887633800507, 0.05869382619857788, 0.06291554868221283, 0.0808098092675209, 0.1130577102303505, 0.059249553829431534, 0.07763153314590454, 0.07791309058666229, 0.11799615621566772, 0.08080632984638214, 0.09483806788921356, 0.07475170493125916, 0.13284076750278473, 0.12254700809717178, 0.0900615006685257, 0.1290888786315918, 0.06200021505355835, 0.07030779868364334, 0.0977901965379715, 0.10226414352655411, 0.06870627403259277, 0.15169814229011536, 0.10982507467269897, 0.06268202513456345, 0.11619260907173157, 0.07376784086227417, 0.08306916058063507, 0.09928439557552338, 0.12602601945400238, 0.10108382999897003, 0.08900631964206696, 0.09460306167602539, 0.09792405366897583, 0.0927174836397171, 0.08350242674350739, 0.087290458381176, 0.09879621863365173, 0.11917749047279358, 0.07463306933641434, 0.1301780492067337, 0.06632582098245621, 0.08362699300050735, 0.09549214690923691, 0.14061850309371948, 0.07756328582763672, 0.12014146149158478, 0.09746421873569489, 0.07517221570014954, 0.05092901363968849, 0.07522173225879669, 0.11453813314437866, 0.07871807366609573, 0.06611640751361847, 0.09562049806118011, 0.11131133884191513, 0.08514586091041565, 0.08242057263851166, 0.09877819567918777, 0.09091579914093018, 0.07500442117452621, 0.07705438882112503, 0.08372973650693893, 0.09001724421977997, 0.08374954015016556, 0.0682486817240715, 0.05694279819726944, 0.06490599364042282, 0.06994643807411194, 0.10841146111488342, 0.14505726099014282, 0.10756567120552063, 0.06809473782777786, 0.1429513692855835, 0.09130895882844925, 0.1270006150007248, 0.1259358823299408, 0.0678328424692154, 0.0760800838470459, 0.07540207356214523, 0.09770698100328445, 0.08180081099271774, 0.13419809937477112, 0.08799678087234497, 0.072066530585289, 0.09370553493499756, 0.103976771235466, 0.07331210374832153, 0.09942269325256348, 0.09045900404453278, 0.09106771647930145, 0.08960014581680298, 0.09562928229570389, 0.13826203346252441, 0.08801591396331787, 0.1954604834318161, 0.09357817471027374, 0.09195683151483536, 0.10680995136499405, 0.08246737718582153, 0.0744798332452774, 0.08878321945667267, 0.06100361794233322, 0.09502501040697098, 0.08966890722513199, 0.11228974908590317, 0.08942365646362305, 0.0857556164264679, 0.09733027964830399, 0.13476544618606567, 0.07864613085985184, 0.12286314368247986, 0.09685582667589188, 0.06834082305431366, 0.05910256877541542, 0.12205038964748383, 0.18320433795452118, 0.11992550641298294, 0.10090555250644684, 0.10980117321014404, 0.12660740315914154, 0.1364428699016571, 0.11409307271242142, 0.19053204357624054, 0.4016553461551666, 0.10429738461971283, 0.1033082976937294, 0.13194473087787628, 0.21982043981552124, 0.1269993931055069, 0.08897164463996887, 0.15317319333553314, 0.162614643573761, 0.1329539269208908, 0.1336117386817932, 0.12028728425502777, 0.11405032873153687, 0.09274394810199738, 0.10244753956794739, 0.1827383041381836, 0.13323161005973816, 0.11991751194000244, 0.16858328878879547, 0.13548441231250763, 0.12894600629806519, 0.1184522733092308, 0.09376412630081177, 0.10692229866981506, 0.332062691450119, 0.1384366750717163]\n",
    "plt.plot(train_loss_noregu[0:l :1] )\n",
    "plt.plot(train_loss[0:l :1] , '.-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend(('Traditional Aversarial Training', 'Proposed'))\n",
    "plt.savefig('/Users/wenting/Documents/04_research/08_Explainable_DNN/04_report/Figures/loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_perturb_0.5pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.6804 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.7018 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.6471 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.8573 accuracy= 89.8551 1-hop accuracy = 1.0000\n",
      "Test_perturb_0.5pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6707 accuracy= 95.6522 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6853 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6521 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6638 accuracy= 91.3043 1-hop accuracy = 1.0000\n",
      "Test_perturb_0.5pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.3957 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.4413 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.5211 accuracy= 91.3043 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.9274 accuracy= 85.5072 1-hop accuracy = 0.9710\n",
      "Test_perturb_0.5pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6504 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6255 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6483 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6229 accuracy= 91.3043 1-hop accuracy = 1.0000\n",
      "Test_perturb_0.5pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 0.5694 accuracy= 82.6087 1-hop accuracy = 0.9710\n",
      "Test_perturb_1pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 0.7779 accuracy= 76.8116 1-hop accuracy = 0.9565\n",
      "Test_perturb_2pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 1.7659 accuracy= 50.7246 1-hop accuracy = 0.7971\n",
      "Test_perturb_3pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 4.1735 accuracy= 28.9855 1-hop accuracy = 0.5942\n",
      "Test_perturb_0.5pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 0.4060 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 0.4743 accuracy= 92.7536 1-hop accuracy = 0.9855\n",
      "Test_perturb_2pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 0.5712 accuracy= 91.3043 1-hop accuracy = 0.9710\n",
      "Test_perturb_3pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 1.0131 accuracy= 82.6087 1-hop accuracy = 0.9130\n",
      "Test_perturb_0.5pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 0.4183 accuracy= 88.4058 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 0.5469 accuracy= 88.4058 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 1.4025 accuracy= 62.3188 1-hop accuracy = 0.8696\n",
      "Test_perturb_3pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 3.3117 accuracy= 40.5797 1-hop accuracy = 0.6812\n",
      "Test_perturb_0.5pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6336 accuracy= 95.6522 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6715 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6416 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6314 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "[[[94.2  94.2  92.75 89.86]\n",
      "  [95.65 94.2  92.75 91.3 ]]\n",
      "\n",
      " [[94.2  94.2  91.3  85.51]\n",
      "  [94.2  94.2  92.75 91.3 ]]\n",
      "\n",
      " [[82.61 76.81 50.72 28.99]\n",
      "  [94.2  92.75 91.3  82.61]]\n",
      "\n",
      " [[88.41 88.41 62.32 40.58]\n",
      "  [95.65 94.2  94.2  92.75]]]\n"
     ]
    }
   ],
   "source": [
    "model_test = Net(dim_input, dim_hidden)\n",
    "#optimizer_test = SGD(model.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)   \n",
    "model_test.load_state_dict(torch.load( os.path.join(model_dir, 'model-' + savename + '.pt')))\n",
    "#torch.load(  os.path.join(model_dir, 'opt-fgsm-checkpoint_epoch{}.tar'.format(996)))\n",
    "   \n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "mag = [ '0.5','1','2', '3'] \n",
    "acc_list = np.zeros((4,2,4))\n",
    "acc_hop_list = np.zeros(acc_list.shape)\n",
    "for fault_type in range(4):\n",
    "    for impe_type in range(2): \n",
    "        #X1 = np.zeros((69 * len(mag),1, 272,1))\n",
    "        for i in range(len(mag)):\n",
    "            testName =  'Test_perturb_'+ mag[i] + 'pu_type_' + str(fault_type+1) + '_impedance_' \\\n",
    "            + str(impe_type+1)+ '_sigNew' \n",
    "            print(testName)\n",
    "            test_x, test_i,  test_labels,test_num= load_data_VI(w,rootPath, testName)  \n",
    "            acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "            acc_list[fault_type, impe_type, i] = float(\"{:.2f}\".format(acc)) \n",
    "            acc_hop_list[fault_type, impe_type, i] = float(\"{:.2f}\".format(acc_hop))  \n",
    "print( (acc_list ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94.925 94.2   92.75  90.58 ]\n",
      " [94.2   94.2   92.025 88.405]\n",
      " [88.405 84.78  71.01  55.8  ]\n",
      " [92.03  91.305 78.26  66.665]]\n",
      "[92.39    91.12125 83.51125 75.3625 ]\n"
     ]
    }
   ],
   "source": [
    "print( np.mean(acc_list[:,:,:] , axis = 1))\n",
    "a = np.mean(acc_list[:,:,:] , axis = 1)\n",
    "print(np.mean(a, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.    100.     97.825  97.825]\n",
      " [100.     99.275  96.375  92.755]\n",
      " [ 98.55   94.205  72.46   57.25 ]\n",
      " [ 99.275  97.1    81.16   69.565]]\n",
      "[99.45625 97.645   86.955   79.34875]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97.1   94.925 92.75  93.475]\n",
      " [99.275 98.55  95.65  91.305]\n",
      " [97.825 94.205 70.29  55.8  ]\n",
      " [97.825 96.375 80.435 68.115]]\n",
      "[98.00625 96.01375 84.78125 77.17375]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89.86  89.86  84.06  84.785]\n",
      " [88.405 86.235 84.785 82.61 ]\n",
      " [53.625 51.45  46.375 36.96 ]\n",
      " [57.975 56.525 51.45  50.   ]]\n",
      "[72.46625 71.0175  66.6675  63.58875]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.    98.55  98.55  98.55]\n",
      " [100.    98.55  95.65  89.86]\n",
      " [ 98.55  85.51  46.38  27.54]\n",
      " [ 97.1   92.75  52.17  31.88]]\n"
     ]
    }
   ],
   "source": [
    "print(  (acc_list[:,0,:]  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.    98.55  98.55  98.55]\n",
      " [100.   100.    98.55  98.55]\n",
      " [100.    97.1   89.86  84.06]\n",
      " [100.    98.55  98.55  98.55]]\n"
     ]
    }
   ],
   "source": [
    "print(  (acc_list[:,1,:]  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
