{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse \n",
    "from numpy.lib.function_base import copy\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "from scipy import *\n",
    "from numpy import dot, multiply, diag, power, pi, exp, sin, cos, cosh, tanh, real, imag\n",
    "from numpy.linalg import inv, eig, pinv,norm\n",
    "from scipy.linalg import svd, svdvals \n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io as sio  \n",
    "import re \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD   \n",
    "import torch.optim as optim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "savename = 'Trade_regu_new11'\n",
    "fault_type = 2; # 0--Tp; 1--DLG; 2--LG; 3--LL\n",
    "impe_type = 1; # 1~2 fault impedance increases when the fault_type is from 1-3  \n",
    "n_class =87\n",
    "dim_input = 1\n",
    "# parameters for CNN \n",
    "patience = 30  \n",
    "lambda_loss_amount = 0.02#0.001 # gamma 0.1\n",
    "batch_size =70\n",
    "display_step = 100  \n",
    "num_bus =68 \n",
    "learning_rate = 0.01               \n",
    "rootPath =  '/Users/wenting/Documents/04_research/08_Explainable_DNN/02_simu/01_data'\n",
    "trainName = 'train_original_sigPQ.mat'#'train_PQ_pertub_02pu.mat'   \n",
    "testName = 'testing_sigPQ_perturb_0.5_percentage'#  'Test_perturb_0.5pu_type_' + str(fault_type) + '_impedance_' + str(impe_type) + '_sigNew'#'Test_input_perturb_05pu_type_' + str(fault_type) + '_impedance_' + str(impe_type) + '_new'\n",
    "model_dir  = '/Users/wenting/Documents/04_research/08_Explainable_DNN/02_simu/03_saved' \n",
    "epsilon = 0.01\n",
    "k = 7\n",
    "alpha = 0.01#step size for sigma \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr_max = 0.1 * learning_rate\n",
    "lr_min = 1.2 * learning_rate \n",
    "weight_decay = 5e-5\n",
    "epochs =1000\n",
    "lr_schedule = 'cyclic'\n",
    "dim_input = 1 \n",
    "dim_hidden = [4,8,8,8]\n",
    "nclass = 87\n",
    "seed = 1\n",
    "pgd_alpha =  alpha\n",
    "early_stop = False\n",
    "beta = 0.1 # lambda 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadline():\n",
    "    #linePath = '/Users/wenting/Documents/04_research/08_Explainable_DNN/02_simu/01_data' \n",
    "    data = sio.loadmat(os.path.join(rootPath, 'train_original_sig.mat'))\n",
    "    line = data['line']\n",
    "    Y = data['Y']\n",
    "    #v = data['dV_feature']\n",
    "    #psi= imag(Y @ v)\n",
    "    labels = data['Labels'][0]\n",
    "    Yabs = abs(Y)\n",
    "    Yabs1= Yabs.todense()# - Yabs.diagonal() # not include itself\n",
    "    line_neib = {}\n",
    "    for r in range(line.shape[0]):\n",
    "        busl , busr = line[r, :2]\n",
    "        nl = np.r_[np.where(line[:, 0] ==  busl )[0], np.where(line[:, 1] ==  busl )[0] ]\n",
    "        nr = np.r_[np.where(line[:, 0] ==  busr )[0], np.where(line[:,1] ==  busr )[0] ] \n",
    "        #print(np.r_[nl, nr])\n",
    "        line_neib[r + 1] = np.unique(np.r_[nl+1, nr+1])\n",
    "    return line, Y.todense(),    line_neib \n",
    "\n",
    "def normalize(x):\n",
    "    N, col = x.shape\n",
    "    x = (x - np.mean(x, 0))/np.std(x, 0)\n",
    "    return x\n",
    "  \n",
    "def load_data_VI(w,path,name, norm = 0): \n",
    "    base = 100.0\n",
    "    # 'I0_all', 'Itheta0_all','I1_all', 'Itheta1_all', 'V0_all', 'theta0_all','V1_all', 'theta1_all', 'Labels','I1_all', 'I0_all' , 'Y', 'line', 'bus' \n",
    "    PathName = os.path.join(path, name)\n",
    "    data=sio.loadmat(PathName);   \n",
    "    V0= data['V0_all'] \n",
    "    theta0= data['theta0_all'] \n",
    "    V1= data['V1_all'] \n",
    "    theta1= data['theta1_all'] \n",
    "    I0= data['I0_all'] \n",
    "    Itheta0= data['Itheta0_all'] \n",
    "    I1= data['I1_all'] \n",
    "    Itheta1= data['Itheta1_all'] \n",
    "    Y_ad= data['Y'] \n",
    "    num_bus, num_sample = np.shape(V0)\n",
    "    dreal = np.zeros((num_bus, num_sample))\n",
    "    dimg = np.zeros((num_bus, num_sample)) \n",
    "    dmag  = V1  - V0 \n",
    "    dtheta  = theta1  - theta0 \n",
    "    temp_real = dmag * np.cos(dtheta)\n",
    "    #print('before',temp_real[:, 0])\n",
    "    dreal[w,:] =temp_real[w,:]\n",
    "    #print('after',dreal[:, 0])\n",
    "    temp_imag = (dmag * np.sin(dtheta))\n",
    "    dimg[w,:] = temp_imag[w,:] \n",
    "    dIreal = np.zeros((num_bus, num_sample))\n",
    "    dIimg = np.zeros((num_bus, num_sample))\n",
    "    dImag = (I1  - I0  ) \n",
    "    dItheta  = Itheta1 - Itheta0 \n",
    "    tempI_r = (dImag * np.cos(dItheta)) \n",
    "    dIreal[w,:] = tempI_r[w,:] \n",
    "    tempI_i =  (dImag * np.sin(dItheta))\n",
    "    dIimg[w,:] =tempI_i[w,:]\n",
    "    #Labels = data['Labels'][0]\n",
    "    #psi= Y_ad[:,w] @ dV[w,:] \n",
    "    train_x = np.float64(np.r_[dreal, dimg].T)#, dIreal, dIimg \n",
    "    train_i = np.float64(np.r_[  dIreal, dIimg ].T)\n",
    "    #print(train_x[0,:])\n",
    "    if norm:\n",
    "        train_x = normalize(train_x)\n",
    "    #train_psi = np.float64((psi).imag.T )  \n",
    "    train_labels = data['Labels'][0]# sio.loadmat(os.path.join(path, 'train_PQ_pertub_1pu.mat'))['Labels'][0]#data['y_num'] \n",
    "    col, buses = np.shape(train_x) \n",
    "    #train_psi =   torch.FloatTensor(train_psi)\n",
    "    train_x = torch.FloatTensor(train_x)\n",
    "    train_x = torch.unsqueeze(train_x, 1)\n",
    "    train_x = torch.unsqueeze(train_x, 3)\n",
    "    train_i = torch.FloatTensor(train_i)\n",
    "    train_i = torch.unsqueeze(train_i, 1)\n",
    "    train_i = torch.unsqueeze(train_i, 3)\n",
    "    #train_psi = torch.unsqueeze(train_psi, 1)\n",
    "    #train_psi = torch.unsqueeze(train_psi, 3)\n",
    "    #train_x = np.float64(np.reshape(train_data, (int(col ), buses,times)))  \n",
    "    train_y = np.reshape(train_labels, [col,]) #np.zeros((int(col/times), n_classes))\n",
    "    #for i in range(int(col/times)):\n",
    "        #train_y[i,train_labels[0][i] ] = 1;\n",
    "    return train_x, train_i,  torch.LongTensor(train_y) ,col \n",
    "    \n",
    "def load_data_VI_new(w,path,name,mag = False,  norm = 0): \n",
    "    base = 100.0\n",
    "    # 'I0_all', 'Itheta0_all','I1_all', 'Itheta1_all', 'V0_all', 'theta0_all','V1_all', 'theta1_all', 'Labels','I1_all', 'I0_all' , 'Y', 'line', 'bus' \n",
    "    PathName = os.path.join(path, name)\n",
    "    data=sio.loadmat(PathName);   \n",
    "    mag0= data['V0_all']  \n",
    "    theta0= data['theta0_all'] \n",
    "    mag1= data['V1_all'] \n",
    "    theta1= data['theta1_all'] \n",
    "    Imag0= data['I0_all'] \n",
    "    Itheta0= data['Itheta0_all'] \n",
    "    Imag1= data['I1_all'] \n",
    "    Itheta1= data['Itheta1_all'] \n",
    "    Y_ad= data['Y'] \n",
    "    V0 = mag0 * np.cos(theta0  ) + 1j * mag0 * np.sin(theta0  )\n",
    "    V1 = mag1 * np.cos(theta1 ) + 1j * mag1 * np.sin(theta1  )\n",
    "    num_bus, num_sample = np.shape(V0)\n",
    "    dV = V1 - V0 \n",
    "    if not mag:  \n",
    "        dreal = np.zeros((num_bus, num_sample))\n",
    "        dimg = np.zeros((num_bus, num_sample))  \n",
    "        all_real = np.real(dV)\n",
    "        all_imag = np.imag(dV) \n",
    "        dreal[w,:] =all_real[w,:]  \n",
    "        dimg[w,:] = all_imag[w,:]  \n",
    "        train_x = np.float64(np.r_[dreal, dimg].T) \n",
    "    else:\n",
    "        dmag = np.zeros((num_bus, num_sample))\n",
    "        dtheta = np.zeros((num_bus, num_sample))  \n",
    "        all_mag = np.abs(dV)\n",
    "        all_theta = np.angle(dV) \n",
    "        dmag[w,:] =all_mag[w,:]  \n",
    "        dtheta[w,:] = all_theta[w,:]  \n",
    "        train_x = np.float64(np.r_[dmag, dtheta].T)  \n",
    "    if norm:\n",
    "        train_x = normalize(train_x) \n",
    "    train_labels = data['Labels'][0]# sio.loadmat(os.path.join(path, 'train_PQ_pertub_1pu.mat'))['Labels'][0]#data['y_num'] \n",
    "    col, buses = np.shape(train_x) \n",
    "    #train_psi =   torch.FloatTensor(train_psi)\n",
    "    train_x = torch.FloatTensor(train_x)\n",
    "    train_x = torch.unsqueeze(train_x, 1)\n",
    "    train_x = torch.unsqueeze(train_x, 3) \n",
    "    train_y = np.reshape(train_labels, [col,])  \n",
    "    return train_x,   torch.LongTensor(train_y) ,col  \n",
    " \n",
    "\n",
    "def choose_w(line, thres, num_bus = 68): # choose the measured buses by the threshold of degrees\n",
    "    all_freq =np.zeros((1,num_bus))\n",
    "    for i in range( num_bus):\n",
    "        ifreq = np.shape(np.where(line[:,0] == i+1 ))[1] + np.shape(np.where(line[:,1] == i+1))[1]\n",
    "        all_freq[0][i] = ifreq \n",
    "    w = [i for i in range(num_bus) if all_freq[0][i] >thres]  \n",
    "    return w\n",
    "\n",
    "# load data\n",
    "def load_all_data_VI(w):\n",
    "    global train_x,    train_labels, train_num, test_x,test_psi, test_labels, test_num,  samples,buses, times \n",
    "    train_x,   train_labels, train_num = load_data_VI_new(w,rootPath, trainName)  \n",
    "    test_x,    test_labels,test_num= load_data_VI_new(w,rootPath, testName)  \n",
    "    samples,buses,_, _  = np.shape(train_x)  \n",
    "\n",
    "def one_hot_neib(test_labels, line_neib):\n",
    "    num_class = torch.max(test_labels)  + 1\n",
    "    num_sample = test_labels.shape[0]\n",
    "    y_neib = torch.zeros((num_sample, num_class)) \n",
    "    for i in range(num_sample): \n",
    "        ind = (test_labels[i].cpu() ).numpy()   \n",
    "        y_neib[i,  line_neib[int(ind)  ]  ] = 1  \n",
    "        #assert line_neib[test_labels[i]-1].shape[0] == line_neib[int(ind)-1].shape[0]\n",
    "    return y_neib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self, dim_input = 1, dim_hidden = [4,8,8,8], batch_size = 50 ):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d( dim_input , dim_hidden[0] ,  kernel_size=(5,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[0]), # normalize along the 2-dim if input has four dims \n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(dim_hidden[0], dim_hidden[1], kernel_size=(5,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[1]),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(dim_hidden[1], dim_hidden[2], kernel_size= (3,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[2]),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(dim_hidden[2], dim_hidden[3], kernel_size= (3,1), stride=1, padding=0),\n",
    "            BatchNorm2d(dim_hidden[3]),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=(2,1), stride=(2,1)),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(6 * dim_hidden[3] , nclass)\n",
    "        )\n",
    "    \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x) \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.linear_layers(x)\n",
    "        return x \n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        #torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.xavier_uniform_(m.weight) \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias) \n",
    "        \n",
    "def evaluate_pgd(test_loader, model, attack_iters, restarts):\n",
    "    epsilon = (8 / 255.) / std\n",
    "    alpha = (2 / 255.) / std\n",
    "    pgd_loss = 0\n",
    "    pgd_acc = 0\n",
    "    n = 0\n",
    "    model.eval()\n",
    "    for i, (X, y) in enumerate(test_loader):\n",
    "        #X, y = X.cuda(), y.cuda()\n",
    "        pgd_delta = attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts)\n",
    "        with torch.no_grad():\n",
    "            output = model(X + pgd_delta)\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            pgd_loss += loss.item() * y.size(0)\n",
    "            pgd_acc += (output.max(1)[1] == y).sum().item()\n",
    "            n += y.size(0)\n",
    "    return pgd_loss/n, pgd_acc/n\n",
    "\n",
    "\n",
    "def evaluate_standard(test_loader, model):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    n = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "            #X, y = X.cuda(), y.cuda()\n",
    "            output = model(X)\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            test_loss += loss.item() * y.size(0)\n",
    "            test_acc += (output.max(1)[1] == y).sum().item()\n",
    "            n += y.size(0)\n",
    "    return test_loss/n, test_acc/n\n",
    "\n",
    "def clamp(X, lower_limit, upper_limit): \n",
    "    return torch.max(torch.min(X, upper_limit ), lower_limit)   \n",
    "\n",
    " \n",
    "\n",
    "def attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts, opt=None):\n",
    "    # Restart PGD to find the adversarial examples that have larger loss than the max_loss\n",
    "    #After restarts serveral times, many data samples have the worst-case perturbations generated with different init delta\n",
    "    max_loss = torch.zeros(y.shape[0]) \n",
    "    max_delta = torch.zeros_like(X) \n",
    "    for zz in range(restarts): \n",
    "        delta = torch.FloatTensor(X).uniform_(-epsilon, epsilon)\n",
    "        #delta.data = clamp(delta, lower_limit - X, upper_limit - X)\n",
    "        delta.requires_grad = True\n",
    "        for _ in range(attack_iters):\n",
    "            output = model(X + delta)\n",
    "            index = torch.where(output.max(1)[1] == y)# only use the correct examples\n",
    "            if len(index[0]) == 0:\n",
    "                break\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            loss.backward()\n",
    "            grad = delta.grad.detach()\n",
    "            d = delta[index[0], :, :, :]\n",
    "            g = grad[index[0], :, :, :]\n",
    "            limit = epsilon* torch.ones_like( delta[index[0], :, :, :])\n",
    "            d = clamp(d + alpha * torch.sign(g), -limit, limit)\n",
    "            #d = clamp(d, lower_limit - X[index[0], :, :, :], upper_limit - X[index[0], :, :, :]) \n",
    "            delta.data[index[0], :, :, :] = d\n",
    "            delta.grad.zero_()\n",
    "        all_loss = F.cross_entropy(model(X+delta), y, reduction='none').detach()\n",
    "        max_delta[all_loss >= max_loss] = delta.detach()[all_loss >= max_loss]\n",
    "        max_loss = torch.max(max_loss, all_loss)\n",
    "    return max_delta\n",
    "\n",
    "\n",
    "model = Net(dim_input, dim_hidden)  \n",
    "model.apply(weights_init) \n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "#optimizer = SGD(model.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)   \n",
    "criterion = CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_dist():\n",
    "    data = sio.loadmat(os.path.join(rootPath, 'distri_cur_nofault.mat'))\n",
    "    up_real = data['up_real']\n",
    "    up_imag = data['up_imag']\n",
    "    down_real = data['down_real']\n",
    "    down_imag = data['down_imag'] \n",
    "    up_limit = np.r_[up_real, up_imag]\n",
    "    down_limit = np.r_[down_real, down_imag]\n",
    "    return  up_limit, down_limit\n",
    "\n",
    "def vol_dist():\n",
    "    data = sio.loadmat(os.path.join(rootPath, 'distri_vol_nofault.mat'))\n",
    "    up_real = data['up_real']\n",
    "    up_imag = data['up_imag']\n",
    "    down_real = data['down_real']\n",
    "    down_imag = data['down_imag'] \n",
    "    up_limit = np.r_[up_real, up_imag]\n",
    "    down_limit = np.r_[down_real, down_imag]\n",
    "    return  up_limit, down_limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def sample_I(Imean_list, Istd_list, choice, num):\n",
    "    rol, col = Imean_list.shape \n",
    "    assert choice < col \n",
    "    result = np.zeros((num, rol))\n",
    "    for i in range(rol):\n",
    "        if (Imean_list[ i, choice] - Istd_list[ i, choice]) < (Imean_list[ i, choice] + Istd_list[ i, choice]):\n",
    "            result[:, i] = np.random.uniform( (Imean_list[ i, choice] - Istd_list[ i, choice]),(Imean_list[ i, choice] + Istd_list[ i, choice]),num)\n",
    "    return result \n",
    "\n",
    "def range_I(Imean_list, Istd_list ):\n",
    "    rol, col = Imean_list.shape  \n",
    "    rangel = np.zeros((rol, 1))\n",
    "    ranger = np.zeros((rol, 1))\n",
    "    for i in range(rol):\n",
    "        rangel[ i ] = max(Imean_list[ i, :]  ) \n",
    "        ranger[ i] = min(Imean_list[ i, :] - Istd_list[ i, :] ) \n",
    "    return rangel, ranger\n",
    "\n",
    "def convert_shape(up_limit, batch_size):\n",
    "    up_limit = torch.FloatTensor(up_limit)\n",
    "    up_limit = torch.unsqueeze(up_limit, 0)\n",
    "    up_limit = torch.unsqueeze(up_limit, 0)\n",
    "    up_limit  = up_limit.repeat(batch_size,1,1,1)\n",
    "    return up_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 136)\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat(os.path.join(rootPath, trainName))\n",
    "linedata, Y,  line_neib = loadline() \n",
    "Y_ri = np.r_[np.c_[Y.real, -Y.imag], np.c_[ Y.imag, Y.real ]].T\n",
    "print(Y_ri.shape)\n",
    "w = choose_w(linedata,2)  \n",
    "load_all_data_VI(w ) \n",
    "up_limit = np.r_[np.ones((num_bus*4, 1))*epsilon ]\n",
    "down_limit = np.r_[-np.ones((num_bus*4, 1))*epsilon ]\n",
    "    \n",
    "#cur_up_limit, cur_down_limit = current_dist() \n",
    "#vol_up_limit, vol_down_limit = vol_dist() \n",
    "#up_limit = epsilon * np.r_[vol_up_limit, cur_up_limit]\n",
    "#down_limit = epsilon *np.r_[vol_down_limit, cur_down_limit]\n",
    "\n",
    "#plt.plot( cur_up_limit[:52,:])\n",
    "#plt.plot(cur_down_limit[:52,:])\n",
    "#plt.show()\n",
    "#plt.plot( vol_up_limit )\n",
    "#plt.plot(vol_down_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval_train(model, device, train_loader):\n",
    "    model.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            train_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('Training: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        train_loss, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "    training_accuracy = correct / len(train_loader.dataset)\n",
    "    return train_loss, training_accuracy\n",
    "\n",
    "\n",
    "def eval_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = args.lr\n",
    "    if epoch >= 75:\n",
    "        lr = args.lr * 0.1\n",
    "    if epoch >= 90:\n",
    "        lr = args.lr * 0.01\n",
    "    if epoch >= 100:\n",
    "        lr = args.lr * 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_l2_norm(x):\n",
    "    flattened = x.view(x.unsqueeze(0).shape[0], -1)\n",
    "    return (flattened ** 2).sum(1)\n",
    "\n",
    "\n",
    "def l2_norm(x):\n",
    "    return squared_l2_norm(x).sqrt()\n",
    "\n",
    "\n",
    "def trades_loss( model,\n",
    "                x_natural,\n",
    "                y,\n",
    "                optimizer,\n",
    "                up_limit, \n",
    "                down_limit,\n",
    "                step_size=0.003, \n",
    "                perturb_steps=10,\n",
    "                beta=1.0,\n",
    "                distance='l_inf'):\n",
    "    # define KL-loss\n",
    "    criterion_kl = nn.KLDivLoss(size_average=False)#\n",
    "    Physics_regu = nn.L1Loss(size_average = False)\n",
    "    model.eval()\n",
    "    batch_size, _, size_U, _ = x_natural.shape \n",
    "    up_limit = convert_shape(up_limit, batch_size)\n",
    "    down_limit = convert_shape(down_limit, batch_size)\n",
    "    \n",
    "    # generate adversarial example \n",
    "    delta = torch.randn((batch_size, 1, 2*size_U, 1)).detach()\n",
    "    delta = torch.min(torch.max(delta,   down_limit),   up_limit)\n",
    "    #delta.requires_grad = True \n",
    "    #x_adv = x_natural.detach() + 0.001 * delta \n",
    "    if distance == 'l_inf': \n",
    "        x_adv = x_natural +   delta[:, :, :size_U, :]\n",
    "        for _ in range(perturb_steps): \n",
    "            delta.requires_grad_()\n",
    "            delta_U = delta[:, 0, :size_U, 0]\n",
    "            delta_I = delta[:, 0, size_U:,0]  \n",
    "            with torch.enable_grad():\n",
    "                loss_kl = beta *  criterion_kl(F.log_softmax(model(x_natural \\\n",
    "                         +  delta[:, :, :size_U, :] ), dim=1),F.softmax(model(x_natural), dim=1))\\\n",
    "                - lambda_loss_amount*Physics_regu( (torch.matmul( delta_U , torch.FloatTensor(Y_ri))   ), (delta_I))\n",
    "            grad = torch.autograd.grad(loss_kl, [delta])[0]\n",
    "            delta = delta.detach() + step_size * torch.sign(grad.detach()) \n",
    "            delta = torch.min(torch.max(delta,   down_limit),   up_limit)\n",
    "            x_adv = x_natural +  delta[:, :, :size_U, :]\n",
    "            #x_adv = torch.clamp(x_adv, 0.0, 1.0) \n",
    "    else:\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "    model.train() \n",
    "    #x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n",
    "    x_adv = x_natural.detach()  + delta[:, :, :size_U, :].detach()#Variable( x_adv , requires_grad=False)\n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "    # calculate robust loss\n",
    "    logits = model(x_natural)\n",
    "    loss_natural = F.cross_entropy(logits, y)\n",
    "    loss_robust = beta * (1/batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n",
    "                                                    F.softmax(model(x_natural), dim=1))\\\n",
    "    #- lambda_loss_amount*  (1/batch_size) *Physics_regu( (torch.matmul( delta_U , torch.FloatTensor(Y_ri))   ), (delta_I))\n",
    "    loss = loss_natural + loss_robust\n",
    "    return loss , model(x_adv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_x, test_labels , model, line_neib ):\n",
    "    #test_x = torch.unsqueeze(test_x, 3) \n",
    "    test_x, test_labels = test_x , test_labels \n",
    "    num_test = test_labels.shape[0]\n",
    "    model.eval()\n",
    "    output = model(test_x  )  \n",
    "    loss_test = criterion(output , test_labels )\n",
    "    #loss_test = F.nll_loss(output , test_labels )\n",
    "    logit = torch.softmax(output, 1)\n",
    "    pred = output.max(1)[1] \n",
    "    acc_test = torch.eq(pred,test_labels).sum().item()*100/num_test\n",
    "    multi_labels =  one_hot_neib(test_labels, line_neib) \n",
    "    match = 0\n",
    "    #pred = output.max(1)[1] \n",
    "    for i in range(num_test):\n",
    "        match += multi_labels[i, pred[i]  ]  \n",
    "    acc_hop = torch.true_divide(match,num_test)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test ),\n",
    "         \"1-hop accuracy = {:.4f}\".format(acc_hop))\n",
    "    return acc_test, acc_hop\n",
    "\n",
    "def test_old(test_x, test_labels , model, line_neib ):\n",
    "    #test_x = torch.unsqueeze(test_x, 3) \n",
    "    test_x, test_labels = test_x , test_labels \n",
    "    num_test = test_labels.shape[0]\n",
    "    model.eval()\n",
    "    output = model(test_x  )  \n",
    "    loss_test = criterion(output , test_labels )\n",
    "    #loss_test = F.nll_loss(output , test_labels )\n",
    "    logit = torch.softmax(output, 1)\n",
    "    pred = output.max(1)[1] \n",
    "    acc_test = torch.eq(pred,test_labels).sum().item()*100/num_test\n",
    "    multi_labels =  one_hot_neib(test_labels, line_neib) \n",
    "    match = 0\n",
    "    #pred = output.max(1)[1] \n",
    "    for i in range(num_test):\n",
    "        match += multi_labels[i, pred[i]  ]  \n",
    "    acc_hop = torch.true_divide(match,num_test)\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test ),\n",
    "         \"1-hop accuracy = {:.4f}\".format(acc_hop))\n",
    "    return acc_test, acc_hop\n",
    "\n",
    "def main(seed, dim_input, dim_hidden):\n",
    "    np.random.seed( seed)\n",
    "    torch.manual_seed( seed)\n",
    "    #torch.cuda.manual_seed(  seed)\n",
    "    \n",
    "    model = Net(dim_input, dim_hidden) \n",
    "    model.apply(weights_init) \n",
    "    model.train()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "    criterion = CrossEntropyLoss()  \n",
    "    \n",
    "    #lr_steps = epochs * int(train_x.shape[0]/batch_size)\n",
    "    #if lr_schedule == 'cyclic':\n",
    "    #    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr= lr_min, max_lr= lr_max,\n",
    "    #        step_size_up=lr_steps / 2, step_size_down=lr_steps / 2, cycle_momentum=False )\n",
    "    #elif lr_schedule == 'multistep':\n",
    "    #    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[lr_steps / 2, lr_steps * 3 / 4], gamma=0.1)\n",
    "    # Training\n",
    "    #Imean_list, Istd_list = current_dist()\n",
    "    pre_robust_acc = 0.\n",
    "    start = time.time() \n",
    "    x_train, y_train = Variable(train_x)  , Variable(train_labels) \n",
    "    train_best = float('Inf') \n",
    "    train_loss_list= []\n",
    "    for epoch in range( epochs): \n",
    "        start_epoch_time = time.time() \n",
    "        train_acc = 0\n",
    "        train_n = 0 \n",
    "        train_loss = 0\n",
    "        #sample_deltaI = sample_I(Imean_list, Istd_list, epoch%4, batch_size)\n",
    "        #delta_I =   torch.FloatTensor( sample_deltaI)\n",
    "        for i in range(int(train_x.shape[0] / batch_size)):\n",
    "            id_train = np.random.choice(train_x.shape[0], batch_size, replace= False) \n",
    "            optimizer.zero_grad() \n",
    "            loss, output  = trades_loss(  model=model,\n",
    "                               x_natural=x_train[id_train],\n",
    "                               y=y_train[id_train],\n",
    "                               optimizer=optimizer,\n",
    "                               up_limit = up_limit,\n",
    "                               down_limit = down_limit,\n",
    "                               step_size= alpha, \n",
    "                               perturb_steps= k,\n",
    "                               beta= beta)   \n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            train_loss_list.append(loss.item())  #* y_train[id_train].size(0)\n",
    "            train_acc  = (output.max(1)[1] == y_train[id_train]).sum().item() *100/np.shape(id_train)[0]  \n",
    "            train_n += y_train[id_train].size(0)\n",
    "            if epoch%5 == 0: \n",
    "                print('Training Epoch: {}, [{}/{}, {:.0f}%], loss is {:.6f}'\\\n",
    "                      .format(epoch  , i * batch_size, train_x.shape[0],\\\n",
    "                              train_acc  , loss.item()  )) \n",
    "                if train_loss < train_best:\n",
    "                    print('best model')\n",
    "                    print('Epoch', epoch)\n",
    "                    train_best += loss.item()\n",
    "                    torch.save(model.state_dict(), os.path.join(model_dir,  'model-' + savename + '.pt'))\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(model_dir, 'opt-' + savename + '.tar'))\n",
    "            train_loss_list.append(loss.item())\n",
    "            #scheduler.step() \n",
    "        train_time = time.time()\n",
    "        if not early_stop:\n",
    "            best_state_dict = model.state_dict() \n",
    "        #torch.save(model.state_dict(), os.path.join(model_dir, 'model-fgsm_sig_noregu.pt'))\n",
    "        #torch.save(optimizer.state_dict(), os.path.join(model_dir, 'opt-fgsm-checkpoint_sig_noregu.tar'))\n",
    "         \n",
    "        # Evaluation\n",
    "    model_test = Net(dim_input, dim_hidden)\n",
    "    model_test.load_state_dict(best_state_dict)\n",
    "    model_test.float()\n",
    "    model_test.eval()\n",
    "    test_x,  test_labels,test_num= load_data_VI_new(w,rootPath, testName)  \n",
    "    acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "    plt.plot(train_loss_list)\n",
    "    return train_loss_list , train_best, model_test\n",
    "\n",
    "    #pgd_loss, pgd_acc = evaluate_pgd(test_loader, model_test, 50, 10)\n",
    "    #test_loss, test_acc = evaluate_standard(test_loader, model_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenting/opt/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 0, [0/560, 0%], loss is 4.604379\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 0, [70/560, 3%], loss is 4.493090\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 0, [140/560, 7%], loss is 4.434260\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 0, [210/560, 3%], loss is 4.296442\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 0, [280/560, 0%], loss is 4.267248\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 0, [350/560, 3%], loss is 4.235191\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 0, [420/560, 3%], loss is 4.060098\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 0, [490/560, 10%], loss is 3.998548\n",
      "best model\n",
      "Epoch 0\n",
      "Training Epoch: 5, [0/560, 34%], loss is 2.419361\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 5, [70/560, 34%], loss is 2.247650\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 5, [140/560, 43%], loss is 2.205443\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 5, [210/560, 47%], loss is 2.085081\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 5, [280/560, 40%], loss is 2.262362\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 5, [350/560, 50%], loss is 1.883036\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 5, [420/560, 39%], loss is 2.000816\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 5, [490/560, 39%], loss is 2.059906\n",
      "best model\n",
      "Epoch 5\n",
      "Training Epoch: 10, [0/560, 51%], loss is 1.341782\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 10, [70/560, 60%], loss is 1.240372\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 10, [140/560, 70%], loss is 1.052054\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 10, [210/560, 64%], loss is 1.178527\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 10, [280/560, 64%], loss is 1.047397\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 10, [350/560, 66%], loss is 1.147610\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 10, [420/560, 67%], loss is 1.014624\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 10, [490/560, 66%], loss is 1.079427\n",
      "best model\n",
      "Epoch 10\n",
      "Training Epoch: 15, [0/560, 67%], loss is 0.807981\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 15, [70/560, 69%], loss is 0.740866\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 15, [140/560, 54%], loss is 1.007629\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 15, [210/560, 67%], loss is 0.871852\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 15, [280/560, 63%], loss is 0.846725\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 15, [350/560, 69%], loss is 0.878922\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 15, [420/560, 69%], loss is 0.847708\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 15, [490/560, 71%], loss is 0.751673\n",
      "best model\n",
      "Epoch 15\n",
      "Training Epoch: 20, [0/560, 79%], loss is 0.614849\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 20, [70/560, 84%], loss is 0.547517\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 20, [140/560, 77%], loss is 0.547186\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 20, [210/560, 86%], loss is 0.483443\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 20, [280/560, 87%], loss is 0.454229\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 20, [350/560, 80%], loss is 0.537078\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 20, [420/560, 71%], loss is 0.580146\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 20, [490/560, 79%], loss is 0.493637\n",
      "best model\n",
      "Epoch 20\n",
      "Training Epoch: 25, [0/560, 81%], loss is 0.336296\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 25, [70/560, 74%], loss is 0.430526\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 25, [140/560, 64%], loss is 0.499062\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 25, [210/560, 67%], loss is 0.436702\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 25, [280/560, 74%], loss is 0.407694\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 25, [350/560, 74%], loss is 0.488957\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 25, [420/560, 64%], loss is 0.438851\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 25, [490/560, 73%], loss is 0.393902\n",
      "best model\n",
      "Epoch 25\n",
      "Training Epoch: 30, [0/560, 57%], loss is 0.543871\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 30, [70/560, 76%], loss is 0.332967\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 30, [140/560, 70%], loss is 0.474378\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 30, [210/560, 71%], loss is 0.461884\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 30, [280/560, 64%], loss is 0.416134\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 30, [350/560, 57%], loss is 0.641821\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 30, [420/560, 61%], loss is 0.559778\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 30, [490/560, 70%], loss is 0.366426\n",
      "best model\n",
      "Epoch 30\n",
      "Training Epoch: 35, [0/560, 66%], loss is 0.363599\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 35, [70/560, 66%], loss is 0.402660\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 35, [140/560, 70%], loss is 0.413758\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 35, [210/560, 69%], loss is 0.426114\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 35, [280/560, 77%], loss is 0.349437\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 35, [350/560, 70%], loss is 0.412115\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 35, [420/560, 77%], loss is 0.322675\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 35, [490/560, 81%], loss is 0.304431\n",
      "best model\n",
      "Epoch 35\n",
      "Training Epoch: 40, [0/560, 77%], loss is 0.290025\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 40, [70/560, 74%], loss is 0.325477\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 40, [140/560, 83%], loss is 0.305108\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 40, [210/560, 74%], loss is 0.337272\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 40, [280/560, 81%], loss is 0.267955\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 40, [350/560, 89%], loss is 0.205410\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 40, [420/560, 83%], loss is 0.235737\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 40, [490/560, 80%], loss is 0.255995\n",
      "best model\n",
      "Epoch 40\n",
      "Training Epoch: 45, [0/560, 71%], loss is 0.264053\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 45, [70/560, 73%], loss is 0.320970\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 45, [140/560, 71%], loss is 0.353533\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 45, [210/560, 81%], loss is 0.200310\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 45, [280/560, 86%], loss is 0.158374\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 45, [350/560, 81%], loss is 0.212091\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 45, [420/560, 76%], loss is 0.243732\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 45, [490/560, 81%], loss is 0.260244\n",
      "best model\n",
      "Epoch 45\n",
      "Training Epoch: 50, [0/560, 73%], loss is 0.288467\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 50, [70/560, 69%], loss is 0.319411\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 50, [140/560, 70%], loss is 0.257017\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 50, [210/560, 67%], loss is 0.286886\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 50, [280/560, 76%], loss is 0.289259\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 50, [350/560, 73%], loss is 0.219425\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 50, [420/560, 79%], loss is 0.240692\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 50, [490/560, 70%], loss is 0.241182\n",
      "best model\n",
      "Epoch 50\n",
      "Training Epoch: 55, [0/560, 79%], loss is 0.295786\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 55, [70/560, 81%], loss is 0.208773\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 55, [140/560, 76%], loss is 0.222090\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 55, [210/560, 83%], loss is 0.257333\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 55, [280/560, 77%], loss is 0.274499\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 55, [350/560, 79%], loss is 0.296018\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 55, [420/560, 84%], loss is 0.216156\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 55, [490/560, 81%], loss is 0.249761\n",
      "best model\n",
      "Epoch 55\n",
      "Training Epoch: 60, [0/560, 67%], loss is 0.325642\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 60, [70/560, 81%], loss is 0.239944\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 60, [140/560, 83%], loss is 0.323338\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 60, [210/560, 83%], loss is 0.203820\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 60, [280/560, 76%], loss is 0.171050\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 60, [350/560, 81%], loss is 0.210890\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 60, [420/560, 77%], loss is 0.272700\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 60, [490/560, 71%], loss is 0.330414\n",
      "best model\n",
      "Epoch 60\n",
      "Training Epoch: 65, [0/560, 64%], loss is 0.514297\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 65, [70/560, 69%], loss is 0.576852\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 65, [140/560, 70%], loss is 0.345407\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 65, [210/560, 74%], loss is 0.259769\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 65, [280/560, 69%], loss is 0.381362\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 65, [350/560, 74%], loss is 0.355805\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 65, [420/560, 66%], loss is 0.462569\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 65, [490/560, 69%], loss is 0.364116\n",
      "best model\n",
      "Epoch 65\n",
      "Training Epoch: 70, [0/560, 74%], loss is 0.325633\n",
      "best model\n",
      "Epoch 70\n",
      "Training Epoch: 70, [70/560, 79%], loss is 0.195572\n",
      "best model\n",
      "Epoch 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 70, [140/560, 69%], loss is 0.264743\n",
      "best model\n",
      "Epoch 70\n",
      "Training Epoch: 70, [210/560, 73%], loss is 0.308108\n",
      "best model\n",
      "Epoch 70\n",
      "Training Epoch: 70, [280/560, 79%], loss is 0.274429\n",
      "best model\n",
      "Epoch 70\n",
      "Training Epoch: 70, [350/560, 74%], loss is 0.323094\n",
      "best model\n",
      "Epoch 70\n",
      "Training Epoch: 70, [420/560, 77%], loss is 0.229127\n",
      "best model\n",
      "Epoch 70\n",
      "Training Epoch: 70, [490/560, 67%], loss is 0.291534\n",
      "best model\n",
      "Epoch 70\n",
      "Training Epoch: 75, [0/560, 77%], loss is 0.148341\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 75, [70/560, 84%], loss is 0.168947\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 75, [140/560, 77%], loss is 0.194371\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 75, [210/560, 86%], loss is 0.128543\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 75, [280/560, 93%], loss is 0.108190\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 75, [350/560, 90%], loss is 0.169675\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 75, [420/560, 87%], loss is 0.140808\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 75, [490/560, 86%], loss is 0.173424\n",
      "best model\n",
      "Epoch 75\n",
      "Training Epoch: 80, [0/560, 70%], loss is 0.239603\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 80, [70/560, 74%], loss is 0.163843\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 80, [140/560, 73%], loss is 0.266517\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 80, [210/560, 86%], loss is 0.121221\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 80, [280/560, 84%], loss is 0.151584\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 80, [350/560, 83%], loss is 0.150337\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 80, [420/560, 86%], loss is 0.171722\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 80, [490/560, 74%], loss is 0.200647\n",
      "best model\n",
      "Epoch 80\n",
      "Training Epoch: 85, [0/560, 77%], loss is 0.197059\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 85, [70/560, 69%], loss is 0.224575\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 85, [140/560, 81%], loss is 0.221555\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 85, [210/560, 70%], loss is 0.220154\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 85, [280/560, 79%], loss is 0.195155\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 85, [350/560, 64%], loss is 0.261851\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 85, [420/560, 76%], loss is 0.217611\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 85, [490/560, 77%], loss is 0.148048\n",
      "best model\n",
      "Epoch 85\n",
      "Training Epoch: 90, [0/560, 79%], loss is 0.193518\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 90, [70/560, 79%], loss is 0.197820\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 90, [140/560, 71%], loss is 0.185037\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 90, [210/560, 80%], loss is 0.175897\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 90, [280/560, 73%], loss is 0.158718\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 90, [350/560, 77%], loss is 0.195287\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 90, [420/560, 76%], loss is 0.175950\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 90, [490/560, 71%], loss is 0.235270\n",
      "best model\n",
      "Epoch 90\n",
      "Training Epoch: 95, [0/560, 79%], loss is 0.188547\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 95, [70/560, 71%], loss is 0.230780\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 95, [140/560, 83%], loss is 0.119686\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 95, [210/560, 83%], loss is 0.178509\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 95, [280/560, 80%], loss is 0.128688\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 95, [350/560, 79%], loss is 0.196376\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 95, [420/560, 84%], loss is 0.114070\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 95, [490/560, 73%], loss is 0.181863\n",
      "best model\n",
      "Epoch 95\n",
      "Training Epoch: 100, [0/560, 74%], loss is 0.230041\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 100, [70/560, 69%], loss is 0.211783\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 100, [140/560, 76%], loss is 0.175740\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 100, [210/560, 80%], loss is 0.139968\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 100, [280/560, 76%], loss is 0.207925\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 100, [350/560, 77%], loss is 0.173208\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 100, [420/560, 76%], loss is 0.224466\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 100, [490/560, 71%], loss is 0.202783\n",
      "best model\n",
      "Epoch 100\n",
      "Training Epoch: 105, [0/560, 81%], loss is 0.164672\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 105, [70/560, 71%], loss is 0.159839\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 105, [140/560, 81%], loss is 0.201994\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 105, [210/560, 67%], loss is 0.262452\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 105, [280/560, 81%], loss is 0.144333\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 105, [350/560, 79%], loss is 0.129957\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 105, [420/560, 80%], loss is 0.223717\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 105, [490/560, 79%], loss is 0.190390\n",
      "best model\n",
      "Epoch 105\n",
      "Training Epoch: 110, [0/560, 76%], loss is 0.228423\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 110, [70/560, 86%], loss is 0.119915\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 110, [140/560, 81%], loss is 0.131358\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 110, [210/560, 76%], loss is 0.187533\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 110, [280/560, 71%], loss is 0.186477\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 110, [350/560, 89%], loss is 0.114606\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 110, [420/560, 81%], loss is 0.189872\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 110, [490/560, 81%], loss is 0.201108\n",
      "best model\n",
      "Epoch 110\n",
      "Training Epoch: 115, [0/560, 83%], loss is 0.200979\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 115, [70/560, 71%], loss is 0.244684\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 115, [140/560, 80%], loss is 0.290330\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 115, [210/560, 86%], loss is 0.095058\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 115, [280/560, 73%], loss is 0.177350\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 115, [350/560, 76%], loss is 0.303013\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 115, [420/560, 79%], loss is 0.242333\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 115, [490/560, 73%], loss is 0.249470\n",
      "best model\n",
      "Epoch 115\n",
      "Training Epoch: 120, [0/560, 76%], loss is 0.448802\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 120, [70/560, 74%], loss is 0.232776\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 120, [140/560, 66%], loss is 0.393333\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 120, [210/560, 63%], loss is 0.375942\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 120, [280/560, 70%], loss is 0.385544\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 120, [350/560, 74%], loss is 0.241702\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 120, [420/560, 70%], loss is 0.384920\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 120, [490/560, 76%], loss is 0.280029\n",
      "best model\n",
      "Epoch 120\n",
      "Training Epoch: 125, [0/560, 70%], loss is 0.222403\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 125, [70/560, 76%], loss is 0.188108\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 125, [140/560, 63%], loss is 0.268486\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 125, [210/560, 76%], loss is 0.237444\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 125, [280/560, 77%], loss is 0.195647\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 125, [350/560, 81%], loss is 0.169017\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 125, [420/560, 87%], loss is 0.142633\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 125, [490/560, 73%], loss is 0.226028\n",
      "best model\n",
      "Epoch 125\n",
      "Training Epoch: 130, [0/560, 77%], loss is 0.126501\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 130, [70/560, 70%], loss is 0.262940\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 130, [140/560, 81%], loss is 0.241407\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 130, [210/560, 83%], loss is 0.161603\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 130, [280/560, 81%], loss is 0.188152\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 130, [350/560, 80%], loss is 0.174938\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 130, [420/560, 81%], loss is 0.143812\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 130, [490/560, 76%], loss is 0.146575\n",
      "best model\n",
      "Epoch 130\n",
      "Training Epoch: 135, [0/560, 81%], loss is 0.119964\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 135, [70/560, 70%], loss is 0.176492\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 135, [140/560, 84%], loss is 0.113011\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 135, [210/560, 80%], loss is 0.115765\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 135, [280/560, 80%], loss is 0.144722\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 135, [350/560, 86%], loss is 0.190019\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 135, [420/560, 73%], loss is 0.233923\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 135, [490/560, 77%], loss is 0.167974\n",
      "best model\n",
      "Epoch 135\n",
      "Training Epoch: 140, [0/560, 83%], loss is 0.158938\n",
      "best model\n",
      "Epoch 140\n",
      "Training Epoch: 140, [70/560, 77%], loss is 0.202200\n",
      "best model\n",
      "Epoch 140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 140, [140/560, 79%], loss is 0.142025\n",
      "best model\n",
      "Epoch 140\n",
      "Training Epoch: 140, [210/560, 84%], loss is 0.131258\n",
      "best model\n",
      "Epoch 140\n",
      "Training Epoch: 140, [280/560, 77%], loss is 0.171515\n",
      "best model\n",
      "Epoch 140\n",
      "Training Epoch: 140, [350/560, 89%], loss is 0.087152\n",
      "best model\n",
      "Epoch 140\n",
      "Training Epoch: 140, [420/560, 77%], loss is 0.146612\n",
      "best model\n",
      "Epoch 140\n",
      "Training Epoch: 140, [490/560, 76%], loss is 0.167126\n",
      "best model\n",
      "Epoch 140\n",
      "Training Epoch: 145, [0/560, 86%], loss is 0.100878\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 145, [70/560, 80%], loss is 0.219345\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 145, [140/560, 74%], loss is 0.228432\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 145, [210/560, 81%], loss is 0.119846\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 145, [280/560, 79%], loss is 0.174426\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 145, [350/560, 79%], loss is 0.147493\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 145, [420/560, 77%], loss is 0.161042\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 145, [490/560, 79%], loss is 0.134259\n",
      "best model\n",
      "Epoch 145\n",
      "Training Epoch: 150, [0/560, 83%], loss is 0.110463\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 150, [70/560, 80%], loss is 0.180795\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 150, [140/560, 81%], loss is 0.199905\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 150, [210/560, 74%], loss is 0.248250\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 150, [280/560, 70%], loss is 0.370407\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 150, [350/560, 74%], loss is 0.337125\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 150, [420/560, 87%], loss is 0.124266\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 150, [490/560, 81%], loss is 0.182535\n",
      "best model\n",
      "Epoch 150\n",
      "Training Epoch: 155, [0/560, 71%], loss is 0.278370\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 155, [70/560, 76%], loss is 0.354879\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 155, [140/560, 76%], loss is 0.265109\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 155, [210/560, 80%], loss is 0.453673\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 155, [280/560, 76%], loss is 0.266596\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 155, [350/560, 79%], loss is 0.333821\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 155, [420/560, 83%], loss is 0.306953\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 155, [490/560, 80%], loss is 0.285058\n",
      "best model\n",
      "Epoch 155\n",
      "Training Epoch: 160, [0/560, 80%], loss is 0.158454\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 160, [70/560, 79%], loss is 0.192251\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 160, [140/560, 81%], loss is 0.250471\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 160, [210/560, 81%], loss is 0.216272\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 160, [280/560, 79%], loss is 0.231075\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 160, [350/560, 76%], loss is 0.200935\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 160, [420/560, 70%], loss is 0.212020\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 160, [490/560, 87%], loss is 0.157460\n",
      "best model\n",
      "Epoch 160\n",
      "Training Epoch: 165, [0/560, 87%], loss is 0.204814\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 165, [70/560, 90%], loss is 0.096499\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 165, [140/560, 84%], loss is 0.097796\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 165, [210/560, 90%], loss is 0.128464\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 165, [280/560, 89%], loss is 0.109934\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 165, [350/560, 86%], loss is 0.124586\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 165, [420/560, 87%], loss is 0.124846\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 165, [490/560, 86%], loss is 0.088223\n",
      "best model\n",
      "Epoch 165\n",
      "Training Epoch: 170, [0/560, 80%], loss is 0.125260\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 170, [70/560, 80%], loss is 0.160221\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 170, [140/560, 83%], loss is 0.101921\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 170, [210/560, 91%], loss is 0.119084\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 170, [280/560, 90%], loss is 0.138483\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 170, [350/560, 81%], loss is 0.170674\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 170, [420/560, 81%], loss is 0.161044\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 170, [490/560, 84%], loss is 0.146166\n",
      "best model\n",
      "Epoch 170\n",
      "Training Epoch: 175, [0/560, 74%], loss is 0.150559\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 175, [70/560, 79%], loss is 0.141825\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 175, [140/560, 84%], loss is 0.144756\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 175, [210/560, 86%], loss is 0.144015\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 175, [280/560, 76%], loss is 0.161812\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 175, [350/560, 79%], loss is 0.148521\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 175, [420/560, 74%], loss is 0.211295\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 175, [490/560, 81%], loss is 0.146318\n",
      "best model\n",
      "Epoch 175\n",
      "Training Epoch: 180, [0/560, 79%], loss is 0.149787\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 180, [70/560, 70%], loss is 0.193755\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 180, [140/560, 83%], loss is 0.148651\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 180, [210/560, 84%], loss is 0.117534\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 180, [280/560, 71%], loss is 0.161608\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 180, [350/560, 81%], loss is 0.140920\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 180, [420/560, 73%], loss is 0.191937\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 180, [490/560, 73%], loss is 0.231021\n",
      "best model\n",
      "Epoch 180\n",
      "Training Epoch: 185, [0/560, 84%], loss is 0.110994\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 185, [70/560, 80%], loss is 0.103871\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 185, [140/560, 81%], loss is 0.111460\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 185, [210/560, 84%], loss is 0.126268\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 185, [280/560, 89%], loss is 0.079508\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 185, [350/560, 81%], loss is 0.127676\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 185, [420/560, 86%], loss is 0.072904\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 185, [490/560, 87%], loss is 0.107048\n",
      "best model\n",
      "Epoch 185\n",
      "Training Epoch: 190, [0/560, 79%], loss is 0.135134\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 190, [70/560, 79%], loss is 0.087044\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 190, [140/560, 81%], loss is 0.128401\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 190, [210/560, 81%], loss is 0.125109\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 190, [280/560, 80%], loss is 0.141300\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 190, [350/560, 71%], loss is 0.140572\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 190, [420/560, 79%], loss is 0.148368\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 190, [490/560, 80%], loss is 0.113061\n",
      "best model\n",
      "Epoch 190\n",
      "Training Epoch: 195, [0/560, 74%], loss is 0.205864\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 195, [70/560, 84%], loss is 0.113573\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 195, [140/560, 79%], loss is 0.172235\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 195, [210/560, 81%], loss is 0.115864\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 195, [280/560, 77%], loss is 0.213324\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 195, [350/560, 81%], loss is 0.158234\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 195, [420/560, 87%], loss is 0.157615\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 195, [490/560, 86%], loss is 0.087313\n",
      "best model\n",
      "Epoch 195\n",
      "Training Epoch: 200, [0/560, 84%], loss is 0.119902\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 200, [70/560, 81%], loss is 0.101826\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 200, [140/560, 90%], loss is 0.091021\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 200, [210/560, 77%], loss is 0.201783\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 200, [280/560, 83%], loss is 0.224556\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 200, [350/560, 83%], loss is 0.097089\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 200, [420/560, 79%], loss is 0.162189\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 200, [490/560, 83%], loss is 0.107847\n",
      "best model\n",
      "Epoch 200\n",
      "Training Epoch: 205, [0/560, 77%], loss is 0.160982\n",
      "best model\n",
      "Epoch 205\n",
      "Training Epoch: 205, [70/560, 76%], loss is 0.125466\n",
      "best model\n",
      "Epoch 205\n",
      "Training Epoch: 205, [140/560, 86%], loss is 0.088590\n",
      "best model\n",
      "Epoch 205\n",
      "Training Epoch: 205, [210/560, 81%], loss is 0.136294\n",
      "best model\n",
      "Epoch 205\n",
      "Training Epoch: 205, [280/560, 77%], loss is 0.136236\n",
      "best model\n",
      "Epoch 205\n",
      "Training Epoch: 205, [350/560, 80%], loss is 0.117126\n",
      "best model\n",
      "Epoch 205\n",
      "Training Epoch: 205, [420/560, 80%], loss is 0.117808\n",
      "best model\n",
      "Epoch 205\n",
      "Training Epoch: 205, [490/560, 79%], loss is 0.141903\n",
      "best model\n",
      "Epoch 205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 210, [0/560, 83%], loss is 0.119387\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 210, [70/560, 79%], loss is 0.216068\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 210, [140/560, 86%], loss is 0.168698\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 210, [210/560, 89%], loss is 0.080762\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 210, [280/560, 93%], loss is 0.088663\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 210, [350/560, 86%], loss is 0.194292\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 210, [420/560, 84%], loss is 0.111197\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 210, [490/560, 80%], loss is 0.150507\n",
      "best model\n",
      "Epoch 210\n",
      "Training Epoch: 215, [0/560, 80%], loss is 0.213053\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 215, [70/560, 76%], loss is 0.252349\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 215, [140/560, 76%], loss is 0.213448\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 215, [210/560, 76%], loss is 0.169936\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 215, [280/560, 79%], loss is 0.157954\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 215, [350/560, 90%], loss is 0.076431\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 215, [420/560, 79%], loss is 0.170132\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 215, [490/560, 79%], loss is 0.281439\n",
      "best model\n",
      "Epoch 215\n",
      "Training Epoch: 220, [0/560, 80%], loss is 0.162794\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 220, [70/560, 89%], loss is 0.094905\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 220, [140/560, 81%], loss is 0.122054\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 220, [210/560, 79%], loss is 0.167517\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 220, [280/560, 76%], loss is 0.185602\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 220, [350/560, 80%], loss is 0.165425\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 220, [420/560, 81%], loss is 0.137974\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 220, [490/560, 89%], loss is 0.084122\n",
      "best model\n",
      "Epoch 220\n",
      "Training Epoch: 225, [0/560, 86%], loss is 0.145395\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 225, [70/560, 77%], loss is 0.145353\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 225, [140/560, 80%], loss is 0.143890\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 225, [210/560, 86%], loss is 0.085281\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 225, [280/560, 79%], loss is 0.131800\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 225, [350/560, 90%], loss is 0.101639\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 225, [420/560, 86%], loss is 0.077588\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 225, [490/560, 83%], loss is 0.207025\n",
      "best model\n",
      "Epoch 225\n",
      "Training Epoch: 230, [0/560, 79%], loss is 0.185784\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 230, [70/560, 86%], loss is 0.101581\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 230, [140/560, 83%], loss is 0.165286\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 230, [210/560, 80%], loss is 0.141792\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 230, [280/560, 90%], loss is 0.105222\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 230, [350/560, 79%], loss is 0.150014\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 230, [420/560, 91%], loss is 0.090951\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 230, [490/560, 86%], loss is 0.105572\n",
      "best model\n",
      "Epoch 230\n",
      "Training Epoch: 235, [0/560, 77%], loss is 0.122624\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 235, [70/560, 77%], loss is 0.134923\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 235, [140/560, 83%], loss is 0.110368\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 235, [210/560, 80%], loss is 0.104379\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 235, [280/560, 87%], loss is 0.082351\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 235, [350/560, 80%], loss is 0.115170\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 235, [420/560, 81%], loss is 0.087498\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 235, [490/560, 79%], loss is 0.130073\n",
      "best model\n",
      "Epoch 235\n",
      "Training Epoch: 240, [0/560, 79%], loss is 0.123339\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 240, [70/560, 76%], loss is 0.145932\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 240, [140/560, 77%], loss is 0.139610\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 240, [210/560, 83%], loss is 0.075288\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 240, [280/560, 81%], loss is 0.119187\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 240, [350/560, 76%], loss is 0.120024\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 240, [420/560, 79%], loss is 0.206397\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 240, [490/560, 84%], loss is 0.101837\n",
      "best model\n",
      "Epoch 240\n",
      "Training Epoch: 245, [0/560, 81%], loss is 0.092192\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 245, [70/560, 77%], loss is 0.136538\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 245, [140/560, 80%], loss is 0.150158\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 245, [210/560, 79%], loss is 0.108333\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 245, [280/560, 84%], loss is 0.085398\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 245, [350/560, 80%], loss is 0.130109\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 245, [420/560, 86%], loss is 0.127890\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 245, [490/560, 80%], loss is 0.123053\n",
      "best model\n",
      "Epoch 245\n",
      "Training Epoch: 250, [0/560, 86%], loss is 0.091241\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 250, [70/560, 77%], loss is 0.107582\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 250, [140/560, 79%], loss is 0.102046\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 250, [210/560, 74%], loss is 0.130551\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 250, [280/560, 77%], loss is 0.134921\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 250, [350/560, 77%], loss is 0.133545\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 250, [420/560, 81%], loss is 0.121715\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 250, [490/560, 80%], loss is 0.171891\n",
      "best model\n",
      "Epoch 250\n",
      "Training Epoch: 255, [0/560, 77%], loss is 0.109527\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 255, [70/560, 86%], loss is 0.102080\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 255, [140/560, 81%], loss is 0.130131\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 255, [210/560, 83%], loss is 0.114610\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 255, [280/560, 77%], loss is 0.147089\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 255, [350/560, 69%], loss is 0.190347\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 255, [420/560, 81%], loss is 0.152063\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 255, [490/560, 80%], loss is 0.157989\n",
      "best model\n",
      "Epoch 255\n",
      "Training Epoch: 260, [0/560, 84%], loss is 0.114192\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 260, [70/560, 80%], loss is 0.111745\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 260, [140/560, 86%], loss is 0.067097\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 260, [210/560, 80%], loss is 0.096333\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 260, [280/560, 79%], loss is 0.102042\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 260, [350/560, 91%], loss is 0.064432\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 260, [420/560, 79%], loss is 0.103042\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 260, [490/560, 81%], loss is 0.081128\n",
      "best model\n",
      "Epoch 260\n",
      "Training Epoch: 265, [0/560, 93%], loss is 0.053836\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 265, [70/560, 81%], loss is 0.142049\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 265, [140/560, 83%], loss is 0.129920\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 265, [210/560, 84%], loss is 0.092725\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 265, [280/560, 91%], loss is 0.050698\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 265, [350/560, 86%], loss is 0.089965\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 265, [420/560, 74%], loss is 0.203111\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 265, [490/560, 89%], loss is 0.084235\n",
      "best model\n",
      "Epoch 265\n",
      "Training Epoch: 270, [0/560, 71%], loss is 0.490453\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 270, [70/560, 80%], loss is 0.147167\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 270, [140/560, 81%], loss is 0.471611\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 270, [210/560, 70%], loss is 0.531560\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 270, [280/560, 77%], loss is 0.375827\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 270, [350/560, 86%], loss is 0.367468\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 270, [420/560, 86%], loss is 0.287851\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 270, [490/560, 80%], loss is 0.710874\n",
      "best model\n",
      "Epoch 270\n",
      "Training Epoch: 275, [0/560, 80%], loss is 0.171998\n",
      "best model\n",
      "Epoch 275\n",
      "Training Epoch: 275, [70/560, 81%], loss is 0.151800\n",
      "best model\n",
      "Epoch 275\n",
      "Training Epoch: 275, [140/560, 71%], loss is 0.165904\n",
      "best model\n",
      "Epoch 275\n",
      "Training Epoch: 275, [210/560, 71%], loss is 0.210370\n",
      "best model\n",
      "Epoch 275\n",
      "Training Epoch: 275, [280/560, 73%], loss is 0.254160\n",
      "best model\n",
      "Epoch 275\n",
      "Training Epoch: 275, [350/560, 74%], loss is 0.237558\n",
      "best model\n",
      "Epoch 275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 275, [420/560, 71%], loss is 0.192642\n",
      "best model\n",
      "Epoch 275\n",
      "Training Epoch: 275, [490/560, 86%], loss is 0.181513\n",
      "best model\n",
      "Epoch 275\n",
      "Training Epoch: 280, [0/560, 80%], loss is 0.223587\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 280, [70/560, 79%], loss is 0.153567\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 280, [140/560, 87%], loss is 0.157901\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 280, [210/560, 90%], loss is 0.130597\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 280, [280/560, 76%], loss is 0.136068\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 280, [350/560, 70%], loss is 0.185207\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 280, [420/560, 81%], loss is 0.134761\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 280, [490/560, 83%], loss is 0.130220\n",
      "best model\n",
      "Epoch 280\n",
      "Training Epoch: 285, [0/560, 76%], loss is 0.152594\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 285, [70/560, 80%], loss is 0.170305\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 285, [140/560, 84%], loss is 0.164081\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 285, [210/560, 77%], loss is 0.254701\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 285, [280/560, 80%], loss is 0.120622\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 285, [350/560, 74%], loss is 0.297095\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 285, [420/560, 76%], loss is 0.794040\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 285, [490/560, 81%], loss is 0.358502\n",
      "best model\n",
      "Epoch 285\n",
      "Training Epoch: 290, [0/560, 83%], loss is 0.172502\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 290, [70/560, 76%], loss is 0.171977\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 290, [140/560, 76%], loss is 0.249748\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 290, [210/560, 76%], loss is 0.151416\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 290, [280/560, 84%], loss is 0.128548\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 290, [350/560, 87%], loss is 0.102691\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 290, [420/560, 80%], loss is 0.136844\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 290, [490/560, 81%], loss is 0.129886\n",
      "best model\n",
      "Epoch 290\n",
      "Training Epoch: 295, [0/560, 76%], loss is 0.448733\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 295, [70/560, 80%], loss is 0.136620\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 295, [140/560, 77%], loss is 0.532401\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 295, [210/560, 77%], loss is 0.474883\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 295, [280/560, 80%], loss is 0.132194\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 295, [350/560, 77%], loss is 0.227855\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 295, [420/560, 81%], loss is 0.199838\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 295, [490/560, 81%], loss is 0.124609\n",
      "best model\n",
      "Epoch 295\n",
      "Training Epoch: 300, [0/560, 74%], loss is 0.204034\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 300, [70/560, 71%], loss is 0.386771\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 300, [140/560, 84%], loss is 0.227006\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 300, [210/560, 84%], loss is 0.144879\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 300, [280/560, 86%], loss is 0.061281\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 300, [350/560, 77%], loss is 0.296773\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 300, [420/560, 80%], loss is 0.205871\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 300, [490/560, 76%], loss is 0.234290\n",
      "best model\n",
      "Epoch 300\n",
      "Training Epoch: 305, [0/560, 77%], loss is 0.226693\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 305, [70/560, 84%], loss is 0.110967\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 305, [140/560, 76%], loss is 0.159732\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 305, [210/560, 77%], loss is 0.159007\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 305, [280/560, 87%], loss is 0.077994\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 305, [350/560, 83%], loss is 0.096635\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 305, [420/560, 79%], loss is 0.122883\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 305, [490/560, 79%], loss is 0.144735\n",
      "best model\n",
      "Epoch 305\n",
      "Training Epoch: 310, [0/560, 76%], loss is 0.173772\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 310, [70/560, 76%], loss is 0.169639\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 310, [140/560, 77%], loss is 0.144930\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 310, [210/560, 83%], loss is 0.135059\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 310, [280/560, 80%], loss is 0.172258\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 310, [350/560, 79%], loss is 0.120476\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 310, [420/560, 83%], loss is 0.101934\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 310, [490/560, 86%], loss is 0.121264\n",
      "best model\n",
      "Epoch 310\n",
      "Training Epoch: 315, [0/560, 80%], loss is 0.245903\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 315, [70/560, 81%], loss is 0.095362\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 315, [140/560, 81%], loss is 0.110780\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 315, [210/560, 77%], loss is 0.111242\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 315, [280/560, 74%], loss is 0.128022\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 315, [350/560, 76%], loss is 0.181544\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 315, [420/560, 83%], loss is 0.106192\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 315, [490/560, 79%], loss is 0.131855\n",
      "best model\n",
      "Epoch 315\n",
      "Training Epoch: 320, [0/560, 81%], loss is 0.150193\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 320, [70/560, 87%], loss is 0.076936\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 320, [140/560, 83%], loss is 0.105464\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 320, [210/560, 84%], loss is 0.089696\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 320, [280/560, 87%], loss is 0.137592\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 320, [350/560, 80%], loss is 0.103738\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 320, [420/560, 89%], loss is 0.106385\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 320, [490/560, 83%], loss is 0.100575\n",
      "best model\n",
      "Epoch 320\n",
      "Training Epoch: 325, [0/560, 81%], loss is 0.123071\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 325, [70/560, 89%], loss is 0.078924\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 325, [140/560, 90%], loss is 0.046781\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 325, [210/560, 80%], loss is 0.103324\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 325, [280/560, 84%], loss is 0.123597\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 325, [350/560, 84%], loss is 0.076525\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 325, [420/560, 89%], loss is 0.122661\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 325, [490/560, 81%], loss is 0.112494\n",
      "best model\n",
      "Epoch 325\n",
      "Training Epoch: 330, [0/560, 84%], loss is 0.084913\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 330, [70/560, 89%], loss is 0.055284\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 330, [140/560, 89%], loss is 0.053645\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 330, [210/560, 81%], loss is 0.126515\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 330, [280/560, 81%], loss is 0.091603\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 330, [350/560, 89%], loss is 0.103094\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 330, [420/560, 83%], loss is 0.096052\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 330, [490/560, 74%], loss is 0.157615\n",
      "best model\n",
      "Epoch 330\n",
      "Training Epoch: 335, [0/560, 87%], loss is 0.096177\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 335, [70/560, 80%], loss is 0.116815\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 335, [140/560, 81%], loss is 0.108999\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 335, [210/560, 81%], loss is 0.094222\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 335, [280/560, 80%], loss is 0.098340\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 335, [350/560, 81%], loss is 0.128170\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 335, [420/560, 83%], loss is 0.104319\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 335, [490/560, 81%], loss is 0.101191\n",
      "best model\n",
      "Epoch 335\n",
      "Training Epoch: 340, [0/560, 84%], loss is 0.087000\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 340, [70/560, 81%], loss is 0.121906\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 340, [140/560, 80%], loss is 0.130575\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 340, [210/560, 83%], loss is 0.100479\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 340, [280/560, 84%], loss is 0.089498\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 340, [350/560, 83%], loss is 0.099212\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 340, [420/560, 87%], loss is 0.100961\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 340, [490/560, 77%], loss is 0.159511\n",
      "best model\n",
      "Epoch 340\n",
      "Training Epoch: 345, [0/560, 81%], loss is 0.148964\n",
      "best model\n",
      "Epoch 345\n",
      "Training Epoch: 345, [70/560, 80%], loss is 0.178193\n",
      "best model\n",
      "Epoch 345\n",
      "Training Epoch: 345, [140/560, 84%], loss is 0.110125\n",
      "best model\n",
      "Epoch 345\n",
      "Training Epoch: 345, [210/560, 89%], loss is 0.068089\n",
      "best model\n",
      "Epoch 345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 345, [280/560, 77%], loss is 0.131591\n",
      "best model\n",
      "Epoch 345\n",
      "Training Epoch: 345, [350/560, 79%], loss is 0.137680\n",
      "best model\n",
      "Epoch 345\n",
      "Training Epoch: 345, [420/560, 87%], loss is 0.090176\n",
      "best model\n",
      "Epoch 345\n",
      "Training Epoch: 345, [490/560, 90%], loss is 0.096282\n",
      "best model\n",
      "Epoch 345\n",
      "Training Epoch: 350, [0/560, 90%], loss is 0.047422\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 350, [70/560, 81%], loss is 0.079254\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 350, [140/560, 79%], loss is 0.147289\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 350, [210/560, 83%], loss is 0.094695\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 350, [280/560, 89%], loss is 0.079848\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 350, [350/560, 70%], loss is 0.168029\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 350, [420/560, 81%], loss is 0.106288\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 350, [490/560, 83%], loss is 0.128300\n",
      "best model\n",
      "Epoch 350\n",
      "Training Epoch: 355, [0/560, 94%], loss is 0.056262\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 355, [70/560, 76%], loss is 0.124436\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 355, [140/560, 76%], loss is 0.132934\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 355, [210/560, 81%], loss is 0.077233\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 355, [280/560, 87%], loss is 0.069386\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 355, [350/560, 86%], loss is 0.127695\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 355, [420/560, 87%], loss is 0.085816\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 355, [490/560, 79%], loss is 0.133716\n",
      "best model\n",
      "Epoch 355\n",
      "Training Epoch: 360, [0/560, 84%], loss is 0.064423\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 360, [70/560, 81%], loss is 0.083882\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 360, [140/560, 76%], loss is 0.141669\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 360, [210/560, 86%], loss is 0.064227\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 360, [280/560, 80%], loss is 0.088507\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 360, [350/560, 87%], loss is 0.082724\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 360, [420/560, 83%], loss is 0.098908\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 360, [490/560, 87%], loss is 0.158552\n",
      "best model\n",
      "Epoch 360\n",
      "Training Epoch: 365, [0/560, 83%], loss is 0.154315\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 365, [70/560, 84%], loss is 0.214657\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 365, [140/560, 79%], loss is 0.140120\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 365, [210/560, 80%], loss is 0.237842\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 365, [280/560, 76%], loss is 0.183079\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 365, [350/560, 87%], loss is 0.086665\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 365, [420/560, 81%], loss is 0.133601\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 365, [490/560, 81%], loss is 0.228834\n",
      "best model\n",
      "Epoch 365\n",
      "Training Epoch: 370, [0/560, 81%], loss is 0.107363\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 370, [70/560, 81%], loss is 0.166784\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 370, [140/560, 79%], loss is 0.108406\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 370, [210/560, 86%], loss is 0.117267\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 370, [280/560, 83%], loss is 0.130447\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 370, [350/560, 84%], loss is 0.068836\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 370, [420/560, 76%], loss is 0.129636\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 370, [490/560, 76%], loss is 0.174637\n",
      "best model\n",
      "Epoch 370\n",
      "Training Epoch: 375, [0/560, 81%], loss is 0.227722\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 375, [70/560, 86%], loss is 0.095873\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 375, [140/560, 84%], loss is 0.144856\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 375, [210/560, 86%], loss is 0.108922\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 375, [280/560, 81%], loss is 0.166450\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 375, [350/560, 77%], loss is 0.196331\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 375, [420/560, 84%], loss is 0.164294\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 375, [490/560, 71%], loss is 0.170900\n",
      "best model\n",
      "Epoch 375\n",
      "Training Epoch: 380, [0/560, 84%], loss is 0.085478\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 380, [70/560, 84%], loss is 0.098051\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 380, [140/560, 87%], loss is 0.087826\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 380, [210/560, 86%], loss is 0.095320\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 380, [280/560, 74%], loss is 0.145009\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 380, [350/560, 86%], loss is 0.073548\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 380, [420/560, 84%], loss is 0.076949\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 380, [490/560, 83%], loss is 0.118789\n",
      "best model\n",
      "Epoch 380\n",
      "Training Epoch: 385, [0/560, 86%], loss is 0.098755\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 385, [70/560, 80%], loss is 0.087941\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 385, [140/560, 89%], loss is 0.079551\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 385, [210/560, 83%], loss is 0.105815\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 385, [280/560, 83%], loss is 0.094297\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 385, [350/560, 81%], loss is 0.094701\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 385, [420/560, 87%], loss is 0.076002\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 385, [490/560, 81%], loss is 0.110982\n",
      "best model\n",
      "Epoch 385\n",
      "Training Epoch: 390, [0/560, 83%], loss is 0.129694\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 390, [70/560, 81%], loss is 0.106577\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 390, [140/560, 81%], loss is 0.117965\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 390, [210/560, 86%], loss is 0.092908\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 390, [280/560, 87%], loss is 0.053095\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 390, [350/560, 83%], loss is 0.131417\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 390, [420/560, 77%], loss is 0.130385\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 390, [490/560, 86%], loss is 0.076232\n",
      "best model\n",
      "Epoch 390\n",
      "Training Epoch: 395, [0/560, 86%], loss is 0.092324\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 395, [70/560, 89%], loss is 0.075455\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 395, [140/560, 84%], loss is 0.097535\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 395, [210/560, 83%], loss is 0.129668\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 395, [280/560, 84%], loss is 0.125315\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 395, [350/560, 84%], loss is 0.099390\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 395, [420/560, 83%], loss is 0.108792\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 395, [490/560, 86%], loss is 0.065307\n",
      "best model\n",
      "Epoch 395\n",
      "Training Epoch: 400, [0/560, 90%], loss is 0.047126\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 400, [70/560, 86%], loss is 0.078179\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 400, [140/560, 87%], loss is 0.093444\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 400, [210/560, 83%], loss is 0.113073\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 400, [280/560, 86%], loss is 0.046429\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 400, [350/560, 81%], loss is 0.120327\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 400, [420/560, 87%], loss is 0.097173\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 400, [490/560, 87%], loss is 0.067292\n",
      "best model\n",
      "Epoch 400\n",
      "Training Epoch: 405, [0/560, 91%], loss is 0.043631\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 405, [70/560, 77%], loss is 0.139949\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 405, [140/560, 74%], loss is 0.210145\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 405, [210/560, 89%], loss is 0.065555\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 405, [280/560, 83%], loss is 0.116442\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 405, [350/560, 84%], loss is 0.114112\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 405, [420/560, 84%], loss is 0.084220\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 405, [490/560, 81%], loss is 0.090716\n",
      "best model\n",
      "Epoch 405\n",
      "Training Epoch: 410, [0/560, 80%], loss is 0.131161\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 410, [70/560, 79%], loss is 0.118328\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 410, [140/560, 89%], loss is 0.076177\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 410, [210/560, 86%], loss is 0.083595\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 410, [280/560, 86%], loss is 0.067904\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 410, [350/560, 86%], loss is 0.079260\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 410, [420/560, 84%], loss is 0.088446\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 410, [490/560, 80%], loss is 0.119020\n",
      "best model\n",
      "Epoch 410\n",
      "Training Epoch: 415, [0/560, 76%], loss is 0.263356\n",
      "best model\n",
      "Epoch 415\n",
      "Training Epoch: 415, [70/560, 79%], loss is 0.145761\n",
      "best model\n",
      "Epoch 415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 415, [140/560, 83%], loss is 0.111430\n",
      "best model\n",
      "Epoch 415\n",
      "Training Epoch: 415, [210/560, 87%], loss is 0.085534\n",
      "best model\n",
      "Epoch 415\n",
      "Training Epoch: 415, [280/560, 86%], loss is 0.114809\n",
      "best model\n",
      "Epoch 415\n",
      "Training Epoch: 415, [350/560, 89%], loss is 0.094941\n",
      "best model\n",
      "Epoch 415\n",
      "Training Epoch: 415, [420/560, 89%], loss is 0.101312\n",
      "best model\n",
      "Epoch 415\n",
      "Training Epoch: 415, [490/560, 94%], loss is 0.072773\n",
      "best model\n",
      "Epoch 415\n",
      "Training Epoch: 420, [0/560, 77%], loss is 0.132789\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 420, [70/560, 87%], loss is 0.072839\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 420, [140/560, 86%], loss is 0.068126\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 420, [210/560, 76%], loss is 0.142395\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 420, [280/560, 80%], loss is 0.098365\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 420, [350/560, 93%], loss is 0.043272\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 420, [420/560, 77%], loss is 0.105637\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 420, [490/560, 83%], loss is 0.118720\n",
      "best model\n",
      "Epoch 420\n",
      "Training Epoch: 425, [0/560, 81%], loss is 0.108639\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 425, [70/560, 89%], loss is 0.080021\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 425, [140/560, 76%], loss is 0.167116\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 425, [210/560, 71%], loss is 0.160344\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 425, [280/560, 80%], loss is 0.110270\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 425, [350/560, 79%], loss is 0.158337\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 425, [420/560, 84%], loss is 0.106159\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 425, [490/560, 89%], loss is 0.073907\n",
      "best model\n",
      "Epoch 425\n",
      "Training Epoch: 430, [0/560, 84%], loss is 0.085984\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 430, [70/560, 89%], loss is 0.053199\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 430, [140/560, 83%], loss is 0.118851\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 430, [210/560, 89%], loss is 0.062174\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 430, [280/560, 84%], loss is 0.079270\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 430, [350/560, 87%], loss is 0.054834\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 430, [420/560, 84%], loss is 0.098044\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 430, [490/560, 86%], loss is 0.081317\n",
      "best model\n",
      "Epoch 430\n",
      "Training Epoch: 435, [0/560, 90%], loss is 0.083577\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 435, [70/560, 80%], loss is 0.098821\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 435, [140/560, 80%], loss is 0.094180\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 435, [210/560, 89%], loss is 0.058972\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 435, [280/560, 86%], loss is 0.078817\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 435, [350/560, 86%], loss is 0.072583\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 435, [420/560, 86%], loss is 0.084262\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 435, [490/560, 80%], loss is 0.104364\n",
      "best model\n",
      "Epoch 435\n",
      "Training Epoch: 440, [0/560, 80%], loss is 0.146981\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 440, [70/560, 74%], loss is 0.254445\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 440, [140/560, 81%], loss is 0.113801\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 440, [210/560, 76%], loss is 0.339369\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 440, [280/560, 81%], loss is 0.179324\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 440, [350/560, 79%], loss is 0.287948\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 440, [420/560, 86%], loss is 0.133056\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 440, [490/560, 70%], loss is 0.270449\n",
      "best model\n",
      "Epoch 440\n",
      "Training Epoch: 445, [0/560, 86%], loss is 0.264797\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 445, [70/560, 80%], loss is 0.205403\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 445, [140/560, 77%], loss is 0.246645\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 445, [210/560, 81%], loss is 0.146467\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 445, [280/560, 84%], loss is 0.225039\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 445, [350/560, 84%], loss is 0.206380\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 445, [420/560, 80%], loss is 0.317960\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 445, [490/560, 74%], loss is 0.329354\n",
      "best model\n",
      "Epoch 445\n",
      "Training Epoch: 450, [0/560, 71%], loss is 0.151499\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 450, [70/560, 74%], loss is 0.270139\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 450, [140/560, 86%], loss is 0.131161\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 450, [210/560, 79%], loss is 0.143992\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 450, [280/560, 84%], loss is 0.129555\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 450, [350/560, 81%], loss is 0.117454\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 450, [420/560, 81%], loss is 0.120632\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 450, [490/560, 84%], loss is 0.116214\n",
      "best model\n",
      "Epoch 450\n",
      "Training Epoch: 455, [0/560, 86%], loss is 0.118459\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 455, [70/560, 83%], loss is 0.082216\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 455, [140/560, 80%], loss is 0.123582\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 455, [210/560, 86%], loss is 0.099253\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 455, [280/560, 81%], loss is 0.091743\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 455, [350/560, 79%], loss is 0.182245\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 455, [420/560, 84%], loss is 0.087586\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 455, [490/560, 77%], loss is 0.151306\n",
      "best model\n",
      "Epoch 455\n",
      "Training Epoch: 460, [0/560, 91%], loss is 0.082217\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 460, [70/560, 86%], loss is 0.098444\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 460, [140/560, 74%], loss is 0.143898\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 460, [210/560, 87%], loss is 0.120686\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 460, [280/560, 80%], loss is 0.155500\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 460, [350/560, 86%], loss is 0.102013\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 460, [420/560, 79%], loss is 0.092637\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 460, [490/560, 80%], loss is 0.199148\n",
      "best model\n",
      "Epoch 460\n",
      "Training Epoch: 465, [0/560, 86%], loss is 0.089743\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 465, [70/560, 90%], loss is 0.072742\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 465, [140/560, 87%], loss is 0.076110\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 465, [210/560, 83%], loss is 0.097017\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 465, [280/560, 91%], loss is 0.073269\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 465, [350/560, 90%], loss is 0.066002\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 465, [420/560, 89%], loss is 0.073358\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 465, [490/560, 80%], loss is 0.110637\n",
      "best model\n",
      "Epoch 465\n",
      "Training Epoch: 470, [0/560, 83%], loss is 0.097617\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 470, [70/560, 77%], loss is 0.109206\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 470, [140/560, 81%], loss is 0.112465\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 470, [210/560, 81%], loss is 0.116794\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 470, [280/560, 87%], loss is 0.094078\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 470, [350/560, 93%], loss is 0.059741\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 470, [420/560, 86%], loss is 0.063781\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 470, [490/560, 86%], loss is 0.096860\n",
      "best model\n",
      "Epoch 470\n",
      "Training Epoch: 475, [0/560, 90%], loss is 0.085613\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 475, [70/560, 83%], loss is 0.116524\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 475, [140/560, 90%], loss is 0.058507\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 475, [210/560, 80%], loss is 0.117066\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 475, [280/560, 83%], loss is 0.059489\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 475, [350/560, 86%], loss is 0.073893\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 475, [420/560, 96%], loss is 0.040573\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 475, [490/560, 81%], loss is 0.136762\n",
      "best model\n",
      "Epoch 475\n",
      "Training Epoch: 480, [0/560, 91%], loss is 0.057926\n",
      "best model\n",
      "Epoch 480\n",
      "Training Epoch: 480, [70/560, 83%], loss is 0.062584\n",
      "best model\n",
      "Epoch 480\n",
      "Training Epoch: 480, [140/560, 91%], loss is 0.053437\n",
      "best model\n",
      "Epoch 480\n",
      "Training Epoch: 480, [210/560, 87%], loss is 0.074339\n",
      "best model\n",
      "Epoch 480\n",
      "Training Epoch: 480, [280/560, 84%], loss is 0.065122\n",
      "best model\n",
      "Epoch 480\n",
      "Training Epoch: 480, [350/560, 89%], loss is 0.070498\n",
      "best model\n",
      "Epoch 480\n",
      "Training Epoch: 480, [420/560, 87%], loss is 0.073865\n",
      "best model\n",
      "Epoch 480\n",
      "Training Epoch: 480, [490/560, 90%], loss is 0.089875\n",
      "best model\n",
      "Epoch 480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 485, [0/560, 74%], loss is 0.147120\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 485, [70/560, 79%], loss is 0.143597\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 485, [140/560, 84%], loss is 0.092734\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 485, [210/560, 87%], loss is 0.072057\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 485, [280/560, 90%], loss is 0.075002\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 485, [350/560, 83%], loss is 0.154610\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 485, [420/560, 89%], loss is 0.070776\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 485, [490/560, 77%], loss is 0.151378\n",
      "best model\n",
      "Epoch 485\n",
      "Training Epoch: 490, [0/560, 77%], loss is 0.141881\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 490, [70/560, 83%], loss is 0.123471\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 490, [140/560, 80%], loss is 0.103623\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 490, [210/560, 84%], loss is 0.095509\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 490, [280/560, 89%], loss is 0.069425\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 490, [350/560, 83%], loss is 0.095237\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 490, [420/560, 89%], loss is 0.102261\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 490, [490/560, 89%], loss is 0.086321\n",
      "best model\n",
      "Epoch 490\n",
      "Training Epoch: 495, [0/560, 84%], loss is 0.112540\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 495, [70/560, 80%], loss is 0.233626\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 495, [140/560, 80%], loss is 0.277970\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 495, [210/560, 81%], loss is 0.141429\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 495, [280/560, 80%], loss is 0.113768\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 495, [350/560, 71%], loss is 0.315827\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 495, [420/560, 77%], loss is 0.226253\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 495, [490/560, 77%], loss is 0.144305\n",
      "best model\n",
      "Epoch 495\n",
      "Training Epoch: 500, [0/560, 81%], loss is 0.145132\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 500, [70/560, 73%], loss is 0.259563\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 500, [140/560, 84%], loss is 0.309185\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 500, [210/560, 83%], loss is 0.088636\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 500, [280/560, 77%], loss is 0.146994\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 500, [350/560, 87%], loss is 0.150126\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 500, [420/560, 81%], loss is 0.249872\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 500, [490/560, 79%], loss is 0.197820\n",
      "best model\n",
      "Epoch 500\n",
      "Training Epoch: 505, [0/560, 84%], loss is 0.131198\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 505, [70/560, 80%], loss is 0.142087\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 505, [140/560, 91%], loss is 0.072822\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 505, [210/560, 79%], loss is 0.162300\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 505, [280/560, 83%], loss is 0.123650\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 505, [350/560, 90%], loss is 0.091267\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 505, [420/560, 83%], loss is 0.185035\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 505, [490/560, 94%], loss is 0.076659\n",
      "best model\n",
      "Epoch 505\n",
      "Training Epoch: 510, [0/560, 90%], loss is 0.089563\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 510, [70/560, 80%], loss is 0.116952\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 510, [140/560, 84%], loss is 0.078049\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 510, [210/560, 90%], loss is 0.049976\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 510, [280/560, 81%], loss is 0.106087\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 510, [350/560, 87%], loss is 0.083434\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 510, [420/560, 77%], loss is 0.110776\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 510, [490/560, 84%], loss is 0.100414\n",
      "best model\n",
      "Epoch 510\n",
      "Training Epoch: 515, [0/560, 79%], loss is 0.119368\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 515, [70/560, 86%], loss is 0.089208\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 515, [140/560, 83%], loss is 0.110447\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 515, [210/560, 80%], loss is 0.080170\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 515, [280/560, 87%], loss is 0.082252\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 515, [350/560, 84%], loss is 0.072917\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 515, [420/560, 86%], loss is 0.091659\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 515, [490/560, 80%], loss is 0.119709\n",
      "best model\n",
      "Epoch 515\n",
      "Training Epoch: 520, [0/560, 74%], loss is 0.124173\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 520, [70/560, 84%], loss is 0.147410\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 520, [140/560, 87%], loss is 0.091685\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 520, [210/560, 90%], loss is 0.066403\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 520, [280/560, 84%], loss is 0.098936\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 520, [350/560, 89%], loss is 0.077523\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 520, [420/560, 84%], loss is 0.094994\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 520, [490/560, 81%], loss is 0.059283\n",
      "best model\n",
      "Epoch 520\n",
      "Training Epoch: 525, [0/560, 81%], loss is 0.091548\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 525, [70/560, 90%], loss is 0.078468\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 525, [140/560, 83%], loss is 0.083791\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 525, [210/560, 86%], loss is 0.069335\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 525, [280/560, 87%], loss is 0.084754\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 525, [350/560, 86%], loss is 0.066217\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 525, [420/560, 84%], loss is 0.073647\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 525, [490/560, 91%], loss is 0.077107\n",
      "best model\n",
      "Epoch 525\n",
      "Training Epoch: 530, [0/560, 84%], loss is 0.076072\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 530, [70/560, 93%], loss is 0.056044\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 530, [140/560, 90%], loss is 0.057675\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 530, [210/560, 86%], loss is 0.060547\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 530, [280/560, 86%], loss is 0.058181\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 530, [350/560, 90%], loss is 0.071509\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 530, [420/560, 87%], loss is 0.090803\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 530, [490/560, 86%], loss is 0.077322\n",
      "best model\n",
      "Epoch 530\n",
      "Training Epoch: 535, [0/560, 86%], loss is 0.079313\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 535, [70/560, 99%], loss is 0.031938\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 535, [140/560, 89%], loss is 0.092381\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 535, [210/560, 94%], loss is 0.075717\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 535, [280/560, 84%], loss is 0.113446\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 535, [350/560, 80%], loss is 0.148495\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 535, [420/560, 91%], loss is 0.062539\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 535, [490/560, 86%], loss is 0.067479\n",
      "best model\n",
      "Epoch 535\n",
      "Training Epoch: 540, [0/560, 77%], loss is 0.165425\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 540, [70/560, 86%], loss is 0.079099\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 540, [140/560, 91%], loss is 0.033634\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 540, [210/560, 80%], loss is 0.098446\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 540, [280/560, 84%], loss is 0.113683\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 540, [350/560, 86%], loss is 0.085569\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 540, [420/560, 81%], loss is 0.096259\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 540, [490/560, 77%], loss is 0.131767\n",
      "best model\n",
      "Epoch 540\n",
      "Training Epoch: 545, [0/560, 86%], loss is 0.088460\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 545, [70/560, 86%], loss is 0.085070\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 545, [140/560, 90%], loss is 0.055656\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 545, [210/560, 84%], loss is 0.084221\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 545, [280/560, 90%], loss is 0.069467\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 545, [350/560, 86%], loss is 0.080700\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 545, [420/560, 87%], loss is 0.066927\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 545, [490/560, 80%], loss is 0.126232\n",
      "best model\n",
      "Epoch 545\n",
      "Training Epoch: 550, [0/560, 86%], loss is 0.086485\n",
      "best model\n",
      "Epoch 550\n",
      "Training Epoch: 550, [70/560, 80%], loss is 0.080270\n",
      "best model\n",
      "Epoch 550\n",
      "Training Epoch: 550, [140/560, 89%], loss is 0.104764\n",
      "best model\n",
      "Epoch 550\n",
      "Training Epoch: 550, [210/560, 86%], loss is 0.065090\n",
      "best model\n",
      "Epoch 550\n",
      "Training Epoch: 550, [280/560, 81%], loss is 0.104790\n",
      "best model\n",
      "Epoch 550\n",
      "Training Epoch: 550, [350/560, 80%], loss is 0.171423\n",
      "best model\n",
      "Epoch 550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 550, [420/560, 83%], loss is 0.097235\n",
      "best model\n",
      "Epoch 550\n",
      "Training Epoch: 550, [490/560, 81%], loss is 0.109720\n",
      "best model\n",
      "Epoch 550\n",
      "Training Epoch: 555, [0/560, 66%], loss is 0.249863\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 555, [70/560, 76%], loss is 0.272769\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 555, [140/560, 81%], loss is 0.191606\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 555, [210/560, 80%], loss is 0.163736\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 555, [280/560, 74%], loss is 0.187688\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 555, [350/560, 70%], loss is 0.174503\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 555, [420/560, 76%], loss is 0.285307\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 555, [490/560, 80%], loss is 0.115100\n",
      "best model\n",
      "Epoch 555\n",
      "Training Epoch: 560, [0/560, 86%], loss is 0.064430\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 560, [70/560, 87%], loss is 0.097268\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 560, [140/560, 77%], loss is 0.227900\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 560, [210/560, 91%], loss is 0.081985\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 560, [280/560, 87%], loss is 0.109526\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 560, [350/560, 81%], loss is 0.123998\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 560, [420/560, 87%], loss is 0.120995\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 560, [490/560, 83%], loss is 0.079170\n",
      "best model\n",
      "Epoch 560\n",
      "Training Epoch: 565, [0/560, 87%], loss is 0.072760\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 565, [70/560, 79%], loss is 0.141055\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 565, [140/560, 84%], loss is 0.070475\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 565, [210/560, 89%], loss is 0.071706\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 565, [280/560, 90%], loss is 0.051550\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 565, [350/560, 89%], loss is 0.064150\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 565, [420/560, 86%], loss is 0.087848\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 565, [490/560, 89%], loss is 0.073570\n",
      "best model\n",
      "Epoch 565\n",
      "Training Epoch: 570, [0/560, 86%], loss is 0.108516\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 570, [70/560, 89%], loss is 0.084470\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 570, [140/560, 89%], loss is 0.051260\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 570, [210/560, 86%], loss is 0.083166\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 570, [280/560, 80%], loss is 0.094577\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 570, [350/560, 86%], loss is 0.081872\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 570, [420/560, 91%], loss is 0.041786\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 570, [490/560, 87%], loss is 0.067207\n",
      "best model\n",
      "Epoch 570\n",
      "Training Epoch: 575, [0/560, 76%], loss is 0.095757\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 575, [70/560, 91%], loss is 0.059581\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 575, [140/560, 87%], loss is 0.103679\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 575, [210/560, 81%], loss is 0.102467\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 575, [280/560, 84%], loss is 0.094748\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 575, [350/560, 87%], loss is 0.069019\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 575, [420/560, 89%], loss is 0.056481\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 575, [490/560, 86%], loss is 0.093236\n",
      "best model\n",
      "Epoch 575\n",
      "Training Epoch: 580, [0/560, 86%], loss is 0.087264\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 580, [70/560, 90%], loss is 0.080437\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 580, [140/560, 81%], loss is 0.094360\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 580, [210/560, 83%], loss is 0.075918\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 580, [280/560, 84%], loss is 0.094831\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 580, [350/560, 86%], loss is 0.086752\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 580, [420/560, 89%], loss is 0.070379\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 580, [490/560, 86%], loss is 0.081678\n",
      "best model\n",
      "Epoch 580\n",
      "Training Epoch: 585, [0/560, 89%], loss is 0.064789\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 585, [70/560, 89%], loss is 0.142918\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 585, [140/560, 94%], loss is 0.081285\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 585, [210/560, 79%], loss is 0.123447\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 585, [280/560, 81%], loss is 0.080055\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 585, [350/560, 87%], loss is 0.068647\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 585, [420/560, 84%], loss is 0.080624\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 585, [490/560, 93%], loss is 0.064372\n",
      "best model\n",
      "Epoch 585\n",
      "Training Epoch: 590, [0/560, 91%], loss is 0.046787\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 590, [70/560, 79%], loss is 0.106620\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 590, [140/560, 87%], loss is 0.068953\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 590, [210/560, 91%], loss is 0.048460\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 590, [280/560, 87%], loss is 0.077779\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 590, [350/560, 86%], loss is 0.093291\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 590, [420/560, 89%], loss is 0.088617\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 590, [490/560, 89%], loss is 0.059822\n",
      "best model\n",
      "Epoch 590\n",
      "Training Epoch: 595, [0/560, 89%], loss is 0.070753\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 595, [70/560, 86%], loss is 0.072313\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 595, [140/560, 84%], loss is 0.105852\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 595, [210/560, 89%], loss is 0.056420\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 595, [280/560, 83%], loss is 0.145932\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 595, [350/560, 83%], loss is 0.073348\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 595, [420/560, 81%], loss is 0.117020\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 595, [490/560, 79%], loss is 0.101619\n",
      "best model\n",
      "Epoch 595\n",
      "Training Epoch: 600, [0/560, 80%], loss is 0.092917\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 600, [70/560, 87%], loss is 0.085077\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 600, [140/560, 84%], loss is 0.085730\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 600, [210/560, 83%], loss is 0.118125\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 600, [280/560, 86%], loss is 0.090469\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 600, [350/560, 89%], loss is 0.078595\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 600, [420/560, 83%], loss is 0.072803\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 600, [490/560, 89%], loss is 0.068185\n",
      "best model\n",
      "Epoch 600\n",
      "Training Epoch: 605, [0/560, 84%], loss is 0.094522\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 605, [70/560, 79%], loss is 0.104029\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 605, [140/560, 86%], loss is 0.092681\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 605, [210/560, 84%], loss is 0.071703\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 605, [280/560, 89%], loss is 0.077094\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 605, [350/560, 89%], loss is 0.099524\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 605, [420/560, 91%], loss is 0.049822\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 605, [490/560, 93%], loss is 0.047858\n",
      "best model\n",
      "Epoch 605\n",
      "Training Epoch: 610, [0/560, 90%], loss is 0.051781\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 610, [70/560, 91%], loss is 0.047351\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 610, [140/560, 81%], loss is 0.081998\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 610, [210/560, 91%], loss is 0.037954\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 610, [280/560, 84%], loss is 0.081659\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 610, [350/560, 77%], loss is 0.123551\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 610, [420/560, 84%], loss is 0.121637\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 610, [490/560, 83%], loss is 0.077852\n",
      "best model\n",
      "Epoch 610\n",
      "Training Epoch: 615, [0/560, 90%], loss is 0.079836\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 615, [70/560, 87%], loss is 0.072142\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 615, [140/560, 86%], loss is 0.061436\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 615, [210/560, 90%], loss is 0.045723\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 615, [280/560, 83%], loss is 0.086606\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 615, [350/560, 91%], loss is 0.059616\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 615, [420/560, 83%], loss is 0.120513\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 615, [490/560, 86%], loss is 0.058933\n",
      "best model\n",
      "Epoch 615\n",
      "Training Epoch: 620, [0/560, 80%], loss is 0.115531\n",
      "best model\n",
      "Epoch 620\n",
      "Training Epoch: 620, [70/560, 87%], loss is 0.074819\n",
      "best model\n",
      "Epoch 620\n",
      "Training Epoch: 620, [140/560, 86%], loss is 0.066320\n",
      "best model\n",
      "Epoch 620\n",
      "Training Epoch: 620, [210/560, 86%], loss is 0.062417\n",
      "best model\n",
      "Epoch 620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 620, [280/560, 84%], loss is 0.068062\n",
      "best model\n",
      "Epoch 620\n",
      "Training Epoch: 620, [350/560, 93%], loss is 0.039375\n",
      "best model\n",
      "Epoch 620\n",
      "Training Epoch: 620, [420/560, 89%], loss is 0.059350\n",
      "best model\n",
      "Epoch 620\n",
      "Training Epoch: 620, [490/560, 89%], loss is 0.072316\n",
      "best model\n",
      "Epoch 620\n",
      "Training Epoch: 625, [0/560, 81%], loss is 0.091309\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 625, [70/560, 94%], loss is 0.036676\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 625, [140/560, 80%], loss is 0.120350\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 625, [210/560, 84%], loss is 0.061264\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 625, [280/560, 84%], loss is 0.084542\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 625, [350/560, 87%], loss is 0.054770\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 625, [420/560, 90%], loss is 0.075366\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 625, [490/560, 83%], loss is 0.094956\n",
      "best model\n",
      "Epoch 625\n",
      "Training Epoch: 630, [0/560, 89%], loss is 0.089574\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 630, [70/560, 84%], loss is 0.127955\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 630, [140/560, 80%], loss is 0.207114\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 630, [210/560, 84%], loss is 0.219976\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 630, [280/560, 86%], loss is 0.125064\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 630, [350/560, 80%], loss is 0.162991\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 630, [420/560, 84%], loss is 0.070569\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 630, [490/560, 89%], loss is 0.086717\n",
      "best model\n",
      "Epoch 630\n",
      "Training Epoch: 635, [0/560, 90%], loss is 0.139545\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 635, [70/560, 76%], loss is 0.149853\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 635, [140/560, 89%], loss is 0.093120\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 635, [210/560, 89%], loss is 0.084210\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 635, [280/560, 86%], loss is 0.143035\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 635, [350/560, 84%], loss is 0.187367\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 635, [420/560, 81%], loss is 0.208267\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 635, [490/560, 81%], loss is 0.120263\n",
      "best model\n",
      "Epoch 635\n",
      "Training Epoch: 640, [0/560, 70%], loss is 0.538295\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 640, [70/560, 76%], loss is 0.205917\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 640, [140/560, 77%], loss is 0.136276\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 640, [210/560, 89%], loss is 0.160424\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 640, [280/560, 79%], loss is 0.310124\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 640, [350/560, 81%], loss is 0.240185\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 640, [420/560, 81%], loss is 0.141709\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 640, [490/560, 76%], loss is 0.266963\n",
      "best model\n",
      "Epoch 640\n",
      "Training Epoch: 645, [0/560, 87%], loss is 0.082256\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 645, [70/560, 90%], loss is 0.068252\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 645, [140/560, 86%], loss is 0.085683\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 645, [210/560, 80%], loss is 0.116205\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 645, [280/560, 87%], loss is 0.086556\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 645, [350/560, 84%], loss is 0.093357\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 645, [420/560, 89%], loss is 0.096161\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 645, [490/560, 87%], loss is 0.090788\n",
      "best model\n",
      "Epoch 645\n",
      "Training Epoch: 650, [0/560, 89%], loss is 0.204840\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 650, [70/560, 89%], loss is 0.172030\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 650, [140/560, 89%], loss is 0.064512\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 650, [210/560, 83%], loss is 0.092234\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 650, [280/560, 81%], loss is 0.099070\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 650, [350/560, 76%], loss is 0.139481\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 650, [420/560, 94%], loss is 0.067482\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 650, [490/560, 80%], loss is 0.133071\n",
      "best model\n",
      "Epoch 650\n",
      "Training Epoch: 655, [0/560, 86%], loss is 0.070180\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 655, [70/560, 87%], loss is 0.065202\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 655, [140/560, 94%], loss is 0.060526\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 655, [210/560, 87%], loss is 0.072454\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 655, [280/560, 90%], loss is 0.048304\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 655, [350/560, 83%], loss is 0.088989\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 655, [420/560, 89%], loss is 0.075069\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 655, [490/560, 87%], loss is 0.082151\n",
      "best model\n",
      "Epoch 655\n",
      "Training Epoch: 660, [0/560, 79%], loss is 0.088802\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 660, [70/560, 90%], loss is 0.073023\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 660, [140/560, 84%], loss is 0.068846\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 660, [210/560, 90%], loss is 0.044706\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 660, [280/560, 90%], loss is 0.063792\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 660, [350/560, 87%], loss is 0.081055\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 660, [420/560, 91%], loss is 0.050912\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 660, [490/560, 90%], loss is 0.046725\n",
      "best model\n",
      "Epoch 660\n",
      "Training Epoch: 665, [0/560, 81%], loss is 0.093695\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 665, [70/560, 87%], loss is 0.066930\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 665, [140/560, 84%], loss is 0.092446\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 665, [210/560, 79%], loss is 0.095173\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 665, [280/560, 91%], loss is 0.052467\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 665, [350/560, 94%], loss is 0.041290\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 665, [420/560, 87%], loss is 0.099559\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 665, [490/560, 81%], loss is 0.107931\n",
      "best model\n",
      "Epoch 665\n",
      "Training Epoch: 670, [0/560, 86%], loss is 0.057299\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 670, [70/560, 81%], loss is 0.094411\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 670, [140/560, 81%], loss is 0.095822\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 670, [210/560, 87%], loss is 0.061470\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 670, [280/560, 94%], loss is 0.041770\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 670, [350/560, 84%], loss is 0.068688\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 670, [420/560, 91%], loss is 0.070843\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 670, [490/560, 86%], loss is 0.099139\n",
      "best model\n",
      "Epoch 670\n",
      "Training Epoch: 675, [0/560, 86%], loss is 0.072836\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 675, [70/560, 84%], loss is 0.108959\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 675, [140/560, 79%], loss is 0.137987\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 675, [210/560, 89%], loss is 0.055413\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 675, [280/560, 83%], loss is 0.075636\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 675, [350/560, 90%], loss is 0.070637\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 675, [420/560, 93%], loss is 0.041333\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 675, [490/560, 90%], loss is 0.120732\n",
      "best model\n",
      "Epoch 675\n",
      "Training Epoch: 680, [0/560, 87%], loss is 0.080755\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 680, [70/560, 83%], loss is 0.121371\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 680, [140/560, 81%], loss is 0.196077\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 680, [210/560, 79%], loss is 0.249579\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 680, [280/560, 84%], loss is 0.102307\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 680, [350/560, 84%], loss is 0.095274\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 680, [420/560, 86%], loss is 0.108435\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 680, [490/560, 84%], loss is 0.133183\n",
      "best model\n",
      "Epoch 680\n",
      "Training Epoch: 685, [0/560, 83%], loss is 0.166833\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 685, [70/560, 84%], loss is 0.179506\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 685, [140/560, 73%], loss is 0.256632\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 685, [210/560, 70%], loss is 0.416770\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 685, [280/560, 83%], loss is 0.321594\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 685, [350/560, 83%], loss is 0.291763\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 685, [420/560, 91%], loss is 0.077736\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 685, [490/560, 87%], loss is 0.164544\n",
      "best model\n",
      "Epoch 685\n",
      "Training Epoch: 690, [0/560, 87%], loss is 0.162859\n",
      "best model\n",
      "Epoch 690\n",
      "Training Epoch: 690, [70/560, 84%], loss is 0.090116\n",
      "best model\n",
      "Epoch 690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 690, [140/560, 86%], loss is 0.110666\n",
      "best model\n",
      "Epoch 690\n",
      "Training Epoch: 690, [210/560, 90%], loss is 0.118988\n",
      "best model\n",
      "Epoch 690\n",
      "Training Epoch: 690, [280/560, 84%], loss is 0.117466\n",
      "best model\n",
      "Epoch 690\n",
      "Training Epoch: 690, [350/560, 90%], loss is 0.083396\n",
      "best model\n",
      "Epoch 690\n",
      "Training Epoch: 690, [420/560, 81%], loss is 0.088174\n",
      "best model\n",
      "Epoch 690\n",
      "Training Epoch: 690, [490/560, 89%], loss is 0.081153\n",
      "best model\n",
      "Epoch 690\n",
      "Training Epoch: 695, [0/560, 77%], loss is 0.169575\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 695, [70/560, 80%], loss is 0.212066\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 695, [140/560, 80%], loss is 0.162996\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 695, [210/560, 84%], loss is 0.105601\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 695, [280/560, 84%], loss is 0.095636\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 695, [350/560, 83%], loss is 0.181768\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 695, [420/560, 80%], loss is 0.135021\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 695, [490/560, 83%], loss is 0.133455\n",
      "best model\n",
      "Epoch 695\n",
      "Training Epoch: 700, [0/560, 87%], loss is 0.074360\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 700, [70/560, 81%], loss is 0.114088\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 700, [140/560, 91%], loss is 0.054566\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 700, [210/560, 84%], loss is 0.098142\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 700, [280/560, 80%], loss is 0.082551\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 700, [350/560, 83%], loss is 0.127867\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 700, [420/560, 81%], loss is 0.101252\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 700, [490/560, 90%], loss is 0.068764\n",
      "best model\n",
      "Epoch 700\n",
      "Training Epoch: 705, [0/560, 89%], loss is 0.067702\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 705, [70/560, 91%], loss is 0.047762\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 705, [140/560, 90%], loss is 0.073820\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 705, [210/560, 90%], loss is 0.060233\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 705, [280/560, 91%], loss is 0.044858\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 705, [350/560, 81%], loss is 0.119983\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 705, [420/560, 84%], loss is 0.078007\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 705, [490/560, 86%], loss is 0.070243\n",
      "best model\n",
      "Epoch 705\n",
      "Training Epoch: 710, [0/560, 83%], loss is 0.074831\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 710, [70/560, 93%], loss is 0.053538\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 710, [140/560, 90%], loss is 0.076067\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 710, [210/560, 74%], loss is 0.157266\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 710, [280/560, 94%], loss is 0.059959\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 710, [350/560, 87%], loss is 0.070480\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 710, [420/560, 84%], loss is 0.047095\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 710, [490/560, 89%], loss is 0.051505\n",
      "best model\n",
      "Epoch 710\n",
      "Training Epoch: 715, [0/560, 83%], loss is 0.092800\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 715, [70/560, 83%], loss is 0.065728\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 715, [140/560, 94%], loss is 0.059436\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 715, [210/560, 90%], loss is 0.066468\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 715, [280/560, 87%], loss is 0.132265\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 715, [350/560, 91%], loss is 0.051285\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 715, [420/560, 89%], loss is 0.056588\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 715, [490/560, 81%], loss is 0.080839\n",
      "best model\n",
      "Epoch 715\n",
      "Training Epoch: 720, [0/560, 89%], loss is 0.043257\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 720, [70/560, 87%], loss is 0.070805\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 720, [140/560, 91%], loss is 0.045281\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 720, [210/560, 86%], loss is 0.075405\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 720, [280/560, 87%], loss is 0.072555\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 720, [350/560, 89%], loss is 0.059854\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 720, [420/560, 90%], loss is 0.080628\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 720, [490/560, 86%], loss is 0.062383\n",
      "best model\n",
      "Epoch 720\n",
      "Training Epoch: 725, [0/560, 86%], loss is 0.070461\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 725, [70/560, 91%], loss is 0.040279\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 725, [140/560, 91%], loss is 0.056076\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 725, [210/560, 90%], loss is 0.079975\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 725, [280/560, 76%], loss is 0.151809\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 725, [350/560, 80%], loss is 0.113018\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 725, [420/560, 86%], loss is 0.085373\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 725, [490/560, 79%], loss is 0.126474\n",
      "best model\n",
      "Epoch 725\n",
      "Training Epoch: 730, [0/560, 84%], loss is 0.093453\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 730, [70/560, 87%], loss is 0.058903\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 730, [140/560, 87%], loss is 0.052478\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 730, [210/560, 86%], loss is 0.073530\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 730, [280/560, 86%], loss is 0.095970\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 730, [350/560, 81%], loss is 0.116666\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 730, [420/560, 84%], loss is 0.073961\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 730, [490/560, 79%], loss is 0.104718\n",
      "best model\n",
      "Epoch 730\n",
      "Training Epoch: 735, [0/560, 84%], loss is 0.064286\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 735, [70/560, 90%], loss is 0.068461\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 735, [140/560, 93%], loss is 0.036160\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 735, [210/560, 91%], loss is 0.077975\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 735, [280/560, 84%], loss is 0.079893\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 735, [350/560, 81%], loss is 0.111529\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 735, [420/560, 84%], loss is 0.072973\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 735, [490/560, 89%], loss is 0.069391\n",
      "best model\n",
      "Epoch 735\n",
      "Training Epoch: 740, [0/560, 90%], loss is 0.068521\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 740, [70/560, 83%], loss is 0.067663\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 740, [140/560, 77%], loss is 0.129147\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 740, [210/560, 81%], loss is 0.099113\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 740, [280/560, 87%], loss is 0.076040\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 740, [350/560, 89%], loss is 0.054739\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 740, [420/560, 91%], loss is 0.052955\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 740, [490/560, 83%], loss is 0.080698\n",
      "best model\n",
      "Epoch 740\n",
      "Training Epoch: 745, [0/560, 90%], loss is 0.099237\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 745, [70/560, 84%], loss is 0.091528\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 745, [140/560, 91%], loss is 0.047774\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 745, [210/560, 90%], loss is 0.060753\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 745, [280/560, 87%], loss is 0.055272\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 745, [350/560, 89%], loss is 0.052512\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 745, [420/560, 87%], loss is 0.065735\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 745, [490/560, 90%], loss is 0.053646\n",
      "best model\n",
      "Epoch 745\n",
      "Training Epoch: 750, [0/560, 83%], loss is 0.097173\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 750, [70/560, 83%], loss is 0.077736\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 750, [140/560, 90%], loss is 0.066066\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 750, [210/560, 90%], loss is 0.059708\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 750, [280/560, 87%], loss is 0.084625\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 750, [350/560, 91%], loss is 0.044529\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 750, [420/560, 83%], loss is 0.113085\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 750, [490/560, 86%], loss is 0.065215\n",
      "best model\n",
      "Epoch 750\n",
      "Training Epoch: 755, [0/560, 87%], loss is 0.080575\n",
      "best model\n",
      "Epoch 755\n",
      "Training Epoch: 755, [70/560, 89%], loss is 0.060255\n",
      "best model\n",
      "Epoch 755\n",
      "Training Epoch: 755, [140/560, 89%], loss is 0.076837\n",
      "best model\n",
      "Epoch 755\n",
      "Training Epoch: 755, [210/560, 81%], loss is 0.086043\n",
      "best model\n",
      "Epoch 755\n",
      "Training Epoch: 755, [280/560, 89%], loss is 0.079254\n",
      "best model\n",
      "Epoch 755\n",
      "Training Epoch: 755, [350/560, 90%], loss is 0.043183\n",
      "best model\n",
      "Epoch 755\n",
      "Training Epoch: 755, [420/560, 76%], loss is 0.428582\n",
      "best model\n",
      "Epoch 755\n",
      "Training Epoch: 755, [490/560, 77%], loss is 0.207709\n",
      "best model\n",
      "Epoch 755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 760, [0/560, 87%], loss is 0.092420\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 760, [70/560, 83%], loss is 0.089829\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 760, [140/560, 90%], loss is 0.071605\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 760, [210/560, 86%], loss is 0.083568\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 760, [280/560, 86%], loss is 0.109941\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 760, [350/560, 84%], loss is 0.087659\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 760, [420/560, 89%], loss is 0.059132\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 760, [490/560, 87%], loss is 0.071518\n",
      "best model\n",
      "Epoch 760\n",
      "Training Epoch: 765, [0/560, 83%], loss is 0.061931\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 765, [70/560, 86%], loss is 0.081608\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 765, [140/560, 86%], loss is 0.100709\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 765, [210/560, 80%], loss is 0.105494\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 765, [280/560, 86%], loss is 0.175326\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 765, [350/560, 84%], loss is 0.304229\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 765, [420/560, 90%], loss is 0.157937\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 765, [490/560, 94%], loss is 0.053568\n",
      "best model\n",
      "Epoch 765\n",
      "Training Epoch: 770, [0/560, 76%], loss is 0.189491\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 770, [70/560, 77%], loss is 0.132410\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 770, [140/560, 83%], loss is 0.152432\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 770, [210/560, 80%], loss is 0.173775\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 770, [280/560, 83%], loss is 0.170308\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 770, [350/560, 83%], loss is 0.097100\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 770, [420/560, 79%], loss is 0.241949\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 770, [490/560, 89%], loss is 0.170755\n",
      "best model\n",
      "Epoch 770\n",
      "Training Epoch: 775, [0/560, 87%], loss is 0.272970\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 775, [70/560, 79%], loss is 0.233446\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 775, [140/560, 77%], loss is 0.156878\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 775, [210/560, 77%], loss is 0.252814\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 775, [280/560, 89%], loss is 0.253205\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 775, [350/560, 80%], loss is 0.118073\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 775, [420/560, 83%], loss is 0.100169\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 775, [490/560, 76%], loss is 0.299076\n",
      "best model\n",
      "Epoch 775\n",
      "Training Epoch: 780, [0/560, 89%], loss is 0.096196\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 780, [70/560, 87%], loss is 0.064553\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 780, [140/560, 87%], loss is 0.094234\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 780, [210/560, 87%], loss is 0.068736\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 780, [280/560, 81%], loss is 0.127866\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 780, [350/560, 84%], loss is 0.094943\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 780, [420/560, 91%], loss is 0.065512\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 780, [490/560, 77%], loss is 0.117293\n",
      "best model\n",
      "Epoch 780\n",
      "Training Epoch: 785, [0/560, 86%], loss is 0.082279\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 785, [70/560, 86%], loss is 0.150795\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 785, [140/560, 84%], loss is 0.089092\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 785, [210/560, 84%], loss is 0.103433\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 785, [280/560, 87%], loss is 0.069807\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 785, [350/560, 86%], loss is 0.080366\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 785, [420/560, 84%], loss is 0.078975\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 785, [490/560, 91%], loss is 0.063192\n",
      "best model\n",
      "Epoch 785\n",
      "Training Epoch: 790, [0/560, 91%], loss is 0.052886\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 790, [70/560, 86%], loss is 0.072333\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 790, [140/560, 90%], loss is 0.053183\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 790, [210/560, 90%], loss is 0.044366\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 790, [280/560, 83%], loss is 0.091289\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 790, [350/560, 89%], loss is 0.050123\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 790, [420/560, 87%], loss is 0.047350\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 790, [490/560, 87%], loss is 0.073933\n",
      "best model\n",
      "Epoch 790\n",
      "Training Epoch: 795, [0/560, 89%], loss is 0.066588\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 795, [70/560, 89%], loss is 0.050330\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 795, [140/560, 90%], loss is 0.071005\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 795, [210/560, 84%], loss is 0.079697\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 795, [280/560, 89%], loss is 0.043735\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 795, [350/560, 83%], loss is 0.059377\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 795, [420/560, 80%], loss is 0.075297\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 795, [490/560, 84%], loss is 0.116312\n",
      "best model\n",
      "Epoch 795\n",
      "Training Epoch: 800, [0/560, 86%], loss is 0.083507\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 800, [70/560, 84%], loss is 0.084609\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 800, [140/560, 84%], loss is 0.089612\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 800, [210/560, 81%], loss is 0.117994\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 800, [280/560, 81%], loss is 0.123100\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 800, [350/560, 84%], loss is 0.124301\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 800, [420/560, 83%], loss is 0.124982\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 800, [490/560, 87%], loss is 0.094550\n",
      "best model\n",
      "Epoch 800\n",
      "Training Epoch: 805, [0/560, 89%], loss is 0.054057\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 805, [70/560, 86%], loss is 0.197318\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 805, [140/560, 83%], loss is 0.091320\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 805, [210/560, 84%], loss is 0.124486\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 805, [280/560, 84%], loss is 0.130040\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 805, [350/560, 90%], loss is 0.052953\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 805, [420/560, 81%], loss is 0.128279\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 805, [490/560, 90%], loss is 0.080726\n",
      "best model\n",
      "Epoch 805\n",
      "Training Epoch: 810, [0/560, 79%], loss is 0.294011\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 810, [70/560, 94%], loss is 0.078984\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 810, [140/560, 81%], loss is 0.116364\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 810, [210/560, 87%], loss is 0.114188\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 810, [280/560, 79%], loss is 0.298379\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 810, [350/560, 83%], loss is 0.071386\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 810, [420/560, 93%], loss is 0.094685\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 810, [490/560, 87%], loss is 0.061733\n",
      "best model\n",
      "Epoch 810\n",
      "Training Epoch: 815, [0/560, 84%], loss is 0.105609\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 815, [70/560, 83%], loss is 0.095640\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 815, [140/560, 89%], loss is 0.102706\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 815, [210/560, 89%], loss is 0.094550\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 815, [280/560, 86%], loss is 0.067995\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 815, [350/560, 86%], loss is 0.057768\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 815, [420/560, 83%], loss is 0.125414\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 815, [490/560, 83%], loss is 0.082293\n",
      "best model\n",
      "Epoch 815\n",
      "Training Epoch: 820, [0/560, 87%], loss is 0.076674\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 820, [70/560, 93%], loss is 0.062472\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 820, [140/560, 91%], loss is 0.045799\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 820, [210/560, 83%], loss is 0.092331\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 820, [280/560, 90%], loss is 0.066942\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 820, [350/560, 81%], loss is 0.100405\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 820, [420/560, 86%], loss is 0.088972\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 820, [490/560, 84%], loss is 0.081001\n",
      "best model\n",
      "Epoch 820\n",
      "Training Epoch: 825, [0/560, 90%], loss is 0.067230\n",
      "best model\n",
      "Epoch 825\n",
      "Training Epoch: 825, [70/560, 87%], loss is 0.080045\n",
      "best model\n",
      "Epoch 825\n",
      "Training Epoch: 825, [140/560, 86%], loss is 0.061543\n",
      "best model\n",
      "Epoch 825\n",
      "Training Epoch: 825, [210/560, 90%], loss is 0.049303\n",
      "best model\n",
      "Epoch 825\n",
      "Training Epoch: 825, [280/560, 94%], loss is 0.051231\n",
      "best model\n",
      "Epoch 825\n",
      "Training Epoch: 825, [350/560, 87%], loss is 0.077531\n",
      "best model\n",
      "Epoch 825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 825, [420/560, 81%], loss is 0.070485\n",
      "best model\n",
      "Epoch 825\n",
      "Training Epoch: 825, [490/560, 90%], loss is 0.074433\n",
      "best model\n",
      "Epoch 825\n",
      "Training Epoch: 830, [0/560, 80%], loss is 0.127616\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 830, [70/560, 83%], loss is 0.087294\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 830, [140/560, 89%], loss is 0.065610\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 830, [210/560, 91%], loss is 0.069394\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 830, [280/560, 90%], loss is 0.053006\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 830, [350/560, 81%], loss is 0.072109\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 830, [420/560, 96%], loss is 0.052519\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 830, [490/560, 87%], loss is 0.086504\n",
      "best model\n",
      "Epoch 830\n",
      "Training Epoch: 835, [0/560, 90%], loss is 0.064452\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 835, [70/560, 86%], loss is 0.119236\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 835, [140/560, 80%], loss is 0.202414\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 835, [210/560, 86%], loss is 0.057210\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 835, [280/560, 84%], loss is 0.076666\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 835, [350/560, 87%], loss is 0.216883\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 835, [420/560, 80%], loss is 0.229734\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 835, [490/560, 91%], loss is 0.145493\n",
      "best model\n",
      "Epoch 835\n",
      "Training Epoch: 840, [0/560, 74%], loss is 0.203323\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 840, [70/560, 91%], loss is 0.080139\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 840, [140/560, 79%], loss is 0.351763\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 840, [210/560, 79%], loss is 0.570481\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 840, [280/560, 87%], loss is 0.162726\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 840, [350/560, 97%], loss is 0.157144\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 840, [420/560, 87%], loss is 0.145011\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 840, [490/560, 86%], loss is 0.156695\n",
      "best model\n",
      "Epoch 840\n",
      "Training Epoch: 845, [0/560, 73%], loss is 0.248888\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 845, [70/560, 84%], loss is 0.105231\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 845, [140/560, 86%], loss is 0.099084\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 845, [210/560, 84%], loss is 0.142683\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 845, [280/560, 86%], loss is 0.096343\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 845, [350/560, 90%], loss is 0.124433\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 845, [420/560, 87%], loss is 0.130938\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 845, [490/560, 74%], loss is 0.165050\n",
      "best model\n",
      "Epoch 845\n",
      "Training Epoch: 850, [0/560, 83%], loss is 0.084019\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 850, [70/560, 87%], loss is 0.062966\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 850, [140/560, 74%], loss is 0.144535\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 850, [210/560, 86%], loss is 0.133412\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 850, [280/560, 83%], loss is 0.091145\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 850, [350/560, 93%], loss is 0.050864\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 850, [420/560, 89%], loss is 0.067972\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 850, [490/560, 89%], loss is 0.075460\n",
      "best model\n",
      "Epoch 850\n",
      "Training Epoch: 855, [0/560, 89%], loss is 0.056346\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 855, [70/560, 91%], loss is 0.060307\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 855, [140/560, 83%], loss is 0.122095\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 855, [210/560, 86%], loss is 0.091330\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 855, [280/560, 90%], loss is 0.061091\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 855, [350/560, 86%], loss is 0.047489\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 855, [420/560, 86%], loss is 0.067587\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 855, [490/560, 87%], loss is 0.077004\n",
      "best model\n",
      "Epoch 855\n",
      "Training Epoch: 860, [0/560, 89%], loss is 0.055159\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 860, [70/560, 83%], loss is 0.076320\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 860, [140/560, 89%], loss is 0.074733\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 860, [210/560, 83%], loss is 0.090452\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 860, [280/560, 90%], loss is 0.085560\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 860, [350/560, 84%], loss is 0.092814\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 860, [420/560, 81%], loss is 0.087374\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 860, [490/560, 80%], loss is 0.091486\n",
      "best model\n",
      "Epoch 860\n",
      "Training Epoch: 865, [0/560, 86%], loss is 0.081556\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 865, [70/560, 87%], loss is 0.079016\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 865, [140/560, 90%], loss is 0.068598\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 865, [210/560, 87%], loss is 0.089336\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 865, [280/560, 94%], loss is 0.026672\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 865, [350/560, 87%], loss is 0.070295\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 865, [420/560, 91%], loss is 0.056010\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 865, [490/560, 90%], loss is 0.046107\n",
      "best model\n",
      "Epoch 865\n",
      "Training Epoch: 870, [0/560, 96%], loss is 0.030601\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 870, [70/560, 89%], loss is 0.029128\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 870, [140/560, 93%], loss is 0.048957\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 870, [210/560, 86%], loss is 0.046723\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 870, [280/560, 90%], loss is 0.051523\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 870, [350/560, 81%], loss is 0.084550\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 870, [420/560, 81%], loss is 0.092143\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 870, [490/560, 89%], loss is 0.054178\n",
      "best model\n",
      "Epoch 870\n",
      "Training Epoch: 875, [0/560, 89%], loss is 0.126909\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 875, [70/560, 84%], loss is 0.209373\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 875, [140/560, 80%], loss is 0.157673\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 875, [210/560, 89%], loss is 0.068147\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 875, [280/560, 80%], loss is 0.112907\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 875, [350/560, 86%], loss is 0.062510\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 875, [420/560, 80%], loss is 0.115802\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 875, [490/560, 89%], loss is 0.125255\n",
      "best model\n",
      "Epoch 875\n",
      "Training Epoch: 880, [0/560, 89%], loss is 0.067228\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 880, [70/560, 83%], loss is 0.086204\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 880, [140/560, 86%], loss is 0.079382\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 880, [210/560, 90%], loss is 0.087318\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 880, [280/560, 90%], loss is 0.071423\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 880, [350/560, 84%], loss is 0.100265\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 880, [420/560, 89%], loss is 0.062226\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 880, [490/560, 81%], loss is 0.092623\n",
      "best model\n",
      "Epoch 880\n",
      "Training Epoch: 885, [0/560, 84%], loss is 0.074784\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 885, [70/560, 89%], loss is 0.050241\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 885, [140/560, 91%], loss is 0.048206\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 885, [210/560, 89%], loss is 0.047047\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 885, [280/560, 87%], loss is 0.063253\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 885, [350/560, 90%], loss is 0.068623\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 885, [420/560, 89%], loss is 0.064510\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 885, [490/560, 86%], loss is 0.087776\n",
      "best model\n",
      "Epoch 885\n",
      "Training Epoch: 890, [0/560, 93%], loss is 0.042406\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 890, [70/560, 93%], loss is 0.065564\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 890, [140/560, 89%], loss is 0.050628\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 890, [210/560, 91%], loss is 0.030394\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 890, [280/560, 87%], loss is 0.064544\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 890, [350/560, 84%], loss is 0.123705\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 890, [420/560, 80%], loss is 0.087815\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 890, [490/560, 89%], loss is 0.075312\n",
      "best model\n",
      "Epoch 890\n",
      "Training Epoch: 895, [0/560, 83%], loss is 0.077352\n",
      "best model\n",
      "Epoch 895\n",
      "Training Epoch: 895, [70/560, 90%], loss is 0.101125\n",
      "best model\n",
      "Epoch 895\n",
      "Training Epoch: 895, [140/560, 89%], loss is 0.069194\n",
      "best model\n",
      "Epoch 895\n",
      "Training Epoch: 895, [210/560, 86%], loss is 0.048646\n",
      "best model\n",
      "Epoch 895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 895, [280/560, 91%], loss is 0.042075\n",
      "best model\n",
      "Epoch 895\n",
      "Training Epoch: 895, [350/560, 87%], loss is 0.043027\n",
      "best model\n",
      "Epoch 895\n",
      "Training Epoch: 895, [420/560, 81%], loss is 0.113225\n",
      "best model\n",
      "Epoch 895\n",
      "Training Epoch: 895, [490/560, 94%], loss is 0.097376\n",
      "best model\n",
      "Epoch 895\n",
      "Training Epoch: 900, [0/560, 84%], loss is 0.118075\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 900, [70/560, 91%], loss is 0.056953\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 900, [140/560, 86%], loss is 0.119661\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 900, [210/560, 87%], loss is 0.094385\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 900, [280/560, 79%], loss is 0.102865\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 900, [350/560, 84%], loss is 0.071739\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 900, [420/560, 89%], loss is 0.069342\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 900, [490/560, 84%], loss is 0.153767\n",
      "best model\n",
      "Epoch 900\n",
      "Training Epoch: 905, [0/560, 87%], loss is 0.096979\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 905, [70/560, 87%], loss is 0.124495\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 905, [140/560, 87%], loss is 0.098835\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 905, [210/560, 84%], loss is 0.087295\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 905, [280/560, 91%], loss is 0.063299\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 905, [350/560, 89%], loss is 0.101283\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 905, [420/560, 87%], loss is 0.102573\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 905, [490/560, 87%], loss is 0.127973\n",
      "best model\n",
      "Epoch 905\n",
      "Training Epoch: 910, [0/560, 83%], loss is 0.082558\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 910, [70/560, 79%], loss is 0.111894\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 910, [140/560, 84%], loss is 0.066281\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 910, [210/560, 86%], loss is 0.104689\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 910, [280/560, 87%], loss is 0.131812\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 910, [350/560, 84%], loss is 0.228577\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 910, [420/560, 86%], loss is 0.188168\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 910, [490/560, 83%], loss is 0.099781\n",
      "best model\n",
      "Epoch 910\n",
      "Training Epoch: 915, [0/560, 84%], loss is 0.070049\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 915, [70/560, 84%], loss is 0.098125\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 915, [140/560, 90%], loss is 0.061308\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 915, [210/560, 79%], loss is 0.107645\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 915, [280/560, 80%], loss is 0.115787\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 915, [350/560, 84%], loss is 0.061441\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 915, [420/560, 83%], loss is 0.118325\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 915, [490/560, 83%], loss is 0.105717\n",
      "best model\n",
      "Epoch 915\n",
      "Training Epoch: 920, [0/560, 86%], loss is 0.066808\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 920, [70/560, 90%], loss is 0.119809\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 920, [140/560, 87%], loss is 0.136550\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 920, [210/560, 83%], loss is 0.092430\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 920, [280/560, 89%], loss is 0.066218\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 920, [350/560, 90%], loss is 0.045738\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 920, [420/560, 90%], loss is 0.075216\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 920, [490/560, 89%], loss is 0.139956\n",
      "best model\n",
      "Epoch 920\n",
      "Training Epoch: 925, [0/560, 93%], loss is 0.095843\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 925, [70/560, 86%], loss is 0.066098\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 925, [140/560, 89%], loss is 0.050958\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 925, [210/560, 87%], loss is 0.085184\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 925, [280/560, 81%], loss is 0.086606\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 925, [350/560, 80%], loss is 0.166097\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 925, [420/560, 90%], loss is 0.054507\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 925, [490/560, 90%], loss is 0.063116\n",
      "best model\n",
      "Epoch 925\n",
      "Training Epoch: 930, [0/560, 86%], loss is 0.055426\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 930, [70/560, 94%], loss is 0.047191\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 930, [140/560, 87%], loss is 0.080067\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 930, [210/560, 91%], loss is 0.067342\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 930, [280/560, 93%], loss is 0.080846\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 930, [350/560, 86%], loss is 0.077188\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 930, [420/560, 94%], loss is 0.033459\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 930, [490/560, 84%], loss is 0.064117\n",
      "best model\n",
      "Epoch 930\n",
      "Training Epoch: 935, [0/560, 84%], loss is 0.082733\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 935, [70/560, 89%], loss is 0.076299\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 935, [140/560, 81%], loss is 0.086230\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 935, [210/560, 83%], loss is 0.087791\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 935, [280/560, 84%], loss is 0.067881\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 935, [350/560, 90%], loss is 0.066214\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 935, [420/560, 86%], loss is 0.062090\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 935, [490/560, 84%], loss is 0.088085\n",
      "best model\n",
      "Epoch 935\n",
      "Training Epoch: 940, [0/560, 89%], loss is 0.076114\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 940, [70/560, 89%], loss is 0.065478\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 940, [140/560, 87%], loss is 0.072462\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 940, [210/560, 87%], loss is 0.077328\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 940, [280/560, 87%], loss is 0.073782\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 940, [350/560, 89%], loss is 0.052294\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 940, [420/560, 96%], loss is 0.040924\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 940, [490/560, 89%], loss is 0.057428\n",
      "best model\n",
      "Epoch 940\n",
      "Training Epoch: 945, [0/560, 87%], loss is 0.064497\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 945, [70/560, 91%], loss is 0.044578\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 945, [140/560, 87%], loss is 0.046077\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 945, [210/560, 89%], loss is 0.068541\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 945, [280/560, 94%], loss is 0.030021\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 945, [350/560, 87%], loss is 0.059785\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 945, [420/560, 89%], loss is 0.079526\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 945, [490/560, 87%], loss is 0.078655\n",
      "best model\n",
      "Epoch 945\n",
      "Training Epoch: 950, [0/560, 86%], loss is 0.085850\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 950, [70/560, 86%], loss is 0.066028\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 950, [140/560, 87%], loss is 0.062992\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 950, [210/560, 89%], loss is 0.052436\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 950, [280/560, 83%], loss is 0.086249\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 950, [350/560, 89%], loss is 0.063958\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 950, [420/560, 86%], loss is 0.075789\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 950, [490/560, 90%], loss is 0.051696\n",
      "best model\n",
      "Epoch 950\n",
      "Training Epoch: 955, [0/560, 83%], loss is 0.088084\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 955, [70/560, 87%], loss is 0.041125\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 955, [140/560, 89%], loss is 0.063566\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 955, [210/560, 86%], loss is 0.074450\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 955, [280/560, 89%], loss is 0.054298\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 955, [350/560, 86%], loss is 0.069377\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 955, [420/560, 83%], loss is 0.063320\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 955, [490/560, 87%], loss is 0.059758\n",
      "best model\n",
      "Epoch 955\n",
      "Training Epoch: 960, [0/560, 94%], loss is 0.037809\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 960, [70/560, 80%], loss is 0.089044\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 960, [140/560, 74%], loss is 0.104684\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 960, [210/560, 86%], loss is 0.065697\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 960, [280/560, 87%], loss is 0.068757\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 960, [350/560, 94%], loss is 0.042616\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 960, [420/560, 91%], loss is 0.036039\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 960, [490/560, 84%], loss is 0.071897\n",
      "best model\n",
      "Epoch 960\n",
      "Training Epoch: 965, [0/560, 89%], loss is 0.073782\n",
      "best model\n",
      "Epoch 965\n",
      "Training Epoch: 965, [70/560, 91%], loss is 0.039269\n",
      "best model\n",
      "Epoch 965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 965, [140/560, 84%], loss is 0.067596\n",
      "best model\n",
      "Epoch 965\n",
      "Training Epoch: 965, [210/560, 94%], loss is 0.048835\n",
      "best model\n",
      "Epoch 965\n",
      "Training Epoch: 965, [280/560, 93%], loss is 0.035309\n",
      "best model\n",
      "Epoch 965\n",
      "Training Epoch: 965, [350/560, 91%], loss is 0.047301\n",
      "best model\n",
      "Epoch 965\n",
      "Training Epoch: 965, [420/560, 94%], loss is 0.041398\n",
      "best model\n",
      "Epoch 965\n",
      "Training Epoch: 965, [490/560, 90%], loss is 0.057102\n",
      "best model\n",
      "Epoch 965\n",
      "Training Epoch: 970, [0/560, 93%], loss is 0.026540\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 970, [70/560, 83%], loss is 0.069581\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 970, [140/560, 89%], loss is 0.058550\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 970, [210/560, 80%], loss is 0.081256\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 970, [280/560, 87%], loss is 0.111020\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 970, [350/560, 84%], loss is 0.071388\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 970, [420/560, 87%], loss is 0.067293\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 970, [490/560, 87%], loss is 0.057122\n",
      "best model\n",
      "Epoch 970\n",
      "Training Epoch: 975, [0/560, 94%], loss is 0.046394\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 975, [70/560, 87%], loss is 0.077351\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 975, [140/560, 90%], loss is 0.082979\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 975, [210/560, 86%], loss is 0.400111\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 975, [280/560, 90%], loss is 0.206111\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 975, [350/560, 86%], loss is 0.051252\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 975, [420/560, 90%], loss is 0.068829\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 975, [490/560, 90%], loss is 0.088558\n",
      "best model\n",
      "Epoch 975\n",
      "Training Epoch: 980, [0/560, 83%], loss is 0.160993\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 980, [70/560, 77%], loss is 0.198256\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 980, [140/560, 81%], loss is 0.127569\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 980, [210/560, 87%], loss is 0.071583\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 980, [280/560, 83%], loss is 0.111371\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 980, [350/560, 79%], loss is 0.159042\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 980, [420/560, 76%], loss is 0.163775\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 980, [490/560, 83%], loss is 0.219694\n",
      "best model\n",
      "Epoch 980\n",
      "Training Epoch: 985, [0/560, 90%], loss is 0.120859\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 985, [70/560, 89%], loss is 0.101075\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 985, [140/560, 90%], loss is 0.059066\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 985, [210/560, 89%], loss is 0.045777\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 985, [280/560, 86%], loss is 0.084433\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 985, [350/560, 90%], loss is 0.564045\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 985, [420/560, 91%], loss is 0.078860\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 985, [490/560, 87%], loss is 0.062841\n",
      "best model\n",
      "Epoch 985\n",
      "Training Epoch: 990, [0/560, 89%], loss is 0.079059\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 990, [70/560, 90%], loss is 0.095759\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 990, [140/560, 77%], loss is 0.166940\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 990, [210/560, 89%], loss is 0.072126\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 990, [280/560, 86%], loss is 0.094188\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 990, [350/560, 81%], loss is 0.167865\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 990, [420/560, 81%], loss is 0.108289\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 990, [490/560, 83%], loss is 0.105091\n",
      "best model\n",
      "Epoch 990\n",
      "Training Epoch: 995, [0/560, 79%], loss is 0.132295\n",
      "best model\n",
      "Epoch 995\n",
      "Training Epoch: 995, [70/560, 89%], loss is 0.059041\n",
      "best model\n",
      "Epoch 995\n",
      "Training Epoch: 995, [140/560, 84%], loss is 0.098543\n",
      "best model\n",
      "Epoch 995\n",
      "Training Epoch: 995, [210/560, 87%], loss is 0.078883\n",
      "best model\n",
      "Epoch 995\n",
      "Training Epoch: 995, [280/560, 87%], loss is 0.092102\n",
      "best model\n",
      "Epoch 995\n",
      "Training Epoch: 995, [350/560, 90%], loss is 0.045894\n",
      "best model\n",
      "Epoch 995\n",
      "Training Epoch: 995, [420/560, 91%], loss is 0.056421\n",
      "best model\n",
      "Epoch 995\n",
      "Training Epoch: 995, [490/560, 87%], loss is 0.050408\n",
      "best model\n",
      "Epoch 995\n",
      "Test set results: loss= 0.0333 accuracy= 99.6429 1-hop accuracy = 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_loss, train_best, model_test = main(seed, dim_input, dim_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc5a13fb9d0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss[0:len(train_loss):30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_sigPQ_perturb_1_percentage.mat\n",
      "Test set results: loss= 0.0757 accuracy= 97.3214 1-hop accuracy = 0.9964\n",
      "testing_sigPQ_perturb_1.5_percentage.mat\n",
      "Test set results: loss= 0.2822 accuracy= 90.5357 1-hop accuracy = 0.9875\n",
      "testing_sigPQ_perturb_2_percentage.mat\n",
      "Test set results: loss= 0.6428 accuracy= 83.0357 1-hop accuracy = 0.9625\n",
      "testing_sigPQ_perturb_3_percentage.mat\n",
      "Test set results: loss= 1.8313 accuracy= 69.1071 1-hop accuracy = 0.8625\n",
      "[[97.32 90.54 83.04 69.11]]\n"
     ]
    }
   ],
   "source": [
    "model_test = Net(dim_input, dim_hidden)\n",
    "model_test.load_state_dict(torch.load( os.path.join(model_dir, 'model-' + savename + '.pt')))\n",
    "   \n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "mag = [  '1','1.5','2' , '3' ] \n",
    "acc_list = np.zeros((len(mag),1))\n",
    "acc_hop_list = np.zeros(acc_list.shape)\n",
    "for i in range(len(mag)):\n",
    "    testName = 'testing_sigPQ_perturb_' + mag[i] + '_percentage.mat'# 'Test_perturb_'+ mag[i] + 'pu_type_' + str(fault_type+1) + '_impedance_' \\+ str(impe_type+1)+ '_sigNew' \n",
    "    print(testName)\n",
    "    test_x,   test_labels,test_num= load_data_VI_new(w,rootPath, testName)  \n",
    "    acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "    acc_list[ i] = float(\"{:.2f}\".format(acc)) \n",
    "    acc_hop_list[  i] = float(\"{:.2f}\".format(acc_hop))  \n",
    "print( (acc_list.T ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_SigControl_allgener_0.1_pu.mat\n",
      "Test set results: loss= 0.0594 accuracy= 98.3929 1-hop accuracy = 0.9964\n",
      "testing_SigControl_allgener_0.2_pu.mat\n",
      "Test set results: loss= 0.1230 accuracy= 96.4286 1-hop accuracy = 0.9857\n",
      "testing_SigControl_allgener_0.3_pu.mat\n",
      "Test set results: loss= 0.2071 accuracy= 93.9286 1-hop accuracy = 0.9750\n",
      "testing_SigControl_allgener_0.4_pu.mat\n",
      "Test set results: loss= 0.3566 accuracy= 91.9643 1-hop accuracy = 0.9589\n",
      "testing_SigControl_allgener_0.5_pu.mat\n",
      "Test set results: loss= 0.5742 accuracy= 86.4286 1-hop accuracy = 0.9268\n",
      "[[98.39 96.43 93.93 91.96 86.43]]\n"
     ]
    }
   ],
   "source": [
    "model_test = Net(dim_input, dim_hidden)\n",
    "model_test.load_state_dict(torch.load( os.path.join(model_dir, 'model-' + savename + '.pt')))\n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "mag_control = ['0.1','0.2','0.3','0.4' , '0.5']\n",
    "acc_list = np.zeros((len(mag_control),1))\n",
    "acc_hop_list = np.zeros(acc_list.shape)\n",
    "for i in range(len(mag_control)):\n",
    "    testName = 'testing_SigControl_allgener_' +  mag_control[i]  + '_pu.mat'\n",
    "    print(testName)\n",
    "    test_x,   test_labels,test_num= load_data_VI_new(w,rootPath, testName)  \n",
    "    acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "    acc_list[ i] = float(\"{:.2f}\".format(acc)) \n",
    "    acc_hop_list[  i] = float(\"{:.2f}\".format(acc_hop))  \n",
    "print( (acc_list.T ))    \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_loss_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = 552//70*300\n",
    "train_loss_noregu =[4.745625972747803, 4.456696510314941, 4.357154846191406, 4.352932453155518, 4.121169090270996, 4.168258190155029, 4.0213117599487305, 4.002157211303711, 3.9735639095306396, 3.883394718170166, 3.852123498916626, 3.754178285598755, 3.6294469833374023, 3.5256497859954834, 3.5135412216186523, 3.511547088623047, 3.524271011352539, 3.4840140342712402, 3.357811689376831, 3.240104913711548, 2.951444149017334, 2.877629041671753, 3.2644052505493164, 2.9462192058563232, 2.9429829120635986, 2.8862850666046143, 3.029782295227051, 2.730166435241699, 2.74981951713562, 2.7309086322784424, 2.5644900798797607, 2.4680681228637695, 2.5348339080810547, 2.4822540283203125, 2.482224225997925, 2.3806493282318115, 2.3882579803466797, 2.3677587509155273, 2.2208054065704346, 2.169201612472534, 2.214273452758789, 2.115992307662964, 1.9817023277282715, 1.984116792678833, 1.9552161693572998, 1.9631586074829102, 1.8889025449752808, 1.881063461303711, 1.9232300519943237, 1.7737377882003784, 1.8902654647827148, 1.9031257629394531, 1.8507863283157349, 1.7808926105499268, 1.6713825464248657, 1.5913234949111938, 1.5057812929153442, 1.8063828945159912, 1.5853183269500732, 1.4316246509552002, 1.6397202014923096, 1.5285996198654175, 1.5735265016555786, 1.3973426818847656, 1.2722923755645752, 1.5791321992874146, 1.3888357877731323, 1.6936458349227905, 1.5535122156143188, 1.5106998682022095, 1.2514206171035767, 1.3893612623214722, 1.2139276266098022, 1.3898402452468872, 1.3412725925445557, 1.4109824895858765, 1.3102518320083618, 1.3015387058258057, 1.2962557077407837, 1.221261978149414, 1.176613450050354, 1.1471909284591675, 1.2550231218338013, 1.1464475393295288, 1.1591686010360718, 1.2740442752838135, 1.1237143278121948, 1.3017354011535645, 1.1372549533843994, 1.1001884937286377, 1.078153133392334, 0.9972777962684631, 0.958417534828186, 1.0767502784729004, 0.9930189847946167, 1.0285550355911255, 1.0878558158874512, 0.9038298726081848, 1.0346252918243408, 0.8888367414474487, 1.0329818725585938, 0.8083903193473816, 1.047920823097229, 0.9852449893951416, 0.9197216629981995, 0.8537733554840088, 0.8486788272857666, 0.8451921343803406, 0.7917031645774841, 0.9271080493927002, 0.8330649733543396, 0.885593593120575, 0.7543959617614746, 0.736476719379425, 0.9533486366271973, 0.8817031979560852, 0.7748497128486633, 0.8707833886146545, 0.8396646976470947, 0.9045959711074829, 0.7217587828636169, 0.7213079333305359, 0.724692702293396, 0.81397545337677, 0.8684503436088562, 0.6405384540557861, 0.7067212462425232, 0.6717466711997986, 0.6640275716781616, 0.8074582815170288, 0.6995357871055603, 0.7374575138092041, 0.7773217558860779, 0.6669643521308899, 0.5895241498947144, 0.8358684182167053, 0.9191528558731079, 0.6642494201660156, 0.730644941329956, 0.6732279658317566, 0.6265103816986084, 0.7192863821983337, 0.6721801161766052, 0.5859752297401428, 0.6770429611206055, 0.7395886778831482, 0.6318424940109253, 0.5706590414047241, 0.6096128225326538, 0.8204998970031738, 0.5903738141059875, 0.7667896747589111, 0.7873279452323914, 0.761865496635437, 0.5575388073921204, 0.730779767036438, 0.6842268109321594, 0.5121209025382996, 0.6615317463874817, 0.5769963264465332, 0.6308512091636658, 0.5564888119697571, 0.6348657608032227, 0.5830143094062805, 0.5458860993385315, 0.6514995098114014, 0.5930621027946472, 0.5502113103866577, 0.6521579027175903, 0.5795019268989563, 0.646162748336792, 0.7116018533706665, 0.5525330305099487, 0.45431020855903625, 0.5522657036781311, 0.49051761627197266, 0.4553261697292328, 0.5146001577377319, 0.5133762359619141, 0.5002856850624084, 0.5817666053771973, 0.4947103261947632, 0.49289804697036743, 0.6058309674263, 0.5334698557853699, 0.4541029632091522, 0.49439698457717896, 0.6084842085838318, 0.5001153945922852, 0.5529870986938477, 0.5984950661659241, 0.4732738733291626, 0.6126233339309692, 0.6488263607025146, 0.4971179962158203, 0.46544697880744934, 0.43052637577056885, 0.5830774307250977, 0.6463861465454102, 0.562667191028595, 0.4816012978553772, 0.654923141002655, 0.49109333753585815, 0.3924477994441986, 0.4988666772842407, 0.5139084458351135, 0.4737192988395691, 0.42545366287231445, 0.46772995591163635, 0.5563077926635742, 0.3777056634426117, 0.5255182981491089, 0.6575437784194946, 0.5354505777359009, 0.40943509340286255, 0.46262508630752563, 0.5063302516937256, 0.4791768789291382, 0.4011112451553345, 0.5132416486740112, 0.48909467458724976, 0.4356824457645416, 0.3230058550834656, 0.5330284237861633, 0.47534891963005066, 0.4317167103290558, 0.4557165801525116, 0.4152212142944336, 0.5389747619628906, 0.4991667568683624, 0.42264455556869507, 0.4940876364707947, 0.4448555111885071, 0.4495790898799896, 0.4701458215713501, 0.4374001622200012, 0.4924028217792511, 0.46709781885147095, 0.4020656943321228, 0.4582594037055969, 0.4437086880207062, 0.43995893001556396, 0.45755043625831604, 0.5516196489334106, 0.45290613174438477, 0.440842866897583, 0.32673129439353943, 0.41268572211265564, 0.4259812533855438, 0.4611772894859314, 0.3880811333656311, 0.4992108643054962, 0.40932008624076843, 0.38320696353912354, 0.41235873103141785, 0.4304876923561096, 0.44723090529441833, 0.4073272943496704, 0.3795734941959381, 0.34030380845069885, 0.4653940200805664, 0.4475490152835846, 0.4361332654953003, 0.396023690700531, 0.36964160203933716, 0.3614920377731323, 0.3898649513721466, 0.45219141244888306, 0.5246117115020752, 0.4826352298259735, 0.4933983087539673, 0.34605205059051514, 0.38348832726478577, 0.3651370704174042, 0.422693133354187, 0.45907294750213623, 0.3404679000377655, 0.3608716130256653, 0.44645553827285767, 0.36850613355636597, 0.3195309042930603, 0.39913561940193176, 0.48339366912841797, 0.37085503339767456, 0.3878006339073181, 0.44835221767425537, 0.3340212106704712, 0.3625069260597229, 0.4383232891559601, 0.31566867232322693, 0.41082146763801575, 0.3881300389766693, 0.3732621669769287, 0.484624445438385, 0.3360592722892761, 0.36697643995285034, 0.3375265896320343, 0.35180357098579407, 0.37742117047309875, 0.30015724897384644, 0.3995770812034607, 0.30456700921058655, 0.38383156061172485, 0.36081165075302124, 0.25400716066360474, 0.30583855509757996, 0.3117940127849579, 0.3692505359649658, 0.2715931534767151, 0.4409260153770447, 0.32378771901130676, 0.3633459508419037, 0.3768215775489807, 0.2993875741958618, 0.37519919872283936, 0.39551815390586853, 0.2635549008846283, 0.3930019736289978, 0.2658346891403198, 0.3083277940750122, 0.30371415615081787, 0.3384823203086853, 0.4028274416923523, 0.3147197365760803, 0.3066392242908478, 0.36061638593673706, 0.3136817514896393, 0.43201810121536255, 0.3394601047039032, 0.3391419053077698, 0.32376980781555176, 0.3554590344429016, 0.3384166657924652, 0.31978002190589905, 0.42889875173568726, 0.31470105051994324, 0.3249585032463074, 0.39953023195266724, 0.4233056902885437, 0.2968738377094269, 0.401172399520874, 0.41206446290016174, 0.3802061676979065, 0.33188462257385254, 0.45912450551986694, 0.34321361780166626, 0.3662393093109131, 0.34656408429145813, 0.3246064782142639, 0.30986911058425903, 0.37226980924606323, 0.3438881039619446, 0.4041655659675598, 0.3902307450771332, 0.30415087938308716, 0.28883394598960876, 0.3503490388393402, 0.35029372572898865, 0.28899988532066345, 0.31503188610076904, 0.3602548837661743, 0.4169408082962036, 0.3272361159324646, 0.3686062693595886, 0.3753862679004669, 0.3167569637298584, 0.26928219199180603, 0.3316333591938019, 0.33107325434684753, 0.29814910888671875, 0.32143720984458923, 0.3061191439628601, 0.27768903970718384, 0.3280285894870758, 0.2926395535469055, 0.42270249128341675, 0.31718552112579346, 0.29623571038246155, 0.3171809911727905, 0.29596102237701416, 0.2984737753868103, 0.26300331950187683, 0.2582225799560547, 0.3481685519218445, 0.28351378440856934, 0.2919612526893616, 0.33213528990745544, 0.31774014234542847, 0.33730387687683105, 0.34487664699554443, 0.2922511398792267, 0.30367332696914673, 0.2544962465763092, 0.30791258811950684, 0.3192288875579834, 0.2518429160118103, 0.23672710359096527, 0.34653857350349426, 0.27501749992370605, 0.26018327474594116, 0.3571816086769104, 0.2674747109413147, 0.2788747251033783, 0.2719871401786804, 0.28422293066978455, 0.25003328919410706, 0.23494037985801697, 0.3018840551376343, 0.33377963304519653, 0.27539196610450745, 0.25090864300727844, 0.3020976185798645, 0.2815520167350769, 0.2491336464881897, 0.31845617294311523, 0.26578080654144287, 0.28852829337120056, 0.27739763259887695, 0.3784862160682678, 0.35844355821609497, 0.3332573175430298, 0.34113702178001404, 0.2431982457637787, 0.26647505164146423, 0.25017017126083374, 0.22959107160568237, 0.21019995212554932, 0.27975165843963623, 0.2578694522380829, 0.2531067132949829, 0.31862324476242065, 0.3241739571094513, 0.260998398065567, 0.3495502769947052, 0.2971629202365875, 0.22282397747039795, 0.23032474517822266, 0.22788548469543457, 0.28991276025772095, 0.23893508315086365, 0.42226454615592957, 0.25363409519195557, 0.29915571212768555, 0.2645730674266815, 0.317699134349823, 0.27109891176223755, 0.303844690322876, 0.276266872882843, 0.23601996898651123, 0.4321078658103943, 0.30753451585769653, 0.29743170738220215, 0.3087676465511322, 0.3275778889656067, 0.3101160526275635, 0.317378431558609, 0.21532854437828064, 0.3259642422199249, 0.2356470823287964, 0.2549136281013489, 0.3123067617416382, 0.2681051790714264, 0.2790820300579071, 0.24212007224559784, 0.35355454683303833, 0.333065927028656, 0.15830601751804352, 0.26575836539268494, 0.29575401544570923, 0.22395741939544678, 0.2313576340675354, 0.32337474822998047, 0.331512451171875, 0.22971515357494354, 0.2570936679840088, 0.18634441494941711, 0.44168397784233093, 0.2918521761894226, 0.35068169236183167, 0.3681826591491699, 0.27722078561782837, 0.2523055672645569, 0.2756650447845459, 0.26706719398498535, 0.3263901174068451, 0.31282877922058105, 0.25714752078056335, 0.26831620931625366, 0.32060497999191284, 0.24742752313613892, 0.2308797836303711, 0.293154239654541, 0.29417771100997925, 0.27934303879737854, 0.2516210079193115, 0.3358502686023712, 0.2991858124732971, 0.4255516529083252, 0.2991947531700134, 0.31157973408699036, 0.33617103099823, 0.2955487370491028, 0.389223575592041, 0.40910908579826355, 0.24055835604667664, 0.24823477864265442, 0.32076895236968994, 0.3832072913646698, 0.34577643871307373, 0.21250499784946442, 0.3769359886646271, 0.33842769265174866, 0.3508821129798889, 0.4481438398361206, 0.28880399465560913, 0.39607298374176025, 0.3862956762313843, 0.39424026012420654, 0.25622475147247314, 0.2123504877090454, 0.3005650043487549, 0.38037919998168945, 0.2593722939491272, 0.3420240879058838, 0.43913596868515015, 0.24174699187278748, 0.27648186683654785, 0.40423139929771423, 0.34539395570755005, 0.29752317070961, 0.3197273910045624, 0.2837180495262146, 0.28011998534202576, 0.2602372169494629, 0.32951560616493225, 0.30365175008773804, 0.24880178272724152, 0.3114336133003235, 0.21484681963920593, 0.2890891432762146, 0.24487483501434326, 0.3043200373649597, 0.3912012577056885, 0.2516552209854126, 0.4059615731239319, 0.22850975394248962, 0.31910884380340576, 0.271999716758728, 0.3609413802623749, 0.23329730331897736, 0.24340660870075226, 0.29828208684921265, 0.2987121343612671, 0.3573665916919708, 0.30429503321647644, 0.24745482206344604, 0.2671503722667694, 0.2533862292766571, 0.29402899742126465, 0.295828640460968, 0.24387094378471375, 0.3123572766780853, 0.28945088386535645, 0.16070139408111572, 0.2927072048187256, 0.3375946283340454, 0.241378515958786, 0.25145381689071655, 0.22545978426933289, 0.23887230455875397, 0.2572730779647827, 0.28785213828086853, 0.1752760112285614, 0.2608170509338379, 0.18745838105678558, 0.22321301698684692, 0.2763218283653259, 0.2886003851890564, 0.30199918150901794, 0.22971931099891663, 0.22339507937431335, 0.21274614334106445, 0.2925374507904053, 0.19977246224880219, 0.21814437210559845, 0.24911029636859894, 0.2597535252571106, 0.2943524122238159, 0.20898157358169556, 0.22510980069637299, 0.2509816586971283, 0.2832266688346863, 0.1713750958442688, 0.20090144872665405, 0.22578376531600952, 0.19224956631660461, 0.20942974090576172, 0.23276811838150024, 0.16266217827796936, 0.2251843810081482, 0.23468366265296936, 0.2597544491291046, 0.228120818734169, 0.3208269476890564, 0.25262683629989624, 0.2599206566810608, 0.16062653064727783, 0.2834988832473755, 0.21262240409851074, 0.209095299243927, 0.21141809225082397, 0.32054126262664795, 0.23552390933036804, 0.23546189069747925, 0.20099809765815735, 0.16645373404026031, 0.19065284729003906, 0.31484559178352356, 0.24682538211345673, 0.20079554617404938, 0.21360570192337036, 0.28010669350624084, 0.24698542058467865, 0.4395103454589844, 0.2217196524143219, 0.19284123182296753, 0.27120721340179443, 0.34576883912086487, 0.25727277994155884, 0.20853765308856964, 0.19959530234336853, 0.2231387495994568, 0.30113136768341064, 0.32839274406433105, 0.2318851798772812, 0.23823337256908417, 0.23946024477481842, 0.2006710171699524, 0.1730905920267105, 0.2910539209842682, 0.41176924109458923, 0.25533032417297363, 0.22470292448997498, 0.31162580847740173, 0.29840803146362305, 0.2741273045539856, 0.26334285736083984, 0.21894489228725433, 0.2617282569408417, 0.20979687571525574, 0.1813143789768219, 0.21426090598106384, 0.287161648273468, 0.2340514063835144, 0.2530955672264099, 0.2075069546699524, 0.2383265495300293, 0.29277706146240234, 0.2585148215293884, 0.3580446243286133, 0.27174052596092224, 0.22940894961357117, 0.19378331303596497, 0.2864968776702881, 0.18565335869789124, 0.19161148369312286, 0.2633703947067261, 0.2086859494447708, 0.2831187844276428, 0.258736252784729, 0.2045799195766449, 0.2589965760707855, 0.31159543991088867, 0.328622043132782, 0.27694135904312134, 0.22387048602104187, 0.19823716580867767, 0.24756571650505066, 0.26377245783805847, 0.2811926305294037, 0.17363181710243225, 0.17754074931144714, 0.21402031183242798, 0.24919292330741882, 0.2524609863758087, 0.3183521628379822, 0.3677509129047394, 0.18496090173721313, 0.19387146830558777, 0.2557925283908844, 0.2723628282546997, 0.2510155141353607, 0.26644161343574524, 0.23031049966812134, 0.2484557330608368, 0.20407646894454956, 0.19570639729499817, 0.14988771080970764, 0.21000558137893677, 0.13384753465652466, 0.24658742547035217, 0.1577543169260025, 0.28106793761253357, 0.26294174790382385, 0.20447617769241333, 0.22044767439365387, 0.3065113425254822, 0.17722702026367188, 0.16158932447433472, 0.1988251805305481, 0.21616223454475403, 0.15259794890880585, 0.30091145634651184, 0.21451881527900696, 0.22224120795726776, 0.1304636299610138, 0.1763506531715393, 0.23748977482318878, 0.18351280689239502, 0.25340741872787476, 0.16908049583435059, 0.20898982882499695, 0.29862236976623535, 0.1961374580860138, 0.3111453056335449, 0.23276343941688538, 0.2134237289428711, 0.1854190230369568, 0.2114543616771698, 0.2041134536266327, 0.20384183526039124, 0.22141483426094055, 0.23882460594177246, 0.170393168926239, 0.20840255916118622, 0.2615855932235718, 0.18355533480644226, 0.36461710929870605, 0.1591426283121109, 0.183224618434906, 0.12571758031845093, 0.2600359320640564, 0.17848685383796692, 0.26234230399131775, 0.18465733528137207, 0.1770423948764801, 0.19686955213546753, 0.25892770290374756, 0.161482036113739, 0.21850118041038513, 0.23738138377666473, 0.174992173910141, 0.15764959156513214, 0.27437594532966614, 0.1642238199710846, 0.2761017382144928, 0.1697794795036316, 0.2021561563014984, 0.16759732365608215, 0.18341955542564392, 0.18938025832176208, 0.1937989741563797, 0.2326931357383728, 0.1545715481042862, 0.2708216607570648, 0.17507454752922058, 0.14933165907859802, 0.22634464502334595, 0.20444688200950623, 0.23302975296974182, 0.16258762776851654, 0.15670761466026306, 0.28840139508247375, 0.2251097708940506, 0.1497500091791153, 0.20370709896087646, 0.22955487668514252, 0.1902121901512146, 0.20629429817199707, 0.23194631934165955, 0.18460211157798767, 0.16500845551490784, 0.20041346549987793, 0.2076006531715393, 0.23506569862365723, 0.1875646412372589, 0.19169166684150696, 0.16778412461280823, 0.20504316687583923, 0.19247213006019592, 0.1949065625667572, 0.18366645276546478, 0.2018280327320099, 0.19960051774978638, 0.24042971432209015, 0.151809960603714, 0.16615080833435059, 0.18123556673526764, 0.1520569920539856, 0.17564618587493896, 0.1438738852739334, 0.22348996996879578, 0.19035604596138, 0.19915446639060974, 0.1906922608613968, 0.18868428468704224, 0.18173524737358093, 0.21980339288711548, 0.18883267045021057, 0.17013972997665405, 0.18683503568172455, 0.21164382994174957, 0.12078613042831421, 0.21739482879638672, 0.16084584593772888, 0.1781194806098938, 0.19029273092746735, 0.25932013988494873, 0.21462124586105347, 0.2346593290567398, 0.2065226137638092, 0.1492299884557724, 0.17848461866378784, 0.19395238161087036, 0.169629767537117, 0.19099201261997223, 0.171796977519989, 0.15380121767520905, 0.1894749402999878, 0.173786461353302, 0.1624947488307953, 0.15333986282348633, 0.25521010160446167, 0.13747453689575195, 0.17540115118026733, 0.189353808760643, 0.20800243318080902, 0.22999149560928345, 0.19249442219734192, 0.23768475651741028, 0.1874961405992508, 0.19823837280273438, 0.16613033413887024, 0.215397447347641, 0.21709616482257843, 0.18845438957214355, 0.17010857164859772, 0.20726129412651062, 0.2645883858203888, 0.2217799425125122, 0.29322707653045654, 0.18574203550815582, 0.18751651048660278, 0.34468328952789307, 0.24180147051811218, 0.20830169320106506, 0.183218315243721, 0.19228586554527283, 0.21993762254714966, 0.19646736979484558, 0.2224033921957016, 0.30738115310668945, 0.1962634027004242, 0.08564354479312897, 0.24191814661026, 0.20330557227134705, 0.2153560370206833, 0.1753169596195221, 0.24730446934700012, 0.1495896577835083, 0.171847864985466, 0.1985698640346527, 0.13431978225708008, 0.18710941076278687, 0.18627285957336426, 0.14031429588794708, 0.21536847949028015, 0.19273173809051514, 0.1386066973209381, 0.24848683178424835, 0.18923509120941162, 0.20919401943683624, 0.12741298973560333, 0.2082548588514328, 0.1905437707901001, 0.1759144365787506, 0.1666644662618637, 0.1649562418460846, 0.1710657775402069, 0.16960474848747253, 0.23377792537212372, 0.18762406706809998, 0.21789801120758057, 0.1862579882144928, 0.24396446347236633, 0.2155223786830902, 0.3321189880371094, 0.2682495713233948, 0.3102368712425232, 0.13632752001285553, 0.22970867156982422, 0.1937919408082962, 0.2510274350643158, 0.1568993628025055, 0.23169642686843872, 0.24352294206619263, 0.17771904170513153, 0.2322387397289276, 0.22148433327674866, 0.16674116253852844, 0.18522807955741882, 0.24665138125419617, 0.20626616477966309, 0.2177392989397049, 0.203688383102417, 0.1995793581008911, 0.16014635562896729, 0.17248941957950592, 0.2588191032409668, 0.13908281922340393, 0.17629894614219666, 0.19591158628463745, 0.22421756386756897, 0.16044017672538757, 0.19752418994903564, 0.18054577708244324, 0.2049325704574585, 0.20506784319877625, 0.1502131223678589, 0.21923229098320007, 0.1804034411907196, 0.1675170361995697, 0.20850259065628052, 0.2050151526927948, 0.2875722050666809, 0.18653452396392822, 0.2072376161813736, 0.1457797884941101, 0.23263749480247498, 0.20403632521629333, 0.30959394574165344, 0.2107255905866623, 0.2256385236978531, 0.15314343571662903, 0.19024395942687988, 0.20764584839344025, 0.18889129161834717, 0.23994861543178558, 0.19271253049373627, 0.19215434789657593, 0.18697631359100342, 0.18259352445602417, 0.24197927117347717, 0.17156969010829926, 0.21808183193206787, 0.23368538916110992, 0.2257446050643921, 0.2092563807964325, 0.1910201907157898, 0.18258632719516754, 0.3298221230506897, 0.19496500492095947, 0.19426700472831726, 0.23297905921936035, 0.15855756402015686, 0.1852424293756485, 0.253670871257782, 0.17798849940299988, 0.21187657117843628, 0.20562048256397247, 0.16582846641540527, 0.17836102843284607, 0.206109881401062, 0.22167053818702698, 0.2003154158592224, 0.15629853308200836, 0.1521952599287033, 0.2037852704524994, 0.16676907241344452, 0.23668795824050903, 0.24059730768203735, 0.17978882789611816, 0.2108238935470581, 0.19912230968475342, 0.2003600001335144, 0.18451453745365143, 0.13383962213993073, 0.1341174989938736, 0.18405112624168396, 0.21960797905921936, 0.14697512984275818, 0.15666931867599487, 0.16834422945976257, 0.2251463234424591, 0.1875876933336258, 0.15864931046962738, 0.22661137580871582, 0.13635395467281342, 0.21329522132873535, 0.2150726020336151, 0.15925511717796326, 0.16541177034378052, 0.1805747151374817, 0.1580614149570465, 0.162927508354187, 0.16718736290931702, 0.15126711130142212, 0.19460749626159668, 0.15701380372047424, 0.251149445772171, 0.16865947842597961, 0.15364596247673035, 0.2776832580566406, 0.1442907750606537, 0.21134531497955322, 0.16712138056755066, 0.1353810727596283, 0.14554357528686523, 0.15699437260627747, 0.17990216612815857, 0.129336416721344, 0.16893544793128967, 0.14435991644859314, 0.12819014489650726, 0.13724713027477264, 0.17356765270233154, 0.16770589351654053, 0.12420099973678589, 0.16490358114242554, 0.16849301755428314, 0.12149610370397568, 0.24041923880577087, 0.17350800335407257, 0.12667623162269592, 0.13267523050308228, 0.14363715052604675, 0.18927200138568878, 0.1658223569393158, 0.1625780612230301, 0.1536216139793396, 0.15762127935886383, 0.17180097103118896, 0.1815544068813324, 0.19388312101364136, 0.1484730839729309, 0.13301058113574982, 0.16201496124267578, 0.15602093935012817, 0.1951034963130951, 0.16516385972499847, 0.11090092360973358, 0.17468976974487305, 0.1539759337902069, 0.19022737443447113, 0.1392901986837387, 0.2042117416858673, 0.18923074007034302, 0.16394557058811188, 0.20913395285606384, 0.21391484141349792, 0.14627154171466827, 0.1743411421775818, 0.13178318738937378, 0.19335100054740906, 0.1707773208618164, 0.1623956263065338, 0.1822521984577179, 0.1486840397119522, 0.1713988482952118, 0.13988527655601501, 0.17321717739105225, 0.17134489119052887, 0.18162375688552856, 0.16676115989685059, 0.11251711845397949, 0.21780434250831604, 0.14836403727531433, 0.1307952105998993, 0.1582086682319641, 0.11148452758789062, 0.14664599299430847, 0.20723628997802734, 0.1602526158094406, 0.20246167480945587, 0.2530284523963928, 0.19180691242218018, 0.16137748956680298, 0.2376861274242401, 0.19784781336784363, 0.178923100233078, 0.17510153353214264, 0.1922704577445984, 0.16003218293190002, 0.186112642288208, 0.17473864555358887, 0.16724099218845367, 0.1659155786037445, 0.1852942705154419, 0.18702375888824463, 0.16786272823810577, 0.16152530908584595, 0.17771683633327484, 0.17322197556495667, 0.17804524302482605, 0.22406920790672302, 0.16168132424354553, 0.17659685015678406, 0.2369508445262909, 0.22854620218276978, 0.2515947222709656, 0.24379760026931763, 0.15178485214710236, 0.19735784828662872, 0.157182976603508, 0.3929890990257263, 0.19340436160564423, 0.36863380670547485, 0.22663314640522003, 0.2506982088088989, 0.20315685868263245, 0.33714357018470764, 0.2546854019165039, 0.19446486234664917, 0.22348123788833618, 0.26252302527427673, 0.34744179248809814, 0.4166359603404999, 0.27925294637680054, 0.2573070526123047, 0.3612704873085022, 0.2577466070652008, 0.3833528757095337, 0.2614169716835022, 0.22544646263122559, 0.21002796292304993, 0.307411253452301, 0.31542104482650757, 0.2878153324127197, 0.7566683292388916, 0.34620118141174316, 0.6083210706710815, 0.24647080898284912, 0.2797798812389374, 0.31446370482444763, 0.3800159990787506, 0.3007061779499054, 0.44041359424591064, 0.8384516835212708, 0.3077883720397949, 0.289261132478714, 0.48400354385375977, 0.3106881380081177, 0.35624080896377563, 0.5787644386291504, 0.5080582499504089, 0.4601324796676636, 0.40271610021591187, 0.2682324945926666, 0.4481912851333618, 0.4486027956008911, 0.4229297637939453, 0.25247666239738464, 0.4112855792045593, 0.607181191444397, 0.3603973090648651, 0.31453099846839905, 0.39696529507637024, 0.5613085031509399, 0.4423871636390686, 0.49568837881088257, 0.30609452724456787, 0.311266154050827, 0.345685750246048, 0.31831443309783936, 0.29012906551361084, 0.2753315567970276, 0.49644210934638977, 0.3219943940639496, 0.4349712133407593, 0.3009856045246124, 0.27788567543029785, 0.3963789641857147, 0.3806755840778351, 0.4364762306213379, 0.18265333771705627, 0.37431392073631287, 0.4628639221191406, 0.31615909934043884, 0.3233693838119507, 0.3003932237625122, 0.3748939335346222, 0.527681827545166, 0.28119608759880066, 0.2868261933326721, 0.5809482336044312, 0.28869691491127014, 0.23715519905090332, 0.46000105142593384, 0.2831836938858032, 0.22276991605758667, 0.28890639543533325, 0.24152183532714844, 0.2083428055047989, 0.2651672065258026, 0.21845021843910217, 0.3756036162376404, 0.22799861431121826, 0.24541231989860535, 0.2063124179840088, 0.19104886054992676, 0.20960184931755066, 0.2619343101978302, 0.22639945149421692, 0.2532832622528076, 0.17411355674266815, 0.14130952954292297, 0.223935067653656, 0.19963310658931732, 0.17659235000610352, 0.19800584018230438, 0.19578483700752258, 0.21440881490707397, 0.1981809437274933, 0.20926707983016968, 0.16903111338615417, 0.2313266545534134, 0.19066496193408966, 0.16051709651947021, 0.1777035892009735, 0.2172214835882187, 0.18638190627098083, 0.15340298414230347, 0.16350898146629333, 0.1800192892551422, 0.16222696006298065, 0.20192134380340576, 0.15609365701675415, 0.21397477388381958, 0.11421921849250793, 0.1634775996208191, 0.1390531063079834, 0.1562911570072174, 0.18398448824882507, 0.17803514003753662, 0.17176738381385803, 0.157565176486969, 0.19681710004806519, 0.1858992576599121, 0.14881004393100739, 0.1803811639547348, 0.1904779076576233, 0.20547950267791748, 0.11771216988563538, 0.15641316771507263, 0.15739023685455322, 0.18558095395565033, 0.20258809626102448, 0.16475580632686615, 0.13970080018043518, 0.20095089077949524, 0.15629304945468903, 0.19143758714199066, 0.1293259859085083, 0.14113137125968933, 0.21058189868927002, 0.18594226241111755, 0.14262880384922028, 0.16039282083511353, 0.187188982963562, 0.1463620513677597, 0.19023346900939941, 0.270359605550766, 0.21357953548431396, 0.14882920682430267, 0.13867680728435516, 0.13665184378623962, 0.18453210592269897, 0.13363677263259888, 0.17448264360427856, 0.19469088315963745, 0.1408834606409073, 0.15850970149040222, 0.174041748046875, 0.16398227214813232, 0.11259284615516663, 0.13229750096797943, 0.11725358664989471, 0.16780705749988556, 0.12369334697723389, 0.1873662918806076, 0.14359909296035767, 0.13735419511795044, 0.19735978543758392, 0.10423386842012405, 0.13756392896175385, 0.08076030015945435, 0.19629627466201782, 0.22469472885131836, 0.13534663617610931, 0.12165969610214233, 0.15263661742210388, 0.14086899161338806, 0.17678585648536682, 0.09450343251228333, 0.19985780119895935, 0.20371320843696594, 0.1481868028640747, 0.18121680617332458, 0.15543153882026672, 0.1577359437942505, 0.20825991034507751, 0.20624032616615295, 0.15747755765914917, 0.2513987421989441, 0.16448426246643066, 0.21525084972381592, 0.18664102256298065, 0.14186140894889832, 0.1800229549407959, 0.1896347999572754, 0.15934345126152039, 0.15641455352306366, 0.13256466388702393, 0.138281911611557, 0.1645497977733612, 0.12465168535709381, 0.16198468208312988, 0.14523720741271973, 0.14777329564094543, 0.1616986095905304, 0.20114178955554962, 0.16790977120399475, 0.1503039300441742, 0.12008421868085861, 0.159682959318161, 0.21557408571243286, 0.13498398661613464, 0.11709144711494446, 0.17252829670906067, 0.08983151614665985, 0.21268489956855774, 0.15417343378067017, 0.2039228081703186, 0.16666346788406372, 0.11421939730644226, 0.13836340606212616, 0.18305149674415588, 0.16836506128311157, 0.18088695406913757, 0.12671589851379395, 0.1582127809524536, 0.11080145835876465, 0.13436678051948547, 0.1388111263513565, 0.1935276985168457, 0.145514577627182, 0.1364784836769104, 0.14071205258369446, 0.20142772793769836, 0.12196749448776245, 0.16120770573616028, 0.10905802249908447, 0.14121244847774506, 0.14334885776042938, 0.15930704772472382, 0.19594323635101318, 0.14020079374313354, 0.10643289983272552, 0.12638351321220398, 0.17613451182842255, 0.1354537308216095, 0.15855181217193604, 0.1143345981836319, 0.18318957090377808, 0.22047948837280273, 0.11140228807926178, 0.15070855617523193, 0.16137900948524475, 0.16935652494430542, 0.15319512784481049, 0.09660328924655914, 0.17272432148456573, 0.17909085750579834, 0.13128423690795898, 0.16127200424671173, 0.16603565216064453, 0.17513945698738098, 0.11130498349666595, 0.13144613802433014, 0.15966002643108368, 0.13601894676685333, 0.11009141802787781, 0.15798763930797577, 0.1017855554819107, 0.1672053039073944, 0.17671936750411987, 0.13497863709926605, 0.11321140825748444, 0.14283207058906555, 0.11853741109371185, 0.1505080759525299, 0.12416225671768188, 0.16187579929828644, 0.14518681168556213, 0.17433404922485352, 0.16915734112262726, 0.14192655682563782, 0.1923040747642517, 0.16226935386657715, 0.1565965712070465, 0.1373957097530365, 0.14671656489372253, 0.141363263130188, 0.16676080226898193, 0.10840099304914474, 0.14570960402488708, 0.13471901416778564, 0.17976996302604675, 0.1423693150281906, 0.12115944921970367, 0.17081546783447266, 0.08312501013278961, 0.1654565930366516, 0.1791449785232544, 0.15593695640563965, 0.17630675435066223, 0.13336636126041412, 0.1659022867679596, 0.10015571117401123, 0.12595272064208984, 0.14575740694999695, 0.13268812000751495, 0.14322713017463684, 0.10978478193283081, 0.11457391083240509, 0.14160580933094025, 0.1578705608844757, 0.11423289775848389, 0.07914645969867706, 0.1744006723165512, 0.17760872840881348, 0.09030140936374664, 0.14029690623283386, 0.1360495686531067, 0.15226870775222778, 0.13453856110572815, 0.11640620976686478, 0.12016257643699646, 0.1154576987028122, 0.1829029619693756, 0.10382585227489471, 0.13469888269901276, 0.16098669171333313, 0.11536924540996552, 0.1628476083278656, 0.17722530663013458, 0.1744125932455063, 0.09852414578199387, 0.1050812304019928, 0.19867412745952606, 0.14416596293449402, 0.10589899867773056, 0.22682535648345947, 0.17108124494552612, 0.129414901137352, 0.24184690415859222, 0.23418690264225006, 0.13372702896595, 0.18835604190826416, 0.14207439124584198, 0.16508710384368896, 0.12767982482910156, 0.13603509962558746, 0.17804370820522308, 0.13362134993076324, 0.11413167417049408, 0.1740049421787262, 0.21279165148735046, 0.1446356475353241, 0.1267501562833786, 0.11935576051473618, 0.23370447754859924, 0.11813540756702423, 0.14950141310691833, 0.1575218290090561, 0.17865273356437683, 0.12551863491535187, 0.14340490102767944, 0.14116552472114563, 0.10052186995744705, 0.12375818192958832, 0.10308749973773956, 0.12084324657917023, 0.23246721923351288, 0.1263085901737213, 0.19605469703674316, 0.1442311406135559, 0.13995224237442017, 0.1969822496175766, 0.16659453511238098, 0.15013785660266876, 0.1157093495130539, 0.15514743328094482, 0.150576651096344, 0.15101562440395355, 0.187723308801651, 0.15451733767986298, 0.15875473618507385, 0.11901314556598663, 0.15200528502464294, 0.16381987929344177, 0.13304701447486877, 0.13514092564582825, 0.17520739138126373, 0.12854702770709991, 0.1418013572692871, 0.1493515819311142, 0.16483789682388306, 0.22386126220226288, 0.11993694305419922, 0.13904723525047302, 0.18843665719032288, 0.13225869834423065, 0.12142616510391235, 0.22478723526000977, 0.2031204253435135, 0.33662688732147217, 0.17266768217086792, 0.20471146702766418, 0.16380132734775543, 0.1952434480190277, 0.1691095232963562, 0.18748319149017334, 0.24857833981513977, 0.25133180618286133, 0.18450719118118286, 0.21155068278312683, 0.1839628368616104, 0.13677576184272766, 0.15430948138237, 0.1933986246585846, 0.2006969451904297, 0.18132995069026947, 0.11323241889476776, 0.15720072388648987, 0.15545868873596191, 0.19714787602424622, 0.1679133176803589, 0.2391255497932434, 0.1716427505016327, 0.19047227501869202, 0.1542457640171051, 0.1811978965997696, 0.23557260632514954, 0.13605037331581116, 0.14908695220947266, 0.14716458320617676, 0.14692358672618866, 0.14234226942062378, 0.15555933117866516, 0.16287511587142944, 0.1212749034166336, 0.12668989598751068, 0.09108585119247437, 0.11236988753080368, 0.15362697839736938, 0.15378670394420624, 0.1437980830669403, 0.15776371955871582, 0.14546632766723633, 0.13665840029716492, 0.12789183855056763, 0.09784609824419022, 0.14128324389457703, 0.15950727462768555, 0.19100740551948547, 0.14414727687835693, 0.14304372668266296, 0.15468132495880127, 0.16283835470676422, 0.14342042803764343, 0.14688646793365479, 0.1960778534412384, 0.1367795318365097, 0.13607612252235413, 0.17432855069637299, 0.20378151535987854, 0.10165181010961533, 0.11444161087274551, 0.1575866937637329, 0.11700663715600967, 0.1594858169555664, 0.164738267660141, 0.23548585176467896, 0.19025494158267975, 0.153657928109169, 0.13780422508716583, 0.1484115719795227, 0.1559484452009201, 0.2278972864151001, 0.24166783690452576, 0.18892398476600647, 0.13050763309001923, 0.23387332260608673, 0.15177014470100403, 0.18291312456130981, 0.11752693355083466, 0.14540211856365204, 0.32283344864845276, 0.19905677437782288, 0.13173580169677734, 0.1391284465789795, 0.14211811125278473, 0.15621864795684814, 0.12311108410358429, 0.11610221117734909, 0.11697077751159668, 0.15240825712680817, 0.1452884078025818, 0.17048917710781097, 0.09716690331697464, 0.13104036450386047, 0.2029232680797577, 0.09831713140010834, 0.11590327322483063, 0.17300210893154144, 0.17237944900989532, 0.1152908205986023, 0.14521275460720062, 0.123070627450943, 0.1931072175502777, 0.14146943390369415, 0.14398914575576782, 0.11783448606729507, 0.11667066067457199, 0.09960610419511795, 0.1610652655363083, 0.1366969645023346, 0.1849389374256134, 0.1727747619152069, 0.1395014524459839, 0.18587416410446167, 0.0891507938504219, 0.11271724104881287, 0.13486480712890625, 0.15329492092132568, 0.15608486533164978, 0.14450979232788086, 0.15745370090007782, 0.11960166692733765, 0.15466563403606415, 0.15286383032798767, 0.16179388761520386, 0.14169855415821075, 0.12803372740745544, 0.19276005029678345, 0.17099922895431519, 0.15316611528396606, 0.13518790900707245, 0.10836309939622879, 0.14210771024227142, 0.14415951073169708, 0.16508495807647705, 0.15945225954055786, 0.09278604388237, 0.171329066157341, 0.08649766445159912, 0.15076500177383423, 0.10358652472496033, 0.14480143785476685, 0.15686923265457153, 0.13324497640132904, 0.13077063858509064, 0.12464462965726852, 0.1604282557964325, 0.13928526639938354, 0.11804550141096115, 0.10789729654788971, 0.12732581794261932, 0.12393920123577118, 0.121296226978302, 0.1281069666147232, 0.14697948098182678, 0.13812661170959473, 0.125965878367424, 0.1552916169166565, 0.1346825659275055, 0.11774338036775589, 0.13910695910453796, 0.12167680263519287, 0.08852198719978333, 0.10140956938266754, 0.11588852107524872, 0.14262980222702026, 0.14332859218120575, 0.10998563468456268, 0.14899824559688568, 0.12092384696006775, 0.1331833004951477, 0.18804562091827393, 0.1395464390516281, 0.13585330545902252, 0.13563647866249084, 0.1486309915781021, 0.1524217128753662, 0.19663697481155396, 0.11157311499118805, 0.13071560859680176, 0.07853469252586365, 0.10831388831138611, 0.1496693342924118, 0.10397516191005707, 0.13321411609649658, 0.14846935868263245, 0.10960298776626587, 0.14805766940116882, 0.13501834869384766, 0.11300913989543915, 0.15338072180747986, 0.12749169766902924, 0.15684333443641663, 0.1647060066461563, 0.1442117989063263, 0.12029287219047546, 0.1592007279396057, 0.11575092375278473, 0.17779392004013062, 0.1319512575864792, 0.12287037074565887, 0.12080160528421402, 0.11652995645999908, 0.4035957455635071, 0.10748328268527985, 0.20757099986076355, 0.14218127727508545, 0.1340741068124771, 0.1800326257944107, 0.23405010998249054, 0.26015084981918335, 0.15318776667118073, 0.13321146368980408, 0.10977191478013992, 0.09733779728412628, 0.2838801443576813, 0.1363058239221573, 0.22308388352394104, 0.15823876857757568, 0.1425229012966156, 0.12650536000728607, 0.19861364364624023, 0.11084889620542526, 0.2078189253807068, 0.20771408081054688, 0.13195565342903137, 0.2349766194820404, 0.17034168541431427, 0.13119402527809143, 0.1443571150302887, 0.14855627715587616, 0.21122172474861145, 0.20140962302684784, 0.17025825381278992, 0.1397925764322281, 0.13541395962238312, 0.23801547288894653, 0.1674303412437439, 0.2636977732181549, 0.18392908573150635, 0.20377640426158905, 0.11825307458639145, 0.08267416805028915, 0.17760781943798065, 0.13856497406959534, 0.16914719343185425, 0.16717562079429626, 0.15857654809951782, 0.1557924896478653, 0.3831038475036621, 0.16119523346424103, 0.16839227080345154, 0.27491751313209534, 0.27851402759552, 0.19751229882240295, 0.17943626642227173, 0.33575817942619324, 0.40365201234817505, 0.30091097950935364, 0.26418009400367737, 0.20258980989456177, 0.2176622450351715, 0.4358598589897156, 0.5558712482452393, 0.1505809873342514, 0.3799227476119995, 0.2225181758403778, 0.40329504013061523, 0.3809118866920471, 0.3946641683578491, 0.35021278262138367, 0.2388782501220703, 0.25423872470855713, 0.26848626136779785, 0.22778292000293732, 0.2786717116832733, 0.24201560020446777, 0.29706746339797974, 0.2243155539035797, 0.28281813859939575, 0.2697423994541168, 0.12931308150291443, 0.18250004947185516, 0.1406630575656891, 0.23663625121116638, 0.1673736870288849, 0.2024572789669037, 0.2006358504295349, 0.17567062377929688, 0.18772412836551666, 0.2377953976392746, 0.20565356314182281, 0.19113804399967194, 0.14983424544334412, 0.15309956669807434, 0.3011769652366638, 0.16929177939891815, 0.13473457098007202, 0.2273971289396286, 0.2091657519340515, 0.2869586646556854, 0.1893613040447235, 0.1944712996482849, 0.36104264855384827, 0.28912100195884705, 0.23623046278953552, 0.23149065673351288, 0.4057806432247162, 0.33902543783187866, 0.27214083075523376, 0.2315470278263092, 0.14104551076889038, 0.22550374269485474, 0.3757362365722656, 0.18396660685539246, 0.32663995027542114, 0.25581130385398865, 0.47937220335006714, 0.3602263033390045, 0.24346423149108887, 0.1942199319601059, 0.16507798433303833, 0.5086200833320618, 0.30288878083229065, 0.5197147130966187, 0.32886171340942383, 0.4025726914405823, 0.4252374768257141, 0.40338778495788574, 0.13790994882583618, 0.2032012641429901, 0.28163638710975647, 0.2914988100528717, 0.2669684886932373, 0.21915385127067566, 0.3791928291320801, 0.1884390115737915, 0.27609825134277344, 0.3212822377681732, 0.22567182779312134, 0.21091139316558838, 0.3365423381328583, 0.21283774077892303, 0.27947068214416504, 0.1974060982465744, 0.23589929938316345, 0.21442854404449463, 0.2643011510372162, 0.20283932983875275, 0.2429690808057785, 0.1870305985212326, 0.16606959700584412, 0.2580127716064453, 0.13391701877117157, 0.17571958899497986, 0.2646089196205139, 0.21857517957687378, 0.17441615462303162, 0.14087462425231934, 0.2552827298641205, 0.18972592055797577, 0.1892865002155304, 0.26504915952682495, 0.21818649768829346, 0.13763712346553802, 0.18104609847068787, 0.20297493040561676, 0.21921652555465698, 0.1778460443019867, 0.1729755848646164, 0.1962289810180664, 0.17134083807468414, 0.16956111788749695, 0.15263321995735168, 0.180885910987854, 0.1462811380624771, 0.18988463282585144, 0.16863030195236206, 0.14012739062309265, 0.15684929490089417, 0.13159720599651337, 0.11522930860519409, 0.16307014226913452, 0.1479557603597641, 0.15675896406173706, 0.23054848611354828, 0.2033861130475998, 0.08750756829977036, 0.19681942462921143, 0.18653059005737305, 0.1521594524383545, 0.13570134341716766, 0.14303025603294373, 0.17398199439048767, 0.13486339151859283, 0.17528170347213745, 0.08336183428764343, 0.13464118540287018, 0.1731269210577011, 0.11467675864696503, 0.1041346937417984, 0.15404237806797028, 0.14376558363437653, 0.13206981122493744, 0.09771868586540222, 0.14950920641422272, 0.13183102011680603, 0.1371513456106186, 0.158030167222023, 0.08450094610452652, 0.13896086812019348, 0.12365131080150604, 0.14180949330329895, 0.12498864531517029, 0.11559131741523743, 0.13080468773841858, 0.12958043813705444, 0.13337019085884094, 0.10305994749069214, 0.11365294456481934, 0.12924757599830627, 0.13664385676383972, 0.11124062538146973, 0.1620575189590454, 0.15476346015930176, 0.09116530418395996, 0.12137013673782349, 0.12011240422725677, 0.14210067689418793, 0.14140941202640533, 0.09698319435119629, 0.0871666818857193, 0.12512031197547913, 0.14465098083019257, 0.12731483578681946, 0.11298315227031708, 0.12471427023410797, 0.16794736683368683, 0.09842946380376816, 0.10996322333812714, 0.10392111539840698, 0.12720105051994324, 0.09227988123893738, 0.09083245694637299, 0.1368103176355362, 0.12637095153331757, 0.10943403840065002, 0.11379273235797882, 0.09098315238952637, 0.11587516963481903, 0.1220175251364708, 0.17519786953926086, 0.17232130467891693, 0.12512587010860443, 0.12515804171562195, 0.11408568173646927, 0.12810054421424866, 0.16796749830245972, 0.15653938055038452, 0.1403542459011078, 0.17312851548194885, 0.13031885027885437, 0.09295960515737534, 0.09434163570404053, 0.09675821661949158, 0.14936639368534088, 0.10940896719694138, 0.1497168242931366, 0.10330955684185028, 0.11768653243780136, 0.11013802886009216, 0.1196136623620987, 0.12854023277759552, 0.12163880467414856, 0.10223571956157684, 0.12520718574523926, 0.20372071862220764, 0.17087338864803314, 0.15994322299957275, 0.1260925531387329, 0.10868135839700699, 0.19894877076148987, 0.13194313645362854, 0.11518733948469162, 0.20433774590492249, 0.11840826272964478, 0.14008411765098572, 0.12545426189899445, 0.3946707844734192, 0.1248781830072403, 0.10360180586576462, 0.1493780016899109, 0.16467662155628204, 0.13830220699310303, 0.17711102962493896, 0.09921148419380188, 0.1483445018529892, 0.14520063996315002, 0.11653649061918259, 0.11383974552154541, 0.12690778076648712, 0.14287163317203522, 0.16388966143131256, 0.14650431275367737, 0.0916433036327362, 0.16153264045715332, 0.10044331103563309, 0.12579277157783508, 0.13368728756904602, 0.1117749959230423, 0.1408856213092804, 0.09074655175209045, 0.1352538764476776, 0.10588930547237396, 0.18720455467700958, 0.13303469121456146, 0.1813063621520996, 0.1390794813632965, 0.1454279124736786, 0.10851775109767914, 0.11794266104698181, 0.1255425065755844, 0.18017402291297913, 0.13059477508068085, 0.11463570594787598, 0.088219553232193, 0.19003835320472717, 0.22133144736289978, 0.10578272491693497, 0.19828888773918152, 0.11053329706192017, 0.1082085594534874, 0.12983982264995575, 0.14958694577217102, 0.14494477212429047, 0.11011288315057755, 0.18102553486824036, 0.1411903202533722, 0.1737237274646759, 0.16762736439704895, 0.125789612531662, 0.09223856776952744, 0.128378763794899, 0.12866374850273132, 0.18227581679821014, 0.15280687808990479, 0.12170135974884033, 0.12852799892425537, 0.11481638252735138, 0.13149069249629974, 0.1725589632987976, 0.08923135697841644, 0.11069084703922272, 0.11151233315467834, 0.13794541358947754, 0.13967680931091309, 0.1365593522787094, 0.2117691934108734, 0.1193646565079689, 0.12932470440864563, 0.15168461203575134, 0.11387322843074799, 0.19676430523395538, 0.14325225353240967, 0.11770673096179962, 0.12277968227863312, 0.19473205506801605, 0.1367194801568985, 0.09308524429798126, 0.12916553020477295, 0.11002374440431595, 0.09262911975383759, 0.09651201218366623, 0.17354930937290192, 0.1601494550704956, 0.11075198650360107, 0.12444876879453659, 0.13071748614311218, 0.13922899961471558, 0.1323995441198349, 0.1411549299955368, 0.11079790443181992, 0.15174239873886108, 0.09710700809955597, 0.09262048453092575, 0.10412280261516571, 0.13920798897743225, 0.08470909297466278, 0.12114521116018295, 0.1595858335494995, 0.15024670958518982, 0.10812442004680634, 0.09767157584428787, 0.08231285959482193, 0.07883903384208679, 0.14163020253181458, 0.1528211385011673, 0.10428144037723541, 0.14323894679546356, 0.17981041967868805, 0.1276814341545105, 0.14692997932434082, 0.12707149982452393, 0.1495848000049591, 0.11766007542610168, 0.1361714005470276, 0.13168799877166748, 0.09056193381547928, 0.11015044152736664, 0.19493785500526428, 0.10150618851184845, 0.22956646978855133, 0.22153541445732117, 0.1390867531299591, 0.13327130675315857, 0.1504497528076172, 0.17065593600273132, 0.12653829157352448, 0.14695629477500916, 0.19510608911514282, 0.18601760268211365, 0.16261576116085052, 0.1269107162952423, 0.15924106538295746, 0.11301153898239136, 0.13711583614349365, 0.16516560316085815, 0.12642598152160645, 0.11177694797515869, 0.12140288949012756, 0.23194652795791626, 0.12860973179340363, 0.11139124631881714, 0.124666228890419, 0.10888557881116867, 0.1110503226518631, 0.1693701148033142, 0.0890393927693367, 0.14599844813346863, 0.10413989424705505, 0.14452779293060303, 0.13875353336334229, 0.12767942249774933, 0.11062313616275787, 0.06859651952981949, 0.08897475153207779, 0.09120488166809082, 0.09379133582115173, 0.11043970286846161, 0.128435879945755, 0.1107913926243782, 0.08368908613920212, 0.14342612028121948, 0.13566343486309052, 0.12283430993556976, 0.09902147948741913, 0.11814230680465698, 0.11000307649374008, 0.1332884579896927, 0.16271251440048218, 0.16052652895450592, 0.11227506399154663, 0.14497974514961243, 0.1270744353532791, 0.15390047430992126, 0.1680537760257721, 0.13361993432044983, 0.11572089791297913, 0.13033974170684814, 0.14052732288837433, 0.10808278620243073, 0.1176978051662445, 0.12554217875003815, 0.1359233856201172, 0.12931832671165466, 0.10644160211086273, 0.115762859582901, 0.11394089460372925, 0.12924329936504364, 0.1029278039932251, 0.11749455332756042, 0.1348249316215515, 0.0974406898021698, 0.12364420294761658, 0.08424529433250427, 0.12783172726631165, 0.11913114786148071, 0.09927599877119064, 0.1146506667137146, 0.1247175857424736, 0.1042441800236702, 0.12066277116537094, 0.07670394331216812, 0.16363313794136047, 0.12447894364595413, 0.08230345696210861, 0.1261730045080185, 0.15613211691379547, 0.10193832963705063, 0.09265124797821045, 0.14757898449897766, 0.10705219209194183, 0.08421384543180466, 0.10221836715936661, 0.10080163180828094, 0.10722319781780243, 0.12657926976680756, 0.12559542059898376, 0.13479475677013397, 0.15149222314357758, 0.10114992409944534, 0.13880807161331177, 0.1061328575015068, 0.10897215455770493, 0.09429770708084106, 0.13787585496902466, 0.1217455267906189, 0.08711685240268707, 0.130044624209404, 0.13394992053508759, 0.12624311447143555, 0.13899502158164978, 0.09438508749008179, 0.12765078246593475, 0.10098925977945328, 0.11989317834377289, 0.11885213106870651, 0.11482217907905579, 0.10321719199419022, 0.09919870644807816, 0.07996843010187149, 0.13272957503795624, 0.08565682917833328, 0.08784899115562439, 0.11163260787725449, 0.08486217260360718, 0.12475424259901047, 0.09321833401918411, 0.10716021060943604, 0.05431456118822098, 0.09461355954408646, 0.09012970328330994, 0.13838306069374084, 0.11984477192163467, 0.09893717616796494, 0.14146676659584045, 0.1317708045244217, 0.09545809775590897, 0.0939139649271965, 0.09492494910955429, 0.10026580095291138, 0.1118190810084343, 0.1061248853802681, 0.15867304801940918, 0.15151572227478027, 0.15357159078121185, 0.09230437129735947, 0.13917618989944458, 0.10622760653495789, 0.09241068363189697, 0.11446326971054077, 0.06830358505249023, 0.10338787734508514, 0.09136676043272018, 0.07313007116317749, 0.10785721242427826, 0.13568516075611115, 0.10251788794994354, 0.09416879713535309, 0.11708353459835052, 0.0990394577383995, 0.07386292517185211, 0.11125652492046356, 0.11752776056528091, 0.1233278214931488, 0.09441864490509033, 0.12669718265533447, 0.10458597540855408, 0.10319672524929047, 0.07835302501916885, 0.11023055016994476, 0.09804391860961914, 0.1126803457736969, 0.1296125203371048, 0.12228044867515564, 0.1281212568283081, 0.11165700107812881, 0.12463921308517456, 0.11695264279842377, 0.09004812687635422, 0.08106324821710587, 0.07158458977937698, 0.07996941357851028, 0.10611514747142792, 0.2534218430519104, 0.15817353129386902, 0.1004243865609169, 0.08459827303886414, 0.11373654007911682, 0.08350294828414917, 0.0735066756606102, 0.10365229845046997, 0.09165643155574799, 0.12009210884571075, 0.08632344007492065, 0.10570340603590012, 0.12759578227996826, 0.07305556535720825, 0.06810431182384491, 0.07137705385684967, 0.15107376873493195, 0.08074414730072021, 0.09031888842582703, 0.10421504825353622, 0.10218474268913269, 0.09393276274204254, 0.11680246144533157, 0.11786742508411407, 0.15174293518066406, 0.06944476068019867, 0.08253142982721329, 0.051907237619161606, 0.07915352284908295, 0.1079908162355423, 0.11999816447496414, 0.11140340566635132, 0.11765425652265549, 0.09368374943733215, 0.10413099080324173, 0.11019984632730484, 0.10744466632604599, 0.14342935383319855, 0.08880660682916641, 0.10397416353225708, 0.07396620512008667, 0.08760474622249603, 0.11826035380363464, 0.10025812685489655, 0.17883780598640442, 0.10229098796844482, 0.13227593898773193, 0.1745237410068512, 0.09482762217521667, 0.12100113928318024, 0.08418454229831696, 0.10611678659915924, 0.11731234937906265, 0.13756327331066132, 0.13491617143154144, 0.12256255745887756, 0.11358633637428284, 0.11178722977638245, 0.10313265025615692, 0.15731260180473328, 0.13080528378486633, 0.10055296123027802, 0.0726328045129776, 0.11233240365982056, 0.11856117844581604, 0.07086621969938278, 0.08366619050502777, 0.08693625777959824, 0.11011388897895813, 0.13023196160793304, 0.12495923787355423, 0.11370525509119034, 0.08583617955446243, 0.07548604160547256, 0.09799443185329437, 0.14010290801525116, 0.09939080476760864, 0.10807690024375916, 0.09401246905326843, 0.0802656039595604, 0.12941046059131622, 0.09797894209623337, 0.12851488590240479, 0.09414727985858917, 0.10423082858324051, 0.1345519721508026, 0.09323355555534363, 0.10550616681575775, 0.09826973080635071, 0.13055503368377686, 0.11167098581790924, 0.15494081377983093, 0.0483240932226181, 0.11926066130399704, 0.14340898394584656, 0.16397954523563385, 0.10554181039333344, 0.11621266603469849, 0.08905098587274551, 0.07689295709133148, 0.12684401869773865, 0.1846221685409546, 0.12806010246276855, 0.10383617877960205, 0.08754384517669678, 0.1456083059310913, 0.1323358714580536, 0.0955260843038559, 0.13006307184696198, 0.12501199543476105, 0.11553379893302917, 0.09769453853368759, 0.1231144368648529, 0.14204752445220947, 0.11652243137359619, 0.11400651931762695, 0.17809167504310608, 0.14942282438278198, 0.09837697446346283, 0.19837695360183716, 0.2704910933971405, 0.10277439653873444, 0.13000236451625824, 0.12420141696929932, 0.11731225252151489, 0.148268461227417, 0.11967883259057999, 0.258375346660614, 0.14417949318885803, 0.13143527507781982, 0.09697331488132477, 0.13058681786060333, 0.22475889325141907, 0.25460103154182434, 0.18630456924438477, 0.26788240671157837, 0.3778345286846161, 0.11293234676122665, 0.23039063811302185, 0.18878291547298431, 0.2717757523059845, 0.36613017320632935, 0.12957698106765747, 0.1665194034576416, 0.32166755199432373, 0.2665475010871887, 0.34603002667427063, 0.3097810447216034, 0.34752508997917175, 0.21628335118293762, 0.18167872726917267, 0.42491579055786133, 0.39867615699768066, 0.24591486155986786, 0.3269723653793335, 0.23143655061721802, 0.43993625044822693, 0.2653823494911194, 0.27572691440582275, 0.17784026265144348, 0.1772216558456421, 0.2428254783153534, 0.2272799164056778, 0.21924349665641785, 0.32566291093826294, 0.17032551765441895, 0.23454052209854126, 0.15176966786384583, 0.21996857225894928, 0.19767925143241882, 0.15771973133087158, 0.21584084630012512, 0.21671459078788757, 0.26143568754196167, 0.213797926902771, 0.25524160265922546, 0.2403220534324646, 0.14238713681697845, 0.12430894374847412, 0.18742908537387848, 0.17925068736076355, 0.14927677810192108, 0.21143725514411926, 0.21057245135307312, 0.18403010070323944, 0.18445232510566711, 0.1846827268600464, 0.1510128676891327, 0.23026379942893982, 0.19627881050109863, 0.1403256207704544, 0.18421044945716858, 0.14906233549118042, 0.15298275649547577, 0.176290363073349, 0.13583378493785858, 0.18220986425876617, 0.14980049431324005, 0.13040438294410706, 0.10536204278469086, 0.1343221366405487, 0.1609029471874237, 0.24938777089118958, 0.14896130561828613, 0.12509027123451233, 0.12732504308223724, 0.12660139799118042, 0.13729538023471832, 0.11747948825359344, 0.11441676318645477, 0.10471997410058975, 0.19351065158843994, 0.11119069159030914, 0.13597974181175232, 0.1249583512544632, 0.2167394459247589, 0.14532926678657532, 0.17395569384098053, 0.2192595899105072, 0.12664642930030823, 0.25416699051856995, 0.18201269209384918, 0.13623785972595215, 0.23702740669250488, 0.1443130224943161, 0.22102732956409454, 0.14685426652431488, 0.15011140704154968, 0.09588128328323364, 0.16247130930423737, 0.20380079746246338, 0.16855594515800476, 0.11049249768257141, 0.16064351797103882, 0.1088319718837738, 0.12123502045869827, 0.23157742619514465, 0.1384953260421753, 0.09965378046035767, 0.16390782594680786, 0.1320711225271225, 0.16456778347492218, 0.15427729487419128, 0.15165874361991882, 0.1737491935491562, 0.1455155909061432, 0.08594447374343872, 0.12895467877388, 0.12170723080635071, 0.1368599832057953, 0.10481962561607361, 0.12849374115467072, 0.1800350546836853, 0.1275826096534729, 0.1257539689540863, 0.16008833050727844, 0.17523735761642456, 0.0997544676065445, 0.14550703763961792, 0.1918538212776184, 0.13439694046974182, 0.15698567032814026, 0.13964518904685974, 0.11138944327831268, 0.14182522892951965, 0.16318152844905853, 0.16204829514026642, 0.15446794033050537, 0.09604662656784058, 0.12408675253391266, 0.1027347594499588, 0.16144460439682007, 0.08186037838459015, 0.1603994369506836, 0.10023344308137894, 0.11894680559635162, 0.11248701810836792, 0.12000928819179535, 0.10848992317914963, 0.0816100463271141, 0.13521897792816162, 0.1424093246459961, 0.1400439739227295, 0.09424208104610443, 0.12135982513427734, 0.09279078245162964, 0.10819121450185776, 0.12063510715961456, 0.08533842861652374, 0.1115371361374855, 0.10423862934112549, 0.12634176015853882, 0.07296739518642426, 0.18221428990364075, 0.1071261316537857, 0.13996614515781403, 0.11836446821689606, 0.1568181812763214, 0.1029881089925766, 0.1734207272529602, 0.11179492622613907, 0.11302441358566284, 0.10996116697788239, 0.11797881871461868, 0.12086857110261917, 0.15235267579555511, 0.13920378684997559, 0.16574950516223907, 0.06913343816995621, 0.07856009900569916, 0.12142064422369003, 0.08701920509338379, 0.08727344870567322, 0.0948130190372467, 0.1345280557870865, 0.1711997389793396, 0.06459078192710876, 0.11620137095451355, 0.12058953940868378, 0.09799760580062866, 0.09328969568014145, 0.16562595963478088, 0.08925479650497437, 0.11804170906543732, 0.1047324538230896, 0.14140447974205017, 0.12393412739038467, 0.11965727806091309, 0.08986727893352509, 0.12249365448951721, 0.0797131136059761, 0.09339249134063721, 0.14953859150409698, 0.06365542858839035, 0.12317684292793274, 0.1019030213356018, 0.07924337685108185, 0.17328590154647827, 0.160760298371315, 0.12735119462013245, 0.12117858976125717, 0.12741363048553467, 0.10649999976158142, 0.10321398079395294, 0.08751696348190308, 0.09817904978990555, 0.17579397559165955, 0.11646226048469543, 0.15082626044750214, 0.14380094408988953, 0.14594492316246033, 0.11119851469993591, 0.157783642411232, 0.20381641387939453, 0.14519058167934418, 0.12482964992523193, 0.1094907596707344, 0.15814918279647827, 0.15970765054225922, 0.16063210368156433, 0.10925597697496414, 0.14363902807235718, 0.10051142424345016, 0.13763855397701263, 0.14774833619594574, 0.1512208878993988, 0.13052256405353546, 0.16067734360694885, 0.25977402925491333, 0.17184171080589294, 0.15542954206466675, 0.16652347147464752, 0.1347169280052185, 0.10828590393066406, 0.11367735266685486, 0.12320128083229065, 0.16120898723602295, 0.10333137214183807, 0.14199703931808472, 0.145480215549469, 0.08347971737384796, 0.12517747282981873, 0.14414694905281067, 0.15834149718284607, 0.11360198259353638, 0.10206975787878036, 0.14411315321922302, 0.13869944214820862, 0.10783005505800247, 0.08886805921792984, 0.08873867243528366, 0.09432615339756012, 0.1674441695213318, 0.1588679850101471, 0.15149015188217163, 0.11261366307735443, 0.07674900442361832, 0.1465529501438141, 0.14850153028964996, 0.07841398566961288, 0.1755271553993225, 0.17605164647102356, 0.11907079815864563, 0.14278267323970795, 0.12187698483467102, 0.21549242734909058, 0.2676298916339874, 0.11701831221580505, 0.14970804750919342, 0.11062857508659363, 0.22398585081100464, 0.26403993368148804, 0.4199107885360718, 0.2429797649383545, 0.14380773901939392, 0.2627592980861664, 0.654637336730957, 0.9977423548698425, 0.23570644855499268, 0.21700423955917358, 0.14655324816703796, 0.4792235493659973, 0.5913019776344299, 0.2356828898191452, 0.2555980086326599, 0.9458914399147034, 0.2162473499774933, 0.40330928564071655, 0.39121362566947937, 0.2987157106399536, 0.17183908820152283, 0.47878584265708923, 0.3562171757221222, 0.34273838996887207, 0.32100290060043335, 0.3641004264354706, 0.23283880949020386, 0.29297253489494324, 0.3783726096153259, 0.25754284858703613, 0.16712048649787903, 0.22726598381996155, 0.5228936672210693, 0.1492280662059784, 0.2706223726272583, 0.32678112387657166, 0.2280365377664566, 0.2654401957988739, 0.16677486896514893, 0.19083251059055328, 0.29765546321868896, 0.3603075444698334, 0.24574699997901917, 0.1716616004705429, 0.1948564648628235, 0.22528384625911713, 0.22421808540821075, 0.2246987670660019, 0.23752768337726593, 0.3048223853111267, 0.20551320910453796, 0.19056743383407593, 0.15528808534145355, 0.20345383882522583, 0.2260468304157257, 0.17117714881896973, 0.1783079355955124, 0.1645059585571289, 0.11722704768180847, 0.13641603291034698, 0.14512301981449127, 0.12400364875793457, 0.1320626437664032, 0.11891166865825653, 0.1798119843006134, 0.11529752612113953, 0.13140088319778442, 0.11194276809692383, 0.12433311343193054, 0.16806074976921082, 0.12486986070871353, 0.13151025772094727, 0.12252342700958252, 0.29302310943603516, 0.13271737098693848, 0.2061789631843567, 0.18144501745700836, 0.3750002384185791, 0.13348278403282166, 0.1617628037929535, 0.120025634765625, 0.17174986004829407, 0.2465115189552307, 0.1678142547607422, 0.1595870405435562, 0.19928482174873352, 0.10953651368618011, 0.14574065804481506, 0.13789156079292297, 0.11476439237594604, 0.13175281882286072, 0.11495158821344376, 0.1558886021375656, 0.12446717917919159, 0.10788394510746002, 0.1515447199344635, 0.12473209947347641, 0.1516483575105667, 0.12491485476493835, 0.16260629892349243, 0.11504319310188293, 0.16096852719783783, 0.13135838508605957, 0.11562405526638031, 0.1243801862001419, 0.12403592467308044, 0.10864806175231934, 0.1407410353422165, 0.08614031225442886, 0.09228388965129852, 0.08568071573972702, 0.13594968616962433, 0.10333552956581116, 0.14160838723182678, 0.12812666594982147, 0.10693681985139847, 0.1592331975698471, 0.10859905183315277, 0.0994369387626648, 0.10148970782756805, 0.10410577058792114, 0.12992452085018158, 0.12897194921970367, 0.1026587188243866, 0.12309492379426956, 0.09793082624673843, 0.07520115375518799, 0.2117636501789093, 0.22393956780433655, 0.13638561964035034, 0.2419913113117218, 0.2264140248298645, 0.15925757586956024, 0.17058399319648743, 0.1654886156320572, 0.15084365010261536, 0.40306341648101807, 0.4268227219581604, 0.1468736231327057, 0.12154002487659454, 0.19274307787418365, 0.24029114842414856, 0.20570099353790283, 0.3963727653026581, 0.18577802181243896, 0.2562372386455536, 0.1890152096748352, 0.2923751771450043, 0.3887600898742676, 0.21530675888061523, 0.16536368429660797, 0.22141891717910767, 0.3612910509109497, 0.29547831416130066, 0.18770578503608704, 0.25245600938796997, 0.17861270904541016, 0.13567377626895905, 0.1529308557510376, 0.35670751333236694, 0.24255330860614777, 0.30719417333602905, 0.19770433008670807, 0.2504686713218689, 0.18165606260299683, 0.22901709377765656, 0.15470799803733826, 0.15017452836036682, 0.19251330196857452, 0.20440129935741425, 0.20663520693778992, 0.22139784693717957, 0.20435892045497894, 0.21935299038887024, 0.17662838101387024, 0.17026707530021667, 0.16295713186264038, 0.21045739948749542, 0.13617077469825745, 0.14311492443084717, 0.16807660460472107, 0.13031598925590515, 0.13627631962299347, 0.14358921349048615, 0.18457558751106262, 0.15153548121452332, 0.14956340193748474, 0.15078097581863403, 0.17691442370414734, 0.15807852149009705, 0.1381409764289856, 0.11576787382364273, 0.16407203674316406, 0.14210660755634308, 0.2460050880908966, 0.11975932121276855, 0.13038384914398193, 0.11688639223575592, 0.10130944103002548, 0.16443224251270294, 0.14105340838432312, 0.12502656877040863, 0.1524999439716339, 0.14688655734062195, 0.1578081250190735, 0.11971477419137955, 0.08360940217971802, 0.12908968329429626, 0.11927545070648193, 0.08284889161586761, 0.07897791266441345, 0.11550964415073395, 0.13876032829284668, 0.10129116475582123, 0.11500303447246552, 0.14688406884670258, 0.12035536766052246, 0.13082385063171387, 0.13641926646232605, 0.10177753865718842, 0.11289627850055695, 0.11759644746780396, 0.08856885135173798, 0.09629766643047333, 0.11001026630401611, 0.1443941295146942, 0.1422286033630371, 0.12404313683509827, 0.15800125896930695, 0.13213256001472473, 0.116029754281044, 0.10465368628501892, 0.12159399688243866, 0.11133566498756409, 0.13515996932983398, 0.14964884519577026, 0.07918307930231094, 0.09911338984966278, 0.16049432754516602, 0.10749772191047668, 0.13348372280597687, 0.0956096202135086, 0.09900536388158798, 0.1146509200334549, 0.08331439644098282, 0.09024711698293686, 0.10170867294073105, 0.10643503069877625, 0.11638079583644867, 0.14200888574123383, 0.13301719725131989, 0.116530641913414, 0.09503071755170822, 0.10933887213468552, 0.09098092466592789, 0.13085971772670746, 0.12191764265298843, 0.09937175363302231, 0.07634707540273666, 0.0972243994474411, 0.10492212325334549, 0.08506740629673004, 0.09352771937847137, 0.14359772205352783, 0.1234058290719986, 0.12820957601070404, 0.11260779201984406, 0.10244473069906235, 0.0969049483537674, 0.05735195428133011, 0.07373020052909851, 0.09528720378875732, 0.1511935591697693, 0.10315141081809998, 0.10361278057098389, 0.09874328970909119, 0.14432691037654877, 0.09481144696474075, 0.11663639545440674, 0.0860576406121254, 0.10502365231513977, 0.10932234674692154, 0.09862476587295532, 0.14697372913360596, 0.11277740448713303, 0.08813096582889557, 0.11474868655204773, 0.08626478910446167, 0.12694795429706573, 0.14525854587554932, 0.10397490859031677, 0.08374529331922531, 0.1099030077457428, 0.1132287085056305, 0.08427199721336365, 0.14503923058509827, 0.08323683589696884, 0.11210799962282181, 0.12401838600635529, 0.09336254000663757, 0.07480488717556, 0.08366668224334717, 0.08653713017702103, 0.11655613780021667, 0.12954188883304596, 0.0889340192079544, 0.10154671967029572, 0.08575938642024994, 0.10989484190940857, 0.08632002025842667, 0.14179308712482452, 0.08551855385303497, 0.07249821722507477, 0.06982126832008362, 0.08254937827587128, 0.12253181636333466, 0.10617274045944214, 0.11632287502288818, 0.07902887463569641, 0.16295135021209717, 0.11472748219966888, 0.1008136123418808, 0.09299452602863312, 0.09718713164329529, 0.09259938448667526, 0.12178286910057068, 0.10250696539878845, 0.11445990204811096, 0.10792358219623566, 0.09338986873626709, 0.08821110427379608, 0.14534716308116913, 0.12840166687965393, 0.09684748947620392, 0.10924128443002701, 0.13386081159114838, 0.12432652711868286, 0.1268644481897354, 0.10989726334810257, 0.12426993250846863, 0.14256903529167175, 0.050297483801841736, 0.12433157861232758, 0.1034885123372078, 0.08424830436706543, 0.11514651030302048, 0.11429151147603989, 0.09955935925245285, 0.1093185618519783, 0.1128835529088974, 0.11422629654407501, 0.17448556423187256, 0.08387212455272675, 0.12512385845184326, 0.10394778847694397, 0.1482352614402771, 0.15958434343338013, 0.11375938355922699, 0.13187657296657562, 0.19085679948329926, 0.12719911336898804, 0.14110146462917328, 0.08186466246843338, 0.18052127957344055, 0.14514872431755066, 0.15543359518051147, 0.08155558258295059, 0.07580272108316422, 0.12146752327680588, 0.06815478205680847, 0.09837482869625092, 0.1452617198228836, 0.12365701794624329, 0.16978806257247925, 0.14148253202438354, 0.10780784487724304, 0.08696753531694412, 0.14449985325336456, 0.07015851140022278, 0.12387847900390625, 0.11334038525819778, 0.11427786946296692, 0.12075695395469666, 0.09282593429088593, 0.11252321302890778, 0.12018769979476929, 0.08707854896783829, 0.08462411165237427, 0.12253980338573456, 0.13981439173221588, 0.11198532581329346, 0.11080959439277649, 0.06988922506570816, 0.13420584797859192, 0.19698184728622437, 0.13191455602645874, 0.12253253161907196, 0.19132333993911743, 0.17371827363967896, 0.09791739284992218, 0.13414843380451202, 0.1862516850233078, 0.14804303646087646, 0.1471431851387024, 0.08088570088148117, 0.157774418592453, 0.1254730224609375, 0.16436991095542908, 0.172728031873703, 0.1298597753047943, 0.1606420874595642, 0.09622229635715485, 0.0899832546710968, 0.09138112515211105, 0.11261391639709473, 0.10996303707361221, 0.11483614146709442, 0.14404913783073425, 0.1074102520942688, 0.12398936599493027, 0.13237430155277252, 0.20142701268196106, 0.0959799513220787, 0.1756829470396042, 0.1292681246995926, 0.10518544912338257, 0.10355620086193085, 0.17871952056884766, 0.09326324611902237, 0.16271471977233887, 0.09338729083538055, 0.1495932787656784, 0.09819348901510239, 0.13984480500221252, 0.13090649247169495, 0.13283416628837585, 0.1220582127571106, 0.1250336766242981, 0.11903905868530273, 0.12468727678060532, 0.12834858894348145, 0.07209227234125137, 0.146245077252388, 0.08455473929643631, 0.14386528730392456, 0.10742229223251343, 0.08077966421842575, 0.12333901226520538, 0.0932060033082962, 0.12410479784011841, 0.10619576275348663, 0.13224998116493225, 0.10945948958396912, 0.11102721840143204, 0.09553578495979309, 0.13113771378993988, 0.07826700806617737, 0.11795901507139206, 0.11829957365989685, 0.09065942466259003, 0.14664345979690552, 0.07936696708202362, 0.19930657744407654, 0.1134333610534668, 0.15582790970802307, 0.09253711998462677, 0.14142043888568878, 0.12380553781986237, 0.1098368763923645, 0.11705093085765839, 0.1447671800851822, 0.09724694490432739, 0.12939348816871643, 0.1319824457168579, 0.061190374195575714, 0.1443665772676468, 0.12250512838363647, 0.12134678661823273, 0.09741149097681046, 0.17388826608657837, 0.12383370101451874, 0.14071302115917206, 0.10681263357400894, 0.0983639732003212, 0.15578430891036987, 0.0976884663105011, 0.13446946442127228, 0.11776559799909592, 0.11383286863565445, 0.16804882884025574, 0.107323057949543, 0.11633150279521942, 0.14774681627750397, 0.10703961551189423, 0.11137884110212326, 0.0913608968257904, 0.09584782272577286, 0.04504565894603729, 0.09184505045413971, 0.08601205050945282, 0.10880425572395325, 0.10934115946292877, 0.07927538454532623, 0.12484002858400345, 0.06375544518232346, 0.1253771185874939, 0.09011615812778473, 0.1663360595703125, 0.0877145379781723, 0.0903405100107193, 0.10472260415554047, 0.11064714193344116, 0.10118064284324646, 0.07806552946567535, 0.08733730018138885, 0.13226056098937988, 0.07857424765825272, 0.13332802057266235, 0.06725392490625381, 0.08712217956781387, 0.09153485298156738, 0.1176857128739357, 0.11056101322174072, 0.11429978162050247, 0.0848277285695076, 0.08171725273132324, 0.08182188868522644, 0.1405406892299652, 0.07857902348041534, 0.11822099983692169, 0.09193968772888184, 0.06744226813316345, 0.08406670391559601, 0.0912405475974083, 0.14504502713680267, 0.10302245616912842, 0.09288804978132248, 0.16203993558883667, 0.09839509427547455, 0.12575595080852509, 0.14853106439113617, 0.10599358379840851, 0.07068918645381927, 0.05372047424316406, 0.10961073637008667, 0.1317947506904602, 0.08836784213781357, 0.10398083925247192, 0.09406976401805878, 0.06787024438381195, 0.09739120304584503, 0.139665886759758, 0.08995360136032104, 0.08035697042942047, 0.09329799562692642, 0.08161145448684692, 0.07225065678358078, 0.08060143142938614, 0.10422402620315552, 0.0895206481218338, 0.07600488513708115, 0.10037659108638763, 0.07618575543165207, 0.12233562022447586, 0.09721408784389496, 0.09459839761257172, 0.043384455144405365, 0.11666866391897202, 0.12780064344406128, 0.07171600311994553, 0.10631328821182251, 0.09720439463853836, 0.08601216971874237, 0.09067722409963608, 0.09019425511360168, 0.03835069388151169, 0.10926467925310135, 0.08650241792201996, 0.13313543796539307, 0.09838007390499115, 0.08219854533672333, 0.10876773297786713, 0.09885033965110779, 0.143303781747818, 0.16381727159023285, 0.09318491071462631, 0.12516151368618011, 0.11638946831226349, 0.09601911157369614, 0.19695940613746643, 0.19185175001621246, 0.11410162597894669, 0.0938071757555008, 0.128243088722229, 0.12017565965652466, 0.1142120510339737, 0.1501648724079132, 0.1064324900507927, 0.09067784994840622, 0.07711221277713776, 0.09106862545013428, 0.10050608217716217, 0.257266104221344, 0.12075930833816528, 0.09931228309869766, 0.09503642469644547, 0.14908365905284882, 0.0920061320066452, 0.13144829869270325, 0.08316095918416977, 0.09558863937854767, 0.11433334648609161, 0.10711275041103363, 0.20578446984291077, 0.12072283029556274, 0.14269791543483734, 0.10698623955249786, 0.1337171196937561, 0.16733916103839874, 0.18996167182922363, 0.17717227339744568, 0.20982539653778076, 0.2447102665901184, 0.10288392007350922, 0.173130601644516, 0.15976858139038086, 0.12407679855823517, 0.1602030247449875, 0.25468185544013977, 0.10119123756885529, 0.16230598092079163, 0.08271822333335876, 0.16714967787265778, 0.1581043303012848, 0.1272198110818863, 0.11403535306453705, 0.18603450059890747, 0.1439826637506485, 0.1450788825750351, 0.11920046806335449, 0.08296102285385132, 0.08712518215179443, 0.12967412173748016, 0.12148110568523407, 0.18148644268512726, 0.1515236794948578, 0.18569758534431458, 0.13598160445690155, 0.0954219251871109, 0.13071046769618988, 0.15538887679576874, 0.10763292759656906, 0.12758418917655945, 0.1390523612499237, 0.10582996159791946, 0.11786980926990509, 0.15284617245197296, 0.1434318721294403, 0.2327985167503357, 0.11684355139732361, 0.09563863277435303, 0.20628592371940613, 0.12426336109638214, 0.08332103490829468, 0.11191745847463608, 0.16786080598831177, 0.16070052981376648, 0.09113392978906631, 0.1291288584470749, 0.23613253235816956, 0.1674358993768692, 0.2566518187522888, 0.10224340856075287, 0.17589260637760162, 0.16672399640083313, 0.18013790249824524, 0.24194490909576416, 0.19533997774124146, 0.11335330456495285, 0.10982454568147659, 0.18505345284938812, 0.1393538862466812, 0.11851932853460312, 0.13282352685928345, 0.1382976919412613, 0.43347057700157166, 0.18359026312828064, 0.20810246467590332, 0.3697453439235687, 0.17393776774406433, 0.26843589544296265, 0.16713017225265503, 0.1858213245868683, 0.21525990962982178, 0.2797260880470276, 0.3553367555141449, 0.11442358046770096, 0.18450121581554413, 0.1927371621131897, 0.3581928610801697, 0.20152156054973602, 0.21169406175613403, 0.24684956669807434, 0.1814502477645874, 0.23515917360782623, 0.1581728756427765, 0.24935923516750336, 0.35029757022857666, 0.22484618425369263, 0.2263331413269043, 0.25723016262054443, 0.15098746120929718, 0.21272072196006775, 0.20444121956825256, 0.22003480792045593, 0.21780255436897278, 0.16186246275901794, 0.2197951376438141, 0.2018476128578186, 0.28778478503227234, 0.1452205777168274, 0.17813509702682495, 0.3804755210876465, 0.22681915760040283, 0.10274533927440643, 0.20056578516960144, 0.1953989863395691, 0.15221664309501648, 0.15984749794006348, 0.18499615788459778, 0.16356390714645386, 0.49060335755348206, 0.16321948170661926, 0.12378869205713272, 0.17402034997940063, 0.16266904771327972, 0.17282181978225708, 0.1938498318195343, 0.21057625114917755, 0.14132553339004517, 0.08964814245700836, 0.19836974143981934, 0.18417465686798096, 0.1491132378578186, 0.12692035734653473, 0.1047387570142746, 0.14349733293056488, 0.15998612344264984, 0.27341505885124207, 0.1637866199016571, 0.10055477917194366, 0.11794233322143555, 0.13968198001384735, 0.16804960370063782, 0.2495594322681427, 0.22641178965568542, 0.1305495649576187, 0.10209496319293976, 0.09759223461151123, 0.19052694737911224, 0.14223308861255646, 0.17662256956100464, 0.09084365516901016, 0.14137399196624756, 0.11992713809013367, 0.13000795245170593, 0.11375963687896729, 0.12043075263500214, 0.1243438571691513, 0.12369734048843384, 0.131602942943573, 0.13298694789409637, 0.10762029886245728, 0.08691595494747162, 0.12606345117092133, 0.10233600437641144, 0.13705690205097198, 0.11162737756967545, 0.10125678777694702, 0.09886269271373749, 0.10186189413070679, 0.0962696298956871, 0.13491129875183105, 0.08480364084243774, 0.08944015204906464, 0.12896119058132172, 0.09313545376062393, 0.17939940094947815, 0.07658198475837708, 0.074102483689785, 0.10448519885540009, 0.11607998609542847, 0.07865260541439056, 0.08616985380649567, 0.13874900341033936, 0.15758207440376282, 0.09062127768993378, 0.16334375739097595, 0.12449885904788971, 0.09137623012065887, 0.09836536645889282, 0.1330651491880417, 0.09751333296298981, 0.1136748343706131, 0.18331944942474365, 0.08464383333921432, 0.1672477424144745, 0.09563587605953217, 0.09184180200099945, 0.15587377548217773, 0.06447252631187439, 0.10714847594499588, 0.12477672845125198, 0.10176633298397064, 0.1453157216310501, 0.09183064848184586, 0.08757345378398895, 0.09759293496608734, 0.10625068843364716, 0.10112318396568298, 0.12712371349334717, 0.0798938125371933, 0.11834325641393661, 0.1742027997970581, 0.12783537805080414, 0.1454988270998001, 0.06857308000326157, 0.10010230541229248, 0.1028485894203186, 0.14210177958011627, 0.07570221275091171, 0.12403746694326401, 0.15330082178115845, 0.07968717813491821, 0.10978516936302185, 0.11596660315990448, 0.0997982919216156, 0.14116902649402618, 0.18255329132080078, 0.07640107721090317, 0.08668119460344315, 0.0891684740781784, 0.14605236053466797, 0.15463607013225555, 0.10940048098564148, 0.10333839058876038, 0.0912991389632225, 0.098067507147789, 0.13184858858585358, 0.08887089788913727, 0.12784072756767273, 0.11551046371459961, 0.2507243752479553, 0.3133300244808197, 0.15405091643333435, 0.1694442629814148, 0.20538339018821716, 0.12586669623851776, 0.13610994815826416, 0.15215826034545898, 0.15640896558761597, 0.11538594961166382, 0.14878344535827637, 0.15574508905410767, 0.2897816002368927, 0.12441373616456985, 0.12564778327941895, 0.24312026798725128, 0.1719222366809845, 0.31052088737487793, 0.15666288137435913, 0.14962469041347504, 0.18984249234199524, 0.09812993556261063, 0.12206314504146576, 0.12009882926940918, 0.27576908469200134, 0.17602191865444183, 0.1239934116601944, 0.25979530811309814, 0.129579558968544, 0.13481667637825012, 0.19053110480308533, 0.2834912836551666, 0.2558002769947052, 0.1903652548789978, 0.15620054304599762, 0.16490685939788818, 0.1566219925880432, 0.14044612646102905, 0.14971935749053955, 0.2398737370967865, 0.16062286496162415, 0.12912234663963318, 0.12580953538417816, 0.0932321771979332, 0.16462558507919312, 0.2126421481370926, 0.14174044132232666, 0.1851438283920288, 0.18297238647937775, 0.10293816030025482, 0.10051459074020386, 0.1523984968662262, 0.10557346045970917, 0.14515788853168488, 0.13622738420963287, 0.1194191575050354, 0.1216830164194107, 0.15379159152507782, 0.09778445214033127, 0.10105779767036438, 0.12853440642356873, 0.1281951367855072, 0.14667309820652008, 0.08381321281194687, 0.14228273928165436, 0.17640379071235657, 0.13576556742191315, 0.11412542313337326, 0.15653787553310394, 0.06793715804815292, 0.1238793432712555, 0.09364411234855652, 0.09919938445091248, 0.1257639229297638, 0.1490200161933899, 0.18348154425621033, 0.11055462807416916, 0.1013524979352951, 0.11078639328479767, 0.10999326407909393, 0.054944612085819244, 0.08653949201107025, 0.09049847722053528, 0.12074810266494751, 0.1295606940984726, 0.12022233754396439, 0.13768725097179413, 0.14254464209079742, 0.09340934455394745, 0.07337946444749832, 0.11533451080322266, 0.08450518548488617, 0.09073248505592346, 0.11121198534965515, 0.1341654658317566, 0.16131243109703064, 0.20117133855819702, 0.13434238731861115, 0.1304989606142044, 0.08897242695093155, 0.10466465353965759, 0.08557621389627457, 0.19509705901145935, 0.14323462545871735, 0.13032042980194092, 0.16130276024341583, 0.12534242868423462, 0.07964945584535599, 0.12840627133846283, 0.08407250791788101, 0.16883811354637146, 0.14813369512557983, 0.1101716011762619, 0.0794694647192955, 0.16588333249092102, 0.11808988451957703, 0.08109387755393982, 0.11132147163152695, 0.10804714262485504, 0.0944904014468193, 0.1248776763677597, 0.10391352325677872, 0.11260901391506195, 0.13243307173252106, 0.1601659059524536, 0.09427303075790405, 0.12081548571586609, 0.1302853524684906, 0.11423998326063156, 0.10929788649082184, 0.12993744015693665, 0.09043070673942566, 0.08438563346862793, 0.08240347355604172, 0.12340866774320602, 0.16369912028312683, 0.10335825383663177, 0.12020248919725418, 0.11982443928718567, 0.15661966800689697, 0.09593941271305084, 0.07064616680145264, 0.10486392676830292, 0.15074539184570312, 0.14262494444847107, 0.15546205639839172, 0.13059505820274353, 0.13769933581352234, 0.05884978175163269, 0.11754504591226578, 0.10059519857168198, 0.09762632846832275, 0.08387554436922073, 0.07706630229949951, 0.11172235012054443, 0.10754668712615967, 0.09288524091243744, 0.11397286504507065, 0.07130825519561768, 0.08675815165042877, 0.12087896466255188, 0.09209631383419037, 0.09110060334205627, 0.09140004217624664, 0.1369459331035614, 0.12609082460403442, 0.14623615145683289, 0.12726512551307678, 0.075836181640625, 0.08434909582138062, 0.10159610956907272, 0.09523805975914001, 0.146207794547081, 0.11797462403774261, 0.10853709280490875, 0.09790680557489395, 0.07306081801652908, 0.1165459081530571, 0.16414324939250946, 0.10973332077264786, 0.12225710600614548, 0.1317671537399292, 0.0876709520816803, 0.086038738489151, 0.10245033353567123, 0.08699073642492294, 0.09461867809295654, 0.08342090249061584, 0.07087394595146179, 0.16107657551765442, 0.1018243357539177, 0.13006684184074402, 0.12211231887340546, 0.11135876923799515, 0.14079290628433228, 0.1098710298538208, 0.12988895177841187, 0.06564559787511826, 0.10229448974132538, 0.09369634091854095, 0.11115167289972305, 0.11271826922893524, 0.10987551510334015, 0.11276198923587799, 0.1126231700181961, 0.11856880784034729, 0.0756477415561676, 0.10838502645492554, 0.1431138813495636, 0.13835960626602173, 0.0748596340417862, 0.10964524745941162, 0.17062516510486603, 0.11576537787914276, 0.09797561913728714, 0.11012755334377289, 0.0941595733165741, 0.10548629611730576, 0.18649476766586304, 0.0894574522972107, 0.08814700692892075, 0.09220311045646667, 0.09089019894599915, 0.1337466537952423, 0.10123216360807419, 0.08484542369842529, 0.1041940450668335, 0.07543431222438812, 0.0634903609752655, 0.10653482377529144, 0.11822139471769333, 0.09055209159851074, 0.06724166125059128, 0.11839713156223297, 0.08185646682977676, 0.08186078816652298, 0.12421111762523651, 0.08962417393922806, 0.09316027909517288, 0.15442636609077454, 0.10470643639564514, 0.08009854704141617, 0.11004050821065903, 0.10551635175943375, 0.0857236385345459, 0.10490011423826218, 0.071988046169281, 0.07415788620710373, 0.08262211829423904, 0.07786542177200317, 0.1028919443488121, 0.07281408458948135, 0.1015879362821579, 0.09641702473163605, 0.05182824283838272, 0.09150058031082153, 0.08955910801887512, 0.12243515998125076, 0.11437541246414185, 0.1055992990732193, 0.07816773653030396, 0.05678245425224304, 0.06810708343982697, 0.078937828540802, 0.07780525088310242, 0.09712594747543335, 0.10733821988105774, 0.09267057478427887, 0.08090472221374512, 0.09293991327285767, 0.09545426070690155, 0.08994989842176437, 0.08964170515537262, 0.10056091099977493, 0.07867331802845001, 0.09608200192451477, 0.12215893715620041, 0.10101824253797531, 0.08230708539485931, 0.07661817967891693, 0.1048232764005661, 0.08120414614677429, 0.05957971140742302, 0.09270969033241272, 0.08917073905467987, 0.07263730466365814, 0.10566579550504684, 0.11025853455066681, 0.12472033500671387, 0.0917607992887497, 0.09608891606330872, 0.12385337054729462, 0.08223867416381836, 0.1030460000038147, 0.0890941470861435, 0.11536256968975067, 0.09043936431407928, 0.1287793666124344, 0.09816411137580872, 0.07337632030248642, 0.09517475962638855, 0.07784430682659149, 0.08815914392471313, 0.09157299995422363, 0.1323089301586151, 0.06355923414230347, 0.10050898790359497, 0.08676546812057495, 0.11934305727481842, 0.11750784516334534, 0.06941413134336472, 0.0955638512969017, 0.09015066921710968, 0.07502066344022751, 0.07767735421657562, 0.0946279987692833, 0.09901785105466843, 0.07491983473300934, 0.10181461274623871, 0.1080288290977478, 0.09300516545772552, 0.062263451516628265, 0.09637697041034698, 0.0962175577878952, 0.13131429255008698, 0.07502983510494232, 0.08880484104156494, 0.09470391273498535, 0.07798977196216583, 0.12567555904388428, 0.06203513219952583, 0.0720016360282898, 0.1136966273188591, 0.0828917920589447, 0.09926337003707886, 0.10715578496456146, 0.0862775444984436, 0.15153788030147552, 0.12959595024585724, 0.08638075739145279, 0.1263016015291214, 0.07869666069746017, 0.09406678378582001, 0.09030002355575562, 0.08711981773376465, 0.07109566777944565, 0.09063442051410675, 0.06341112405061722, 0.09018993377685547, 0.14278866350650787, 0.09101284295320511, 0.1029011607170105, 0.0788436084985733, 0.0917668491601944, 0.09057383239269257, 0.11085875332355499, 0.10123845934867859, 0.0959610790014267, 0.11797232180833817, 0.08498544245958328, 0.07569663971662521, 0.10759287327528, 0.08590199053287506, 0.13988935947418213, 0.08720165491104126, 0.079999178647995, 0.10722951591014862, 0.1352258026599884, 0.12435589730739594, 0.10855220258235931, 0.15212313830852509, 0.06625217944383621, 0.07604674994945526, 0.07746943086385727, 0.10547295957803726, 0.10993922501802444, 0.0777190700173378, 0.09524893760681152, 0.08210337162017822, 0.08045586198568344, 0.08627080917358398, 0.07718844711780548, 0.152016282081604, 0.07514647394418716, 0.12149713933467865, 0.1438867300748825, 0.10645619034767151, 0.09934654086828232, 0.15924228727817535, 0.12067010998725891, 0.07395129650831223, 0.1327986717224121, 0.08007996529340744, 0.13497625291347504, 0.10359799861907959, 0.06423734128475189, 0.12372899055480957, 0.1458369642496109, 0.08216790854930878, 0.09265973418951035, 0.09497658163309097, 0.1489323079586029, 0.07076242566108704, 0.12508727610111237, 0.09304719418287277, 0.10498015582561493, 0.13636493682861328, 0.10086147487163544, 0.10826939344406128, 0.050491008907556534, 0.08327994495630264, 0.08846976608037949, 0.14698249101638794, 0.12196579575538635, 0.07693922519683838, 0.16278895735740662, 0.11471329629421234, 0.13933944702148438, 0.09702564775943756, 0.08035659790039062, 0.1368541419506073, 0.12638741731643677, 0.2412763386964798, 0.11109030991792679, 0.13750214874744415, 0.0697542205452919, 0.17598873376846313, 0.15790444612503052, 0.18130671977996826, 0.0966181829571724, 0.18428589403629303, 0.11720985174179077, 0.23082469403743744, 0.10545031726360321, 0.1338939517736435, 0.14262579381465912, 0.08419131487607956, 0.08119046688079834, 0.11272437125444412, 0.1158614307641983, 0.10139013826847076, 0.11572559177875519, 0.13356028497219086, 0.06847827136516571, 0.07820473611354828, 0.12094332277774811, 0.08651454746723175, 0.0790616050362587, 0.09266497939825058, 0.09636233747005463, 0.10208995640277863, 0.11815926432609558, 0.13451603055000305, 0.1818799525499344, 0.13945472240447998, 0.10801924020051956, 0.18971405923366547, 0.08872272819280624, 0.12944921851158142, 0.16748283803462982, 0.17371885478496552, 0.1908324956893921, 0.14227193593978882, 0.13744062185287476, 0.15289823710918427, 0.5159826874732971, 0.08612525463104248, 0.08531656116247177, 0.16586914658546448, 0.586705207824707, 0.4971444010734558, 0.19653403759002686, 0.3225109577178955, 0.25050246715545654, 0.33922910690307617, 0.3277074694633484, 0.2699025273323059, 0.7000924944877625, 0.24258540570735931, 0.3733535408973694, 0.314236581325531, 0.415981650352478, 0.9638767242431641, 0.22780413925647736, 0.17328763008117676, 0.3197651207447052, 0.40091413259506226, 0.3751522898674011, 0.7777432799339294, 0.4396820068359375, 0.31799936294555664, 0.3902515769004822, 0.32970619201660156, 0.3457079231739044, 0.2600760757923126, 0.3668280839920044, 0.2571072280406952, 0.3600514829158783, 0.2536458969116211, 0.4465838074684143, 0.6865167021751404, 0.28038665652275085, 0.34541985392570496, 0.2957778573036194, 0.6511403918266296, 0.6376298666000366, 0.2677985727787018, 0.2629832625389099, 0.3044588267803192, 0.18226340413093567, 0.30093881487846375, 0.3384479880332947, 0.40531620383262634, 0.37749767303466797, 0.3036630153656006, 0.33426451683044434, 0.2661803364753723, 0.18686547875404358, 0.18536856770515442, 0.16799180209636688, 0.22943967580795288, 0.1702997386455536, 0.14746780693531036, 0.17590740323066711, 0.3547554612159729, 0.36650076508522034, 0.2062901258468628, 0.2616370618343353, 0.19379496574401855, 0.2799126207828522, 0.43994563817977905, 0.2988983690738678, 0.21622006595134735, 0.1881333589553833, 0.10061769187450409, 0.2128836214542389, 0.19362656772136688, 0.20889060199260712, 0.30952906608581543, 0.14968280494213104, 0.2763237953186035, 0.1564546823501587, 0.1571611613035202, 0.18280121684074402, 0.29272425174713135, 0.20062477886676788, 0.17392563819885254, 0.20403356850147247, 0.24845053255558014, 0.18109843134880066, 0.17331057786941528, 0.1646127551794052, 0.1937764436006546, 0.1070362776517868, 0.14505577087402344, 0.16562548279762268, 0.12829384207725525, 0.17911174893379211, 0.1853397637605667, 0.16752979159355164, 0.14075332880020142, 0.15391278266906738, 0.150858536362648, 0.17188812792301178, 0.17568689584732056, 0.1705913543701172, 0.24454501271247864, 0.1650664061307907, 0.17668679356575012, 0.12302906811237335, 0.12518393993377686, 0.10472829639911652, 0.13670456409454346, 0.11127675324678421, 0.12936952710151672, 0.15444537997245789, 0.11752033233642578, 0.13329371809959412, 0.13210885226726532, 0.11156214773654938, 0.0920722484588623, 0.1663896143436432, 0.10540269315242767, 0.09567956626415253, 0.18286094069480896, 0.1156049519777298, 0.08903603255748749, 0.16830459237098694, 0.1410611867904663, 0.10909053683280945, 0.10718472301959991, 0.11558079719543457, 0.07964741438627243, 0.11991056799888611, 0.17163598537445068, 0.08413935452699661, 0.15461274981498718, 0.09284502267837524, 0.1258675754070282, 0.1147313266992569, 0.12106770277023315, 0.07503925263881683, 0.10992677509784698, 0.11094312369823456, 0.0992688313126564, 0.07161775976419449, 0.10850026458501816, 0.18579955399036407, 0.0641951635479927, 0.12704646587371826, 0.11231683194637299, 0.1036452054977417, 0.10096999257802963, 0.09497451782226562, 0.1269603818655014, 0.12892866134643555, 0.06350623071193695, 0.10449706017971039, 0.0711091160774231, 0.09380143135786057, 0.11707136034965515, 0.0964510589838028, 0.1528329849243164, 0.13691622018814087, 0.06480585038661957, 0.10908953100442886, 0.10412061959505081, 0.1362646222114563, 0.09997156262397766, 0.11895811557769775, 0.12540319561958313, 0.11225683242082596, 0.06896333396434784, 0.09200523793697357, 0.10731392353773117, 0.12399265170097351, 0.09232158958911896, 0.11248204112052917, 0.1118597462773323, 0.1383575201034546, 0.07449155300855637, 0.08333976566791534, 0.10577546060085297, 0.1542423665523529, 0.14532789587974548, 0.09883619099855423, 0.21598240733146667, 0.0898575410246849, 0.09992137551307678, 0.08173587918281555, 0.1234789714217186, 0.10537942498922348, 0.1341446340084076, 0.07051236927509308, 0.16513310372829437, 0.11000619828701019, 0.11611651629209518, 0.12806469202041626, 0.1213420182466507, 0.10817718505859375, 0.1678975373506546, 0.14427387714385986, 0.22126415371894836, 0.11285778880119324, 0.13420170545578003, 0.06667502224445343, 0.1111033707857132, 0.11167075484991074, 0.1450829654932022, 0.08780921995639801, 0.15045113861560822, 0.22569185495376587, 0.13029253482818604, 0.1516963690519333, 0.15407219529151917, 0.1171727180480957, 0.1540910005569458, 0.10401353240013123, 0.12637163698673248, 0.10584038496017456, 0.10939682275056839, 0.1249934732913971, 0.14891555905342102, 0.11539777368307114, 0.10722853243350983, 0.10727807879447937, 0.12287561595439911, 0.08179503679275513, 0.07497911155223846, 0.15819445252418518, 0.13404038548469543, 0.15811815857887268, 0.15937016904354095, 0.10206048935651779, 0.07934533059597015, 0.14002498984336853, 0.08080024272203445, 0.11258736252784729, 0.0985046923160553, 0.09811647981405258, 0.10831823199987411, 0.11453990638256073, 0.1318475753068924, 0.06586883962154388, 0.1042601615190506, 0.11147251725196838, 0.11330398917198181, 0.12313651293516159, 0.11914888024330139, 0.11038084328174591, 0.11190265417098999, 0.08464035391807556, 0.1425066888332367, 0.15240326523780823, 0.06781931221485138, 0.10442189872264862, 0.08981435000896454, 0.15581682324409485, 0.09346597641706467, 0.12367226928472519, 0.09670674055814743, 0.08390013128519058, 0.10581999272108078, 0.10138056427240372, 0.08576729148626328, 0.09953237324953079, 0.090762197971344, 0.09105832129716873, 0.07431361079216003, 0.07128462195396423, 0.11723321676254272, 0.06955023854970932, 0.08528867363929749, 0.13066041469573975, 0.09225621819496155, 0.11262963712215424, 0.11854629218578339, 0.12237830460071564, 0.13760165870189667, 0.09709831327199936, 0.11970950663089752, 0.11247321218252182, 0.0979732796549797, 0.09395568817853928, 0.09416381269693375, 0.12100417166948318, 0.0844188928604126, 0.06294265389442444, 0.08938109874725342, 0.11218112707138062, 0.13343828916549683, 0.10322071611881256, 0.12620288133621216, 0.1286337822675705, 0.09225970506668091, 0.10004261136054993, 0.10517358779907227, 0.08428522199392319, 0.08716963231563568, 0.09124530106782913, 0.08217674493789673, 0.10450087487697601, 0.09012198448181152, 0.14974722266197205, 0.09468421339988708, 0.10271166265010834, 0.07451416552066803, 0.09938979893922806, 0.10307581722736359, 0.1222841739654541, 0.06222544610500336, 0.1067914217710495, 0.13977356255054474, 0.1525348275899887, 0.08259657025337219, 0.09756086766719818, 0.1105199009180069, 0.13127490878105164, 0.10270511358976364, 0.0550033301115036, 0.17060044407844543, 0.2215496301651001, 0.14221173524856567, 0.09862393140792847, 0.07668596506118774, 0.11322873085737228, 0.0927218496799469, 0.11652788519859314, 0.09776186943054199, 0.09847572445869446, 0.18601784110069275, 0.0819210410118103, 0.13031554222106934, 0.1925104409456253, 0.07473774254322052, 0.0863739550113678, 0.11445742845535278, 0.08153997361660004, 0.09209070354700089, 0.17617660760879517, 0.06696845591068268, 0.08993781358003616, 0.10628213733434677, 0.11855623126029968, 0.09762629866600037, 0.0733407586812973, 0.07764691114425659, 0.0893731564283371, 0.07630271464586258, 0.09225556254386902, 0.09155210852622986, 0.0902114063501358, 0.15411494672298431, 0.07883693277835846, 0.12801037728786469, 0.10812652111053467, 0.11442115902900696, 0.11929072439670563, 0.12926311790943146, 0.09401103109121323, 0.13828502595424652, 0.11573400348424911, 0.13845831155776978, 0.06995022296905518, 0.1258598268032074, 0.10940505564212799, 0.1250954270362854, 0.11265683174133301, 0.08945773541927338, 0.11427472531795502, 0.07210925221443176, 0.12206576019525528, 0.15029555559158325, 0.09071757644414902, 0.07944919914007187, 0.06230422854423523, 0.10793258249759674, 0.08354637026786804, 0.07274957746267319, 0.0996716096997261, 0.08872927725315094, 0.10612915456295013, 0.07662192732095718, 0.06072334200143814, 0.08800522238016129, 0.08007177710533142, 0.10265108942985535, 0.13003316521644592, 0.11785202473402023, 0.0945606455206871, 0.1176726371049881, 0.08993237465620041, 0.09481512755155563, 0.09941157698631287, 0.15386804938316345, 0.08725155144929886, 0.13556008040905, 0.12132231891155243, 0.09654872119426727, 0.12244385480880737, 0.07720330357551575, 0.07949146628379822, 0.07762223482131958, 0.07360652834177017, 0.10245288908481598, 0.0892501026391983, 0.1327393352985382, 0.13976949453353882, 0.09269475191831589, 0.08516029268503189, 0.0766710713505745, 0.09927602857351303, 0.08112744987010956, 0.09875522553920746, 0.09432147443294525, 0.06962418556213379, 0.09446301311254501, 0.08249839395284653, 0.14184993505477905, 0.13770364224910736, 0.10917158424854279, 0.07263091206550598, 0.08971276134252548, 0.11015427112579346, 0.06558845937252045, 0.08848019689321518, 0.12324753403663635, 0.16619868576526642, 0.06885194778442383, 0.12285872548818588, 0.07561151683330536, 0.10442103445529938, 0.06624244153499603, 0.0834018737077713, 0.06501540541648865, 0.10831080377101898, 0.11919872462749481, 0.10768589377403259, 0.09410438686609268, 0.08575914800167084, 0.1385555863380432, 0.08332070708274841, 0.11557713150978088, 0.08340957760810852, 0.12543389201164246, 0.09345392882823944, 0.12173192203044891, 0.10198666155338287, 0.12943054735660553, 0.1638561189174652, 0.1270357072353363, 0.10117902606725693, 0.12365411967039108, 0.10747209191322327, 0.1631365865468979, 0.13893142342567444, 0.07562854886054993, 0.12957249581813812, 0.12626197934150696, 0.19181859493255615, 0.14607177674770355, 0.13215422630310059, 0.16008427739143372, 0.11020985245704651, 0.13717998564243317, 0.1486397683620453, 0.11932510137557983, 0.12284446507692337, 0.12235039472579956, 0.08200450241565704, 0.11988973617553711, 0.11571262776851654, 0.1407451033592224, 0.1326824426651001, 0.07572513818740845, 0.13685664534568787, 0.13835737109184265, 0.09091067314147949, 0.09903379529714584, 0.21664196252822876, 0.13298526406288147, 0.14486971497535706, 0.0705273300409317, 0.11959618330001831, 0.10549651086330414, 0.13278837502002716, 0.1427442729473114, 0.12165255099534988, 0.07650215923786163, 0.10002833604812622, 0.08959861099720001, 0.08473819494247437, 0.10399297624826431, 0.11592169106006622, 0.06293319165706635, 0.09376783668994904, 0.10781294852495193, 0.12101909518241882, 0.09712409228086472, 0.05854051187634468, 0.1422809362411499, 0.08160136640071869, 0.1323719620704651, 0.08174159377813339, 0.08788622915744781, 0.07651142030954361, 0.1060333177447319, 0.1006445363163948, 0.10423250496387482, 0.07058722525835037, 0.10656636953353882, 0.1476851850748062, 0.10419075936079025, 0.07775270193815231, 0.07218880951404572, 0.10546311736106873, 0.089317686855793, 0.0914527177810669, 0.15001949667930603, 0.09628711640834808, 0.08884011209011078, 0.06963594257831573, 0.07078161835670471, 0.07829931378364563, 0.09222950786352158, 0.11700413376092911, 0.15282714366912842, 0.09269838780164719, 0.07445467263460159, 0.0867139995098114, 0.08677039295434952, 0.07385928928852081, 0.06725148856639862, 0.11419814079999924, 0.10989956557750702, 0.0781393051147461, 0.09406516700983047, 0.09393885731697083, 0.10401704162359238, 0.11355344951152802, 0.10516378283500671, 0.07199488580226898, 0.08911144733428955, 0.11036942154169083, 0.08880932629108429, 0.08783179521560669, 0.10867069661617279, 0.07094943523406982, 0.07657862454652786, 0.09052509814500809, 0.11214835941791534, 0.10380128026008606, 0.13021351397037506, 0.0821080207824707, 0.08916153013706207, 0.14726462960243225, 0.11083874106407166, 0.07814921438694, 0.10206472873687744, 0.14271271228790283, 0.08656780421733856, 0.07940836250782013, 0.09271585941314697, 0.1831340789794922, 0.13181747496128082, 0.13042934238910675, 0.0836779922246933, 0.1150176078081131, 0.1127677857875824, 0.09732456505298615, 0.07994186133146286, 0.09219668060541153, 0.1122141107916832, 0.09084481000900269, 0.13268642127513885, 0.08523102849721909, 0.10368350893259048, 0.08066791296005249, 0.08366283029317856, 0.0797320008277893, 0.13761205971240997, 0.06969592720270157, 0.11072218418121338, 0.10222026705741882, 0.10705267637968063, 0.1404995620250702, 0.14808830618858337, 0.10837681591510773, 0.09197178483009338, 0.16263186931610107, 0.08211568742990494, 0.12188763916492462, 0.1467159390449524, 0.144607275724411, 0.2114793211221695, 0.07416687160730362, 0.07679786533117294, 0.11519747227430344, 0.12610304355621338, 0.14611132442951202, 0.09771180897951126, 0.1367461234331131, 0.1375962793827057, 0.07399770617485046, 0.10806054621934891, 0.11774428188800812, 0.0915418267250061, 0.09054659307003021, 0.11627340316772461, 0.09034338593482971, 0.09810775518417358, 0.08857674151659012, 0.07523006200790405, 0.07698778808116913, 0.10310779511928558, 0.07047379016876221, 0.13418695330619812, 0.1437223255634308, 0.11121068894863129, 0.07527881860733032, 0.0675036609172821, 0.08649957925081253, 0.06611854583024979, 0.08869152516126633, 0.10206729173660278, 0.09692373871803284, 0.1072569265961647, 0.17436566948890686, 0.08390606939792633, 0.09835886210203171, 0.07727545499801636, 0.07615359127521515, 0.10818924754858017, 0.06639721989631653, 0.11216835677623749, 0.08932333439588547, 0.10005076974630356, 0.07725569605827332, 0.11963041871786118, 0.09421529620885849, 0.06377270072698593, 0.07341562211513519, 0.08488550782203674, 0.07324673980474472, 0.09738584607839584, 0.09485089778900146, 0.08732151985168457, 0.10328604280948639, 0.10975226759910583, 0.08437459915876389, 0.08902342617511749, 0.08692361414432526, 0.10958311706781387, 0.11504018306732178, 0.07936453819274902, 0.09181896597146988, 0.06636890769004822, 0.10263713449239731, 0.09137550741434097, 0.08185827732086182, 0.08926822245121002, 0.10401055216789246, 0.10621831566095352, 0.10024138540029526, 0.053117785602808, 0.10023162513971329, 0.10682971775531769, 0.10265035182237625, 0.09327380359172821, 0.12305086106061935, 0.11098048835992813, 0.11085112392902374, 0.08723367750644684, 0.09448396414518356, 0.09168675541877747, 0.06683849543333054, 0.10134203732013702, 0.1034889966249466, 0.13149556517601013, 0.1619783341884613, 0.12612883746623993, 0.14220182597637177, 0.10862115025520325, 0.0749158263206482, 0.11541907489299774, 0.07634496688842773, 0.11702024191617966, 0.10342448204755783, 0.06502708792686462, 0.07055766880512238, 0.08922045677900314, 0.08928847312927246, 0.10777995735406876, 0.08206155896186829, 0.07426419854164124, 0.09150978922843933, 0.13187813758850098, 0.07545941323041916, 0.09249216318130493, 0.061771225184202194, 0.07651732861995697, 0.12056633085012436, 0.09018087387084961, 0.07396157085895538, 0.11675140261650085, 0.0922427624464035, 0.09060370922088623, 0.13361887633800507, 0.05869382619857788, 0.06291554868221283, 0.0808098092675209, 0.1130577102303505, 0.059249553829431534, 0.07763153314590454, 0.07791309058666229, 0.11799615621566772, 0.08080632984638214, 0.09483806788921356, 0.07475170493125916, 0.13284076750278473, 0.12254700809717178, 0.0900615006685257, 0.1290888786315918, 0.06200021505355835, 0.07030779868364334, 0.0977901965379715, 0.10226414352655411, 0.06870627403259277, 0.15169814229011536, 0.10982507467269897, 0.06268202513456345, 0.11619260907173157, 0.07376784086227417, 0.08306916058063507, 0.09928439557552338, 0.12602601945400238, 0.10108382999897003, 0.08900631964206696, 0.09460306167602539, 0.09792405366897583, 0.0927174836397171, 0.08350242674350739, 0.087290458381176, 0.09879621863365173, 0.11917749047279358, 0.07463306933641434, 0.1301780492067337, 0.06632582098245621, 0.08362699300050735, 0.09549214690923691, 0.14061850309371948, 0.07756328582763672, 0.12014146149158478, 0.09746421873569489, 0.07517221570014954, 0.05092901363968849, 0.07522173225879669, 0.11453813314437866, 0.07871807366609573, 0.06611640751361847, 0.09562049806118011, 0.11131133884191513, 0.08514586091041565, 0.08242057263851166, 0.09877819567918777, 0.09091579914093018, 0.07500442117452621, 0.07705438882112503, 0.08372973650693893, 0.09001724421977997, 0.08374954015016556, 0.0682486817240715, 0.05694279819726944, 0.06490599364042282, 0.06994643807411194, 0.10841146111488342, 0.14505726099014282, 0.10756567120552063, 0.06809473782777786, 0.1429513692855835, 0.09130895882844925, 0.1270006150007248, 0.1259358823299408, 0.0678328424692154, 0.0760800838470459, 0.07540207356214523, 0.09770698100328445, 0.08180081099271774, 0.13419809937477112, 0.08799678087234497, 0.072066530585289, 0.09370553493499756, 0.103976771235466, 0.07331210374832153, 0.09942269325256348, 0.09045900404453278, 0.09106771647930145, 0.08960014581680298, 0.09562928229570389, 0.13826203346252441, 0.08801591396331787, 0.1954604834318161, 0.09357817471027374, 0.09195683151483536, 0.10680995136499405, 0.08246737718582153, 0.0744798332452774, 0.08878321945667267, 0.06100361794233322, 0.09502501040697098, 0.08966890722513199, 0.11228974908590317, 0.08942365646362305, 0.0857556164264679, 0.09733027964830399, 0.13476544618606567, 0.07864613085985184, 0.12286314368247986, 0.09685582667589188, 0.06834082305431366, 0.05910256877541542, 0.12205038964748383, 0.18320433795452118, 0.11992550641298294, 0.10090555250644684, 0.10980117321014404, 0.12660740315914154, 0.1364428699016571, 0.11409307271242142, 0.19053204357624054, 0.4016553461551666, 0.10429738461971283, 0.1033082976937294, 0.13194473087787628, 0.21982043981552124, 0.1269993931055069, 0.08897164463996887, 0.15317319333553314, 0.162614643573761, 0.1329539269208908, 0.1336117386817932, 0.12028728425502777, 0.11405032873153687, 0.09274394810199738, 0.10244753956794739, 0.1827383041381836, 0.13323161005973816, 0.11991751194000244, 0.16858328878879547, 0.13548441231250763, 0.12894600629806519, 0.1184522733092308, 0.09376412630081177, 0.10692229866981506, 0.332062691450119, 0.1384366750717163]\n",
    "plt.plot(train_loss_noregu[0:l :1] )\n",
    "plt.plot(train_loss[0:l :1] , '.-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend(('Traditional Aversarial Training', 'Proposed'))\n",
    "plt.savefig('/Users/wenting/Documents/04_research/08_Explainable_DNN/04_report/Figures/loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_perturb_0.5pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.6804 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.7018 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.6471 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_1_impedance_1_sigNew\n",
      "Test set results: loss= 0.8573 accuracy= 89.8551 1-hop accuracy = 1.0000\n",
      "Test_perturb_0.5pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6707 accuracy= 95.6522 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6853 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6521 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_1_impedance_2_sigNew\n",
      "Test set results: loss= 0.6638 accuracy= 91.3043 1-hop accuracy = 1.0000\n",
      "Test_perturb_0.5pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.3957 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.4413 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.5211 accuracy= 91.3043 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_2_impedance_1_sigNew\n",
      "Test set results: loss= 0.9274 accuracy= 85.5072 1-hop accuracy = 0.9710\n",
      "Test_perturb_0.5pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6504 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6255 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6483 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_2_impedance_2_sigNew\n",
      "Test set results: loss= 0.6229 accuracy= 91.3043 1-hop accuracy = 1.0000\n",
      "Test_perturb_0.5pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 0.5694 accuracy= 82.6087 1-hop accuracy = 0.9710\n",
      "Test_perturb_1pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 0.7779 accuracy= 76.8116 1-hop accuracy = 0.9565\n",
      "Test_perturb_2pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 1.7659 accuracy= 50.7246 1-hop accuracy = 0.7971\n",
      "Test_perturb_3pu_type_3_impedance_1_sigNew\n",
      "Test set results: loss= 4.1735 accuracy= 28.9855 1-hop accuracy = 0.5942\n",
      "Test_perturb_0.5pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 0.4060 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 0.4743 accuracy= 92.7536 1-hop accuracy = 0.9855\n",
      "Test_perturb_2pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 0.5712 accuracy= 91.3043 1-hop accuracy = 0.9710\n",
      "Test_perturb_3pu_type_3_impedance_2_sigNew\n",
      "Test set results: loss= 1.0131 accuracy= 82.6087 1-hop accuracy = 0.9130\n",
      "Test_perturb_0.5pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 0.4183 accuracy= 88.4058 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 0.5469 accuracy= 88.4058 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 1.4025 accuracy= 62.3188 1-hop accuracy = 0.8696\n",
      "Test_perturb_3pu_type_4_impedance_1_sigNew\n",
      "Test set results: loss= 3.3117 accuracy= 40.5797 1-hop accuracy = 0.6812\n",
      "Test_perturb_0.5pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6336 accuracy= 95.6522 1-hop accuracy = 1.0000\n",
      "Test_perturb_1pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6715 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_2pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6416 accuracy= 94.2029 1-hop accuracy = 1.0000\n",
      "Test_perturb_3pu_type_4_impedance_2_sigNew\n",
      "Test set results: loss= 0.6314 accuracy= 92.7536 1-hop accuracy = 1.0000\n",
      "[[[94.2  94.2  92.75 89.86]\n",
      "  [95.65 94.2  92.75 91.3 ]]\n",
      "\n",
      " [[94.2  94.2  91.3  85.51]\n",
      "  [94.2  94.2  92.75 91.3 ]]\n",
      "\n",
      " [[82.61 76.81 50.72 28.99]\n",
      "  [94.2  92.75 91.3  82.61]]\n",
      "\n",
      " [[88.41 88.41 62.32 40.58]\n",
      "  [95.65 94.2  94.2  92.75]]]\n"
     ]
    }
   ],
   "source": [
    "model_test = Net(dim_input, dim_hidden)\n",
    "#optimizer_test = SGD(model.parameters(), lr = learning_rate, momentum = momentum, weight_decay = weight_decay)   \n",
    "model_test.load_state_dict(torch.load( os.path.join(model_dir, 'model-' + savename + '.pt')))\n",
    "#torch.load(  os.path.join(model_dir, 'opt-fgsm-checkpoint_epoch{}.tar'.format(996)))\n",
    "   \n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "mag = [ '0.5','1','2', '3'] \n",
    "acc_list = np.zeros((4,2,4))\n",
    "acc_hop_list = np.zeros(acc_list.shape)\n",
    "for fault_type in range(4):\n",
    "    for impe_type in range(2): \n",
    "        #X1 = np.zeros((69 * len(mag),1, 272,1))\n",
    "        for i in range(len(mag)):\n",
    "            testName =  'Test_perturb_'+ mag[i] + 'pu_type_' + str(fault_type+1) + '_impedance_' \\\n",
    "            + str(impe_type+1)+ '_sigNew' \n",
    "            print(testName)\n",
    "            test_x, test_i,  test_labels,test_num= load_data_VI(w,rootPath, testName)  \n",
    "            acc, acc_hop = test(test_x, test_labels, model_test, line_neib)\n",
    "            acc_list[fault_type, impe_type, i] = float(\"{:.2f}\".format(acc)) \n",
    "            acc_hop_list[fault_type, impe_type, i] = float(\"{:.2f}\".format(acc_hop))  \n",
    "print( (acc_list ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94.925 94.2   92.75  90.58 ]\n",
      " [94.2   94.2   92.025 88.405]\n",
      " [88.405 84.78  71.01  55.8  ]\n",
      " [92.03  91.305 78.26  66.665]]\n",
      "[92.39    91.12125 83.51125 75.3625 ]\n"
     ]
    }
   ],
   "source": [
    "print( np.mean(acc_list[:,:,:] , axis = 1))\n",
    "a = np.mean(acc_list[:,:,:] , axis = 1)\n",
    "print(np.mean(a, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.    100.     97.825  97.825]\n",
      " [100.     99.275  96.375  92.755]\n",
      " [ 98.55   94.205  72.46   57.25 ]\n",
      " [ 99.275  97.1    81.16   69.565]]\n",
      "[99.45625 97.645   86.955   79.34875]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97.1   94.925 92.75  93.475]\n",
      " [99.275 98.55  95.65  91.305]\n",
      " [97.825 94.205 70.29  55.8  ]\n",
      " [97.825 96.375 80.435 68.115]]\n",
      "[98.00625 96.01375 84.78125 77.17375]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89.86  89.86  84.06  84.785]\n",
      " [88.405 86.235 84.785 82.61 ]\n",
      " [53.625 51.45  46.375 36.96 ]\n",
      " [57.975 56.525 51.45  50.   ]]\n",
      "[72.46625 71.0175  66.6675  63.58875]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.    98.55  98.55  98.55]\n",
      " [100.    98.55  95.65  89.86]\n",
      " [ 98.55  85.51  46.38  27.54]\n",
      " [ 97.1   92.75  52.17  31.88]]\n"
     ]
    }
   ],
   "source": [
    "print(  (acc_list[:,0,:]  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.    98.55  98.55  98.55]\n",
      " [100.   100.    98.55  98.55]\n",
      " [100.    97.1   89.86  84.06]\n",
      " [100.    98.55  98.55  98.55]]\n"
     ]
    }
   ],
   "source": [
    "print(  (acc_list[:,1,:]  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
